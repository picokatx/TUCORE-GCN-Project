{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import add_path\n",
    "import datasets\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BERT.tucoregcn_bert_pytorch_processor import DialogRE\n",
    "dev = DialogRE()\n",
    "gen = dev._generate_examples(\"../datasets/DialogRE/dev.json\", \"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeakerRelation(speaker_x='Estelle', speaker_y='Speaker 1', rid=[7])\n",
      "Speaker 1, {speaker_y} Hey!\n",
      "Speaker 1, Speaker 2: Hey.\n",
      "Speaker 1, Speaker 3: Hey, man. What's up?\n",
      "Speaker 1, {speaker_y} Maybe you can tell me. My agent would like to know why I didn't show up at the audition I didn't know I had today. The first good thing she gets me in weeks. How could you not give me the message?!\n",
      "Speaker 1, Speaker 3: Well, I'll tell ya I do enjoy guilt, but, ah, it wasn't me.\n",
      "Speaker 1, Speaker 2: Yes, it was! It was him! Uh huh! Okay, it was me!\n",
      "Speaker 1, {speaker_y} How is it you?\n",
      "Speaker 1, Speaker 2: Well, it was just, it was all so crazy, you know. I mean, Chandler was in the closet, counting to 10, and he was up to 7 and I hadn't found a place to hide yet. I-I-I meant to tell you, and I wrote it all down on my hand. See, all of it.\n",
      "Speaker 1, {speaker_y} Yep, that's my audition.\n",
      "Speaker 1, Speaker 4: See, now this is why I keep notepads everywhere.\n",
      "Speaker 1, Speaker 2: Yep, and that's why we don't invite you to play.\n",
      "Speaker 1, Speaker 5: What is the great tragedy here? You go get yourself another appointment.\n",
      "Speaker 1, {speaker_y} Well, Estelle tried, you know. The casting director told her that I missed my chance.\n",
      "Speaker 1, Speaker 2: That is unfair. I'll call her and tell her it was totally my fault.\n",
      "Speaker 1, {speaker_y} Pheebs, you can't do that. The casting director doesn't talk to friends, she only talks to agents.\n",
      "Speaker 1, Speaker 2: What a sad little life she must lead. Okay, ooh.\n",
      "Speaker 1, {speaker_y} What, what are you doing? What are you doing?\n",
      "Speaker 1, Speaker 2: No, no, no, I know, I know, ooh. 'Hi, this is Katelynn, from Phoebe Buffay's office. Um, is um, Ann there for Phoebe, she'll know what it's about.'\n",
      "Speaker 1, {speaker_y} Hang up, hang up.\n",
      "Speaker 1, Speaker 2: 'Annie! Hi. Listen we got a problem with Joey Tribbiani, apparently he missed his audition. Who did you speak to in my office? Estelle, no, I don't know what I'm going to do with her. No. All right, so your husband leaves and burns down the apartment, the world does not stop.'\n",
      "Speaker 1, Speaker 3: Is anybody else scared?\n",
      "Speaker 1, Speaker 2: 'Right, well look, um, if Joey loses this audition, that is it for Estelle. I don't care! Annie you are a doll, what time can you see him?' I need a pen.\n",
      "Speaker 1, Speaker 3: Get the woman a pad! Get the woman a pad! A pad! A pad!\n",
      "Speaker 1, Speaker 4: Oh, now you want a pad.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dialog': \"{speaker_y} hey!\\n{speaker_2} hey.\\n{speaker_3} hey, man. what's up?\\n{speaker_y} maybe you can tell me. my agent would like to know why i didn't show up at the audition i didn't know i had today. the first good thing she gets me in weeks. how could you not give me the message?!\\n{speaker_3} well, i'll tell ya i do enjoy guilt, but, ah, it wasn't me.\\n{speaker_2} yes, it was! it was him! uh huh! okay, it was me!\\n{speaker_y} how is it you?\\n{speaker_2} well, it was just, it was all so crazy, you know. i mean, chandler was in the closet, counting to 10, and he was up to 7 and i hadn't found a place to hide yet. i-i-i meant to tell you, and i wrote it all down on my hand. see, all of it.\\n{speaker_y} yep, that's my audition.\\n{speaker_4} see, now this is why i keep notepads everywhere.\\n{speaker_2} yep, and that's why we don't invite you to play.\\n{speaker_5} what is the great tragedy here? you go get yourself another appointment.\\n{speaker_y} well, estelle tried, you know. the casting director told her that i missed my chance.\\n{speaker_2} that is unfair. i'll call her and tell her it was totally my fault.\\n{speaker_y} pheebs, you can't do that. the casting director doesn't talk to friends, she only talks to agents.\\n{speaker_2} what a sad little life she must lead. okay, ooh.\\n{speaker_y} what, what are you doing? what are you doing?\\n{speaker_2} no, no, no, i know, i know, ooh. 'hi, this is katelynn, from phoebe buffay's office. um, is um, ann there for phoebe, she'll know what it's about.'\\n{speaker_y} hang up, hang up.\\n{speaker_2} 'annie! hi. listen we got a problem with joey tribbiani, apparently he missed his audition. who did you speak to in my office? estelle, no, i don't know what i'm going to do with her. no. all right, so your husband leaves and burns down the apartment, the world does not stop.'\\n{speaker_3} is anybody else scared?\",\n",
       " 'relation': SpeakerRelation(speaker_x='Estelle', speaker_y='{speaker_y}', rid=[7])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = gen.send(None)[1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m speaker_tokenizer \u001b[38;5;241m=\u001b[39m SpeakerBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m speaker_tokens \u001b[38;5;241m=\u001b[39m [entry[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx, entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28menumerate\u001b[39m(SPEAKER_TOKENS\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())))]\n\u001b[1;32m----> 7\u001b[0m sequence \u001b[38;5;241m=\u001b[39m speaker_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[43mtest\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialog\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m speaker_x \u001b[38;5;241m=\u001b[39m speaker_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mspeaker_x)\n\u001b[0;32m      9\u001b[0m speaker_y \u001b[38;5;241m=\u001b[39m speaker_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mspeaker_y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "from models.BERT.speaker_tokens import SPEAKER_TOKENS, SpeakerBertTokenizer\n",
    "\n",
    "n_class = 36\n",
    "speaker_tokenizer = SpeakerBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "speaker_tokens = [entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not x[1][0].startswith(\"__\") else None, enumerate(SPEAKER_TOKENS.__dict__.items())))]\n",
    "sequence = speaker_tokenizer.tokenize(test['dialog'])\n",
    "speaker_x = speaker_tokenizer.tokenize(test['relation'].speaker_x)\n",
    "speaker_y = speaker_tokenizer.tokenize(test['relation'].speaker_y)\n",
    "\n",
    "tokens = [\"[CLS]\"] + sequence + [\"[SEP]\"] + speaker_x + [\"[SEP]\"] + speaker_y + [\"[SEP]\"]\n",
    "input_ids = speaker_tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_mask = [1] + [1]*len(sequence) + [1] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [0]\n",
    "segment_ids = [0] + [0]*len(sequence) + [0] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [1]\n",
    "\n",
    "input_speaker_ids = []\n",
    "input_mention_ids = []\n",
    "current_speaker_id = 0\n",
    "current_speaker_idx = 0\n",
    "for token in sequence:\n",
    "\tif SPEAKER_TOKENS.is_speaker(token):\n",
    "\t\tcurrent_speaker_id = SPEAKER_TOKENS.convert_speaker_to_id(token)\n",
    "\t\tcurrent_speaker_idx+=1\n",
    "\tinput_speaker_ids.append(current_speaker_id)\n",
    "\tinput_mention_ids.append(current_speaker_idx)\n",
    "speaker_ids = [0] + input_speaker_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "mention_ids = [0] + input_mention_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "\n",
    "label_id = []\n",
    "for k in range(n_class):\n",
    "\tif k+1 in test['relation'].rid:\n",
    "\t\tlabel_id.append(1)\n",
    "\telse:\n",
    "\t\tlabel_id.append(0)\n",
    "\n",
    "print(f\"tokens      length: {len(tokens)}, \"+\" \".join([str(token) for token in tokens]))\n",
    "print(f\"input_ids   length: {len(input_ids)}, \"+\" \".join([str(input) for input in input_ids]))\n",
    "print(f\"input_mask  length: {len(input_mask)}, \"+\" \".join([str(mask) for mask in input_mask]))\n",
    "print(f\"segment_ids length: {len(segment_ids)}, \"+\" \".join([str(segment) for segment in segment_ids]))\n",
    "print(f\"speaker_ids length: {len(speaker_ids)}, \"+\" \".join([str(speaker) for speaker in speaker_ids]))\n",
    "print(f\"mention_ids length: {len(mention_ids)}, \"+\" \".join([str(mention) for mention in mention_ids]))\n",
    "print(f\"label_id: {label_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "label_id: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tokens        length: 512, [CLS] {speaker_x} how ##dy ! i ' m flow ##ey , flow ##ey the flower ! {speaker_y} hello flow ##ey . [SEP] {speaker_x} [SEP] {speaker_y} [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "input_ids     length: 512, 101 30531 2129 5149 999 1045 1005 1049 4834 3240 1010 4834 3240 1996 6546 999 30525 7592 4834 3240 1012 102 30531 102 30525 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "input_mask    length: 512, 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids   length: 512, 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "speaker_ids   length: 512, 0 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 10 10 10 10 10 0 9 0 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "mention_ids   length: 512, 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 0 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "turn_masks[0] length: 512, True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False\n",
      "graph:Graph(num_nodes={'node': 5},\n",
      "      num_edges={('node', 'dialog', 'node'): 4, ('node', 'entity', 'node'): 8, ('node', 'speaker', 'node'): 1},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])\n",
      "{'h': [1, 3], 't': [2, 4]}\n"
     ]
    }
   ],
   "source": [
    "from models.BERT.speaker_tokens import SPEAKER_TOKENS, SpeakerBertTokenizer\n",
    "from models.BERT.tucoregcn_bert_pytorch_processor import Conversation, SpeakerRelation, Message\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "import dgl\n",
    "\n",
    "def make_speaker_infor(speaker_id, mention_id):\n",
    "\ttmp = defaultdict(set)\n",
    "\tfor i in range(1, len(speaker_id)):\n",
    "\t\tif speaker_id[i] == 0:\n",
    "\t\t\tbreak\n",
    "\t\ttmp[speaker_id[i]].add(mention_id[i])\n",
    "\t\n",
    "\tspeaker_infor = dict()\n",
    "\tfor k, va in tmp.items():\n",
    "\t\tspeaker_infor[k] = list(va)\n",
    "\treturn speaker_infor\n",
    "\n",
    "def make_entity_edges_infor(input_ids, mention_id):\n",
    "\tentity_edges_infor = {'h':[], 't':[]}\n",
    "\thead_mention_id = max(mention_id) - 1\n",
    "\ttail_mention_id = max(mention_id)\n",
    "\thead = list()\n",
    "\ttail = list()\n",
    "\tfor i in range(len(mention_id)):\n",
    "\t\tif mention_id[i] == head_mention_id:\n",
    "\t\t\thead.append(input_ids[i])\n",
    "\n",
    "\tfor i in range(len(mention_id)):\n",
    "\t\tif mention_id[i] == tail_mention_id:\n",
    "\t\t\ttail.append(input_ids[i])\n",
    "\t\n",
    "\tfor i in range(len(input_ids) - len(head)):\n",
    "\t\tif input_ids[i:i+len(head)] == head:\n",
    "\t\t\tentity_edges_infor['h'].append(mention_id[i])\n",
    "\t\n",
    "\tfor i in range(len(input_ids) - len(tail)):\n",
    "\t\tif input_ids[i:i+len(tail)] == tail:\n",
    "\t\t\tentity_edges_infor['t'].append(mention_id[i])\n",
    "\t\n",
    "\treturn entity_edges_infor\n",
    "\n",
    "def create_graph(speaker_infor, turn_node_num, entity_edges_infor, head_mention_id, tail_mention_id):\n",
    "\td = defaultdict(list)\n",
    "\tused_mention = set()\n",
    "\n",
    "\t# add speaker edges\n",
    "\tfor _, mentions in speaker_infor.items():\n",
    "\t\tfor h, t in permutations(mentions, 2):\n",
    "\t\t\td[('node', 'speaker', 'node')].append((h, t))\n",
    "\t\t\tused_mention.add(h)\n",
    "\t\t\tused_mention.add(t)\n",
    "\t\n",
    "\tif d[('node', 'speaker', 'node')] == []:\n",
    "\t\td[('node', 'speaker', 'node')].append((1, 0))\n",
    "\t\tused_mention.add(1)\n",
    "\t\tused_mention.add(0)\n",
    "\t\n",
    "\n",
    "\t# add dialog edges\n",
    "\tfor i in range(1, turn_node_num+1):\n",
    "\t\td[('node', 'dialog', 'node')].append((i, 0))\n",
    "\t\td[('node', 'dialog', 'node')].append((0, i))\n",
    "\t\tused_mention.add(i)\n",
    "\t\tused_mention.add(0)\n",
    "\tif d[('node', 'dialog', 'node')] == []:\n",
    "\t\td[('node', 'dialog', 'node')].append((1, 0))\n",
    "\t\tused_mention.add(1)\n",
    "\t\tused_mention.add(0)\n",
    "\n",
    "\t# add entity edges\n",
    "\tfor mention in entity_edges_infor['h']:\n",
    "\t\td[('node', 'entity', 'node')].append((head_mention_id, mention))\n",
    "\t\td[('node', 'entity', 'node')].append((mention, head_mention_id))\n",
    "\t\tused_mention.add(head_mention_id)\n",
    "\t\tused_mention.add(mention)\n",
    "\t\n",
    "\tfor mention in entity_edges_infor['t']:\n",
    "\t\td[('node', 'entity', 'node')].append((tail_mention_id, mention))\n",
    "\t\td[('node', 'entity', 'node')].append((mention, tail_mention_id))\n",
    "\t\tused_mention.add(tail_mention_id)\n",
    "\t\tused_mention.add(mention)\n",
    "\t\n",
    "\tif entity_edges_infor['h'] == []:\n",
    "\t\td[('node', 'entity', 'node')].append((head_mention_id, 0))\n",
    "\t\td[('node', 'entity', 'node')].append((0, head_mention_id))\n",
    "\t\tused_mention.add(head_mention_id)\n",
    "\t\tused_mention.add(0)\n",
    "\t\n",
    "\tif entity_edges_infor['t'] == []:\n",
    "\t\td[('node', 'entity', 'node')].append((tail_mention_id, 0))\n",
    "\t\td[('node', 'entity', 'node')].append((0, tail_mention_id))\n",
    "\t\tused_mention.add(tail_mention_id)\n",
    "\t\tused_mention.add(0)\n",
    "\n",
    "\tgraph = dgl.heterograph(d)\n",
    "\n",
    "\treturn graph, used_mention\n",
    "\n",
    "\n",
    "def mention2mask(mention_id, old_behaviour=False):\n",
    "    slen = len(mention_id)\n",
    "    mask = []\n",
    "    if old_behaviour:\n",
    "        turn_mention_ids = [i for i in range(1, np.max(mention_id)-1)] #-1\n",
    "    else:\n",
    "        turn_mention_ids = [i for i in range(1, np.max(mention_id)+1)] #-1\n",
    "    print(turn_mention_ids)\n",
    "    for j in range(slen):\n",
    "        tmp = None\n",
    "        if mention_id[j] not in turn_mention_ids:\n",
    "            tmp = np.zeros(slen, dtype=bool)\n",
    "            if old_behaviour:\n",
    "                tmp[j] = 1\n",
    "        else:\n",
    "            start = mention_id[j]\n",
    "            end = mention_id[j]\n",
    "            if mention_id[j] - 1 in turn_mention_ids:\n",
    "                start = mention_id[j] - 1\n",
    "\n",
    "            if mention_id[j] + 1 in turn_mention_ids:\n",
    "                end = mention_id[j] + 1\n",
    "            \n",
    "            tmp = (mention_id >= start) & (mention_id <= end)\n",
    "        mask.append(tmp)\n",
    "    mask = np.stack(mask)\n",
    "    return mask\n",
    "\n",
    "n_class = 36\n",
    "max_seq_length=512\n",
    "speaker_tokenizer = SpeakerBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "c = Conversation(\n",
    "\tmessages=[\n",
    "\t\tMessage(\"Speaker 1\", \"Howdy! I'm Flowey, Flowey the Flower!\"),\n",
    "\t\tMessage(\"Speaker 2\", \"Hello Flowey.\"),\n",
    "\t],\n",
    "\tspeaker_relations=[\n",
    "\t\tSpeakerRelation(\"Speaker 1\", \"Speaker 2\", [3])\n",
    "\t]\n",
    ")\n",
    "old_behaviour = True\n",
    "test2 = c.build_inputs(speaker_tokenizer)[0]\n",
    "\n",
    "speaker_tokens = [entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not x[1][0].startswith(\"__\") else None, enumerate(SPEAKER_TOKENS.__dict__.items())))]\n",
    "sequence = speaker_tokenizer.tokenize(test2['dialog'])\n",
    "speaker_x = speaker_tokenizer.tokenize(test2['relation'].speaker_x)\n",
    "speaker_y = speaker_tokenizer.tokenize(test2['relation'].speaker_y)\n",
    "\n",
    "tokens = [\"[CLS]\"] + sequence + [\"[SEP]\"] + speaker_x + [\"[SEP]\"] + speaker_y + [\"[SEP]\"]\n",
    "input_ids = speaker_tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_mask = [1] + [1]*len(sequence) + [1] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [0]\n",
    "segment_ids = [0] + [0]*len(sequence) + [0] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [1]\n",
    "\n",
    "input_speaker_ids = []\n",
    "input_mention_ids = []\n",
    "current_speaker_id = 0\n",
    "current_speaker_idx = 0\n",
    "for token in sequence:\n",
    "\tif SPEAKER_TOKENS.is_speaker(token):\n",
    "\t\tcurrent_speaker_id = SPEAKER_TOKENS.convert_speaker_to_id(token)\n",
    "\t\tcurrent_speaker_idx+=1\n",
    "\tinput_speaker_ids.append(current_speaker_id)\n",
    "\tinput_mention_ids.append(current_speaker_idx)\n",
    "if old_behaviour:\n",
    "\tspeaker_ids = [0] + input_speaker_ids + [0] + [SPEAKER_TOKENS.convert_speaker_to_id(test2['relation'].speaker_x)]*len(speaker_x) + [0] + [SPEAKER_TOKENS.convert_speaker_to_id(test2['relation'].speaker_y)]*len(speaker_y) + [0]\n",
    "\tmention_ids = [0] + input_mention_ids + [0] + [current_speaker_idx+1]*len(speaker_x) + [0] + [current_speaker_idx+2]*len(speaker_y) + [0]\n",
    "else:\n",
    "\tspeaker_ids = [0] + input_speaker_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "\tmention_ids = [0] + input_mention_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "label_id = []\n",
    "for k in range(n_class):\n",
    "\tif k+1 in test2['relation'].rid:\n",
    "\t\tlabel_id.append(1)\n",
    "\telse:\n",
    "\t\tlabel_id.append(0)\n",
    "\n",
    "while len(input_ids) < max_seq_length:\n",
    "\ttokens.append('[PAD]')\n",
    "\tinput_ids.append(0)\n",
    "\tinput_mask.append(0)\n",
    "\tsegment_ids.append(0)\n",
    "\tspeaker_ids.append(0)\n",
    "\tmention_ids.append(0)\n",
    "\n",
    "turn_masks = mention2mask(np.array(mention_ids),old_behaviour)\n",
    "\n",
    "speaker_infor = make_speaker_infor(speaker_ids, mention_ids)\n",
    "turn_node_num = max(mention_ids) - 2\n",
    "head_mention_id = max(mention_ids) - 1\n",
    "tail_mention_id = max(mention_ids)\n",
    "entity_edges_infor = make_entity_edges_infor(input_ids, mention_ids)\n",
    "graph, used_mention = create_graph(speaker_infor, turn_node_num, entity_edges_infor, head_mention_id, tail_mention_id)\n",
    "assert len(used_mention) == (max(mention_ids) + 1)\n",
    "\n",
    "print(f\"label_id: {label_id}\")\n",
    "print(f\"tokens        length: {len(tokens)}, \"+\" \".join([str(token) for token in tokens]))\n",
    "print(f\"input_ids     length: {len(input_ids)}, \"+\" \".join([str(input) for input in input_ids]))\n",
    "print(f\"input_mask    length: {len(input_mask)}, \"+\" \".join([str(mask) for mask in input_mask]))\n",
    "print(f\"segment_ids   length: {len(segment_ids)}, \"+\" \".join([str(segment) for segment in segment_ids]))\n",
    "print(f\"speaker_ids   length: {len(speaker_ids)}, \"+\" \".join([str(speaker) for speaker in speaker_ids]))\n",
    "print(f\"mention_ids   length: {len(mention_ids)}, \"+\" \".join([str(mention) for mention in mention_ids]))\n",
    "print(f\"turn_masks[0] length: {len(turn_masks)}, \"+\" \".join([str(turn_mask) for turn_mask in turn_masks[0]]))\n",
    "print(f\"graph:\"+str(graph))\n",
    "print(entity_edges_infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{speaker_x}'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{speaker_1}',\n",
       " '{speaker_2}',\n",
       " '{speaker_3}',\n",
       " '{speaker_4}',\n",
       " '{speaker_5}',\n",
       " '{speaker_6}',\n",
       " '{speaker_7}',\n",
       " '{speaker_8}',\n",
       " '{speaker_9}',\n",
       " '{speaker_x}',\n",
       " '{speaker_y}',\n",
       " <function models.BERT.speaker_tokens.SPEAKER_TOKENS.is_speaker(token)>,\n",
       " <function models.BERT.speaker_tokens.SPEAKER_TOKENS.convert_speaker_to_id(token)>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not x[1][0].startswith(\"__\") else None, enumerate(SPEAKER_TOKENS.__dict__.items())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not (x[1][0].startswith(\"__\") or type(x[1][1])!=str) else None, enumerate(SPEAKER_TOKENS.__dict__.items())))].index(\"{speaker_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(SPEAKER_TOKENS.__dict__.items())[12][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
