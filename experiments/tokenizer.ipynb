{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import add_path\n",
    "import datasets\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BERT.tucoregcn_bert_pytorch_processor import DialogRE\n",
    "dev = DialogRE()\n",
    "gen = dev._generate_examples(\"../datasets/DialogRE/dev.json\", \"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeakerRelation(speaker_x='Estelle', speaker_y='Speaker 1', rid=[7])\n",
      "Speaker 1, {speaker_y} Hey!\n",
      "Speaker 1, Speaker 2: Hey.\n",
      "Speaker 1, Speaker 3: Hey, man. What's up?\n",
      "Speaker 1, {speaker_y} Maybe you can tell me. My agent would like to know why I didn't show up at the audition I didn't know I had today. The first good thing she gets me in weeks. How could you not give me the message?!\n",
      "Speaker 1, Speaker 3: Well, I'll tell ya I do enjoy guilt, but, ah, it wasn't me.\n",
      "Speaker 1, Speaker 2: Yes, it was! It was him! Uh huh! Okay, it was me!\n",
      "Speaker 1, {speaker_y} How is it you?\n",
      "Speaker 1, Speaker 2: Well, it was just, it was all so crazy, you know. I mean, Chandler was in the closet, counting to 10, and he was up to 7 and I hadn't found a place to hide yet. I-I-I meant to tell you, and I wrote it all down on my hand. See, all of it.\n",
      "Speaker 1, {speaker_y} Yep, that's my audition.\n",
      "Speaker 1, Speaker 4: See, now this is why I keep notepads everywhere.\n",
      "Speaker 1, Speaker 2: Yep, and that's why we don't invite you to play.\n",
      "Speaker 1, Speaker 5: What is the great tragedy here? You go get yourself another appointment.\n",
      "Speaker 1, {speaker_y} Well, Estelle tried, you know. The casting director told her that I missed my chance.\n",
      "Speaker 1, Speaker 2: That is unfair. I'll call her and tell her it was totally my fault.\n",
      "Speaker 1, {speaker_y} Pheebs, you can't do that. The casting director doesn't talk to friends, she only talks to agents.\n",
      "Speaker 1, Speaker 2: What a sad little life she must lead. Okay, ooh.\n",
      "Speaker 1, {speaker_y} What, what are you doing? What are you doing?\n",
      "Speaker 1, Speaker 2: No, no, no, I know, I know, ooh. 'Hi, this is Katelynn, from Phoebe Buffay's office. Um, is um, Ann there for Phoebe, she'll know what it's about.'\n",
      "Speaker 1, {speaker_y} Hang up, hang up.\n",
      "Speaker 1, Speaker 2: 'Annie! Hi. Listen we got a problem with Joey Tribbiani, apparently he missed his audition. Who did you speak to in my office? Estelle, no, I don't know what I'm going to do with her. No. All right, so your husband leaves and burns down the apartment, the world does not stop.'\n",
      "Speaker 1, Speaker 3: Is anybody else scared?\n",
      "Speaker 1, Speaker 2: 'Right, well look, um, if Joey loses this audition, that is it for Estelle. I don't care! Annie you are a doll, what time can you see him?' I need a pen.\n",
      "Speaker 1, Speaker 3: Get the woman a pad! Get the woman a pad! A pad! A pad!\n",
      "Speaker 1, Speaker 4: Oh, now you want a pad.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dialog': \"{speaker_y} hey!\\n{speaker_2} hey.\\n{speaker_3} hey, man. what's up?\\n{speaker_y} maybe you can tell me. my agent would like to know why i didn't show up at the audition i didn't know i had today. the first good thing she gets me in weeks. how could you not give me the message?!\\n{speaker_3} well, i'll tell ya i do enjoy guilt, but, ah, it wasn't me.\\n{speaker_2} yes, it was! it was him! uh huh! okay, it was me!\\n{speaker_y} how is it you?\\n{speaker_2} well, it was just, it was all so crazy, you know. i mean, chandler was in the closet, counting to 10, and he was up to 7 and i hadn't found a place to hide yet. i-i-i meant to tell you, and i wrote it all down on my hand. see, all of it.\\n{speaker_y} yep, that's my audition.\\n{speaker_4} see, now this is why i keep notepads everywhere.\\n{speaker_2} yep, and that's why we don't invite you to play.\\n{speaker_5} what is the great tragedy here? you go get yourself another appointment.\\n{speaker_y} well, estelle tried, you know. the casting director told her that i missed my chance.\\n{speaker_2} that is unfair. i'll call her and tell her it was totally my fault.\\n{speaker_y} pheebs, you can't do that. the casting director doesn't talk to friends, she only talks to agents.\\n{speaker_2} what a sad little life she must lead. okay, ooh.\\n{speaker_y} what, what are you doing? what are you doing?\\n{speaker_2} no, no, no, i know, i know, ooh. 'hi, this is katelynn, from phoebe buffay's office. um, is um, ann there for phoebe, she'll know what it's about.'\\n{speaker_y} hang up, hang up.\\n{speaker_2} 'annie! hi. listen we got a problem with joey tribbiani, apparently he missed his audition. who did you speak to in my office? estelle, no, i don't know what i'm going to do with her. no. all right, so your husband leaves and burns down the apartment, the world does not stop.'\\n{speaker_3} is anybody else scared?\",\n",
       " 'relation': SpeakerRelation(speaker_x='Estelle', speaker_y='{speaker_y}', rid=[7])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = gen.send(None)[1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens      length: 487, [CLS] {speaker_y} hey ! {speaker_2} hey . {speaker_3} hey , man . what ' s up ? {speaker_y} maybe you can tell me . my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today . the first good thing she gets me in weeks . how could you not give me the message ? ! {speaker_3} well , i ' ll tell ya i do enjoy guilt , but , ah , it wasn ' t me . {speaker_2} yes , it was ! it was him ! uh huh ! okay , it was me ! {speaker_y} how is it you ? {speaker_2} well , it was just , it was all so crazy , you know . i mean , chandler was in the closet , counting to 10 , and he was up to 7 and i hadn ' t found a place to hide yet . i - i - i meant to tell you , and i wrote it all down on my hand . see , all of it . {speaker_y} yep , that ' s my audition . {speaker_4} see , now this is why i keep note ##pad ##s everywhere . {speaker_2} yep , and that ' s why we don ' t invite you to play . {speaker_5} what is the great tragedy here ? you go get yourself another appointment . {speaker_y} well , este ##lle tried , you know . the casting director told her that i missed my chance . {speaker_2} that is unfair . i ' ll call her and tell her it was totally my fault . {speaker_y} ph ##ee ##bs , you can ' t do that . the casting director doesn ' t talk to friends , she only talks to agents . {speaker_2} what a sad little life she must lead . okay , o ##oh . {speaker_y} what , what are you doing ? what are you doing ? {speaker_2} no , no , no , i know , i know , o ##oh . ' hi , this is kate ##lynn , from phoebe buff ##ay ' s office . um , is um , ann there for phoebe , she ' ll know what it ' s about . ' {speaker_y} hang up , hang up . {speaker_2} ' annie ! hi . listen we got a problem with joey tri ##bb ##iani , apparently he missed his audition . who did you speak to in my office ? este ##lle , no , i don ' t know what i ' m going to do with her . no . all right , so your husband leaves and burns down the apartment , the world does not stop . ' {speaker_3} is anybody else scared ? [SEP] este ##lle [SEP] {speaker_y} [SEP]\n",
      "input_ids   length: 487, 101 30525 4931 999 30526 4931 1012 30522 4931 1010 2158 1012 2054 1005 1055 2039 1029 30525 2672 2017 2064 2425 2033 1012 2026 4005 2052 2066 2000 2113 2339 1045 2134 1005 1056 2265 2039 2012 1996 14597 1045 2134 1005 1056 2113 1045 2018 2651 1012 1996 2034 2204 2518 2016 4152 2033 1999 3134 1012 2129 2071 2017 2025 2507 2033 1996 4471 1029 999 30522 2092 1010 1045 1005 2222 2425 8038 1045 2079 5959 8056 1010 2021 1010 6289 1010 2009 2347 1005 1056 2033 1012 30526 2748 1010 2009 2001 999 2009 2001 2032 999 7910 9616 999 3100 1010 2009 2001 2033 999 30525 2129 2003 2009 2017 1029 30526 2092 1010 2009 2001 2074 1010 2009 2001 2035 2061 4689 1010 2017 2113 1012 1045 2812 1010 13814 2001 1999 1996 9346 1010 10320 2000 2184 1010 1998 2002 2001 2039 2000 1021 1998 1045 2910 1005 1056 2179 1037 2173 2000 5342 2664 1012 1045 1011 1045 1011 1045 3214 2000 2425 2017 1010 1998 1045 2626 2009 2035 2091 2006 2026 2192 1012 2156 1010 2035 1997 2009 1012 30525 15624 1010 2008 1005 1055 2026 14597 1012 30523 2156 1010 2085 2023 2003 2339 1045 2562 3602 15455 2015 7249 1012 30526 15624 1010 1998 2008 1005 1055 2339 2057 2123 1005 1056 13260 2017 2000 2377 1012 30528 2054 2003 1996 2307 10576 2182 1029 2017 2175 2131 4426 2178 6098 1012 30525 2092 1010 28517 6216 2699 1010 2017 2113 1012 1996 9179 2472 2409 2014 2008 1045 4771 2026 3382 1012 30526 2008 2003 15571 1012 1045 1005 2222 2655 2014 1998 2425 2014 2009 2001 6135 2026 6346 1012 30525 6887 4402 5910 1010 2017 2064 1005 1056 2079 2008 1012 1996 9179 2472 2987 1005 1056 2831 2000 2814 1010 2016 2069 7566 2000 6074 1012 30526 2054 1037 6517 2210 2166 2016 2442 2599 1012 3100 1010 1051 11631 1012 30525 2054 1010 2054 2024 2017 2725 1029 2054 2024 2017 2725 1029 30526 2053 1010 2053 1010 2053 1010 1045 2113 1010 1045 2113 1010 1051 11631 1012 1005 7632 1010 2023 2003 5736 27610 1010 2013 18188 23176 4710 1005 1055 2436 1012 8529 1010 2003 8529 1010 5754 2045 2005 18188 1010 2016 1005 2222 2113 2054 2009 1005 1055 2055 1012 1005 30525 6865 2039 1010 6865 2039 1012 30526 1005 8194 999 7632 1012 4952 2057 2288 1037 3291 2007 9558 13012 10322 25443 1010 4593 2002 4771 2010 14597 1012 2040 2106 2017 3713 2000 1999 2026 2436 1029 28517 6216 1010 2053 1010 1045 2123 1005 1056 2113 2054 1045 1005 1049 2183 2000 2079 2007 2014 1012 2053 1012 2035 2157 1010 2061 2115 3129 3727 1998 7641 2091 1996 4545 1010 1996 2088 2515 2025 2644 1012 1005 30522 2003 10334 2842 6015 1029 102 28517 6216 102 30525 102\n",
      "input_mask  length: 487, 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "segment_ids length: 487, 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      "speaker_ids length: 487, 0 10 10 10 1 1 1 2 2 2 2 2 2 2 2 2 2 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 10 10 10 10 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 10 10 10 10 10 10 10 10 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 10 10 10 10 10 10 10 10 10 10 10 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 10 10 10 10 10 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 0 0 0 0 0 0\n",
      "mention_ids length: 487, 0 1 1 1 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 21 21 21 21 21 21 0 0 0 0 0 0\n",
      "label_id: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from models.BERT.speaker_tokens import SPEAKER_TOKENS, SpeakerBertTokenizer\n",
    "\n",
    "n_class = 36\n",
    "speaker_tokenizer = SpeakerBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "speaker_tokens = [entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not x[1][0].startswith(\"__\") else None, enumerate(SPEAKER_TOKENS.__dict__.items())))]\n",
    "sequence = speaker_tokenizer.tokenize(test['dialog'])\n",
    "speaker_x = speaker_tokenizer.tokenize(test['relation'].speaker_x)\n",
    "speaker_y = speaker_tokenizer.tokenize(test['relation'].speaker_y)\n",
    "\n",
    "tokens = [\"[CLS]\"] + sequence + [\"[SEP]\"] + speaker_x + [\"[SEP]\"] + speaker_y + [\"[SEP]\"]\n",
    "input_ids = speaker_tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_mask = [1] + [1]*len(sequence) + [1] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [0]\n",
    "segment_ids = [0] + [0]*len(sequence) + [0] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [1]\n",
    "\n",
    "input_speaker_ids = []\n",
    "input_mention_ids = []\n",
    "current_speaker_id = 0\n",
    "current_speaker_idx = 0\n",
    "for token in sequence:\n",
    "\tif SPEAKER_TOKENS.is_speaker(token):\n",
    "\t\tcurrent_speaker_id = SPEAKER_TOKENS.convert_speaker_to_id(token)\n",
    "\t\tcurrent_speaker_idx+=1\n",
    "\tinput_speaker_ids.append(current_speaker_id)\n",
    "\tinput_mention_ids.append(current_speaker_idx)\n",
    "speaker_ids = [0] + input_speaker_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "mention_ids = [0] + input_mention_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "\n",
    "label_id = []\n",
    "for k in range(n_class):\n",
    "\tif k+1 in test['relation'].rid:\n",
    "\t\tlabel_id.append(1)\n",
    "\telse:\n",
    "\t\tlabel_id.append(0)\n",
    "\n",
    "print(f\"tokens      length: {len(tokens)}, \"+\" \".join([str(token) for token in tokens]))\n",
    "print(f\"input_ids   length: {len(input_ids)}, \"+\" \".join([str(input) for input in input_ids]))\n",
    "print(f\"input_mask  length: {len(input_mask)}, \"+\" \".join([str(mask) for mask in input_mask]))\n",
    "print(f\"segment_ids length: {len(segment_ids)}, \"+\" \".join([str(segment) for segment in segment_ids]))\n",
    "print(f\"speaker_ids length: {len(speaker_ids)}, \"+\" \".join([str(speaker) for speaker in speaker_ids]))\n",
    "print(f\"mention_ids length: {len(mention_ids)}, \"+\" \".join([str(mention) for mention in mention_ids]))\n",
    "print(f\"label_id: {label_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1, {speaker_x} Howdy! I'm Flowey, Flowey the Flower!\n",
      "Speaker 2, {speaker_x} Howdy! I'm Flowey, Flowey the Flower!\n",
      "Speaker 1, Speaker 2: Hello Flowey.\n",
      "Speaker 2, {speaker_y} Hello Flowey.\n",
      "tokens      length: 26, [CLS] {speaker_x} how ##dy ! i ' m flow ##ey , flow ##ey the flower ! {speaker_y} hello flow ##ey . [SEP] {speaker_x} [SEP] {speaker_y} [SEP]\n",
      "input_ids   length: 26, 101 30531 2129 5149 999 1045 1005 1049 4834 3240 1010 4834 3240 1996 6546 999 30525 7592 4834 3240 1012 102 30531 102 30525 102\n",
      "input_mask  length: 26, 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "segment_ids length: 26, 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      "speaker_ids length: 26, 0 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 10 10 10 10 10 0 0 0 0 0\n",
      "mention_ids length: 26, 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 0 0 0 0 0\n",
      "label_id: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from models.BERT.speaker_tokens import SPEAKER_TOKENS, SpeakerBertTokenizer\n",
    "from models.BERT.tucoregcn_bert_pytorch_processor import Conversation, SpeakerRelation, Message\n",
    "\n",
    "n_class = 36\n",
    "speaker_tokenizer = SpeakerBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "c = Conversation(\n",
    "\tmessages=[\n",
    "\t\tMessage(\"Speaker 1\", \"Howdy! I'm Flowey, Flowey the Flower!\"),\n",
    "\t\tMessage(\"Speaker 2\", \"Hello Flowey.\"),\n",
    "\t],\n",
    "\tspeaker_relations=[\n",
    "\t\tSpeakerRelation(\"Speaker 1\", \"Speaker 2\", [3])\n",
    "\t]\n",
    ")\n",
    "test2 = c.build_inputs(speaker_tokenizer)[0]\n",
    "\n",
    "speaker_tokens = [entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not x[1][0].startswith(\"__\") else None, enumerate(SPEAKER_TOKENS.__dict__.items())))]\n",
    "sequence = speaker_tokenizer.tokenize(test2['dialog'])\n",
    "speaker_x = speaker_tokenizer.tokenize(test2['relation'].speaker_x)\n",
    "speaker_y = speaker_tokenizer.tokenize(test2['relation'].speaker_y)\n",
    "\n",
    "tokens = [\"[CLS]\"] + sequence + [\"[SEP]\"] + speaker_x + [\"[SEP]\"] + speaker_y + [\"[SEP]\"]\n",
    "input_ids = speaker_tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_mask = [1] + [1]*len(sequence) + [1] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [0]\n",
    "segment_ids = [0] + [0]*len(sequence) + [0] + [1]*len(speaker_x) + [1] + [1]*len(speaker_y) + [1]\n",
    "\n",
    "input_speaker_ids = []\n",
    "input_mention_ids = []\n",
    "current_speaker_id = 0\n",
    "current_speaker_idx = 0\n",
    "for token in sequence:\n",
    "\tif SPEAKER_TOKENS.is_speaker(token):\n",
    "\t\tcurrent_speaker_id = SPEAKER_TOKENS.convert_speaker_to_id(token)\n",
    "\t\tcurrent_speaker_idx+=1\n",
    "\tinput_speaker_ids.append(current_speaker_id)\n",
    "\tinput_mention_ids.append(current_speaker_idx)\n",
    "speaker_ids = [0] + input_speaker_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "mention_ids = [0] + input_mention_ids + [0] + [0]*len(speaker_x) + [0] + [0]*len(speaker_y) + [0]\n",
    "\n",
    "label_id = []\n",
    "for k in range(n_class):\n",
    "\tif k+1 in test2['relation'].rid:\n",
    "\t\tlabel_id.append(1)\n",
    "\telse:\n",
    "\t\tlabel_id.append(0)\n",
    "\n",
    "print(f\"tokens      length: {len(tokens)}, \"+\" \".join([str(token) for token in tokens]))\n",
    "print(f\"input_ids   length: {len(input_ids)}, \"+\" \".join([str(input) for input in input_ids]))\n",
    "print(f\"input_mask  length: {len(input_mask)}, \"+\" \".join([str(mask) for mask in input_mask]))\n",
    "print(f\"segment_ids length: {len(segment_ids)}, \"+\" \".join([str(segment) for segment in segment_ids]))\n",
    "print(f\"speaker_ids length: {len(speaker_ids)}, \"+\" \".join([str(speaker) for speaker in speaker_ids]))\n",
    "print(f\"mention_ids length: {len(mention_ids)}, \"+\" \".join([str(mention) for mention in mention_ids]))\n",
    "print(f\"label_id: {label_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{speaker_x} howdy! i'm flowey, flowey the flower!\\n{speaker_y} hello flowey.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2['dialog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeakerRelation(speaker_x='{speaker_x}', speaker_y='{speaker_y}', rid=[3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2['relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entry[1] for idx, entry in list(filter(lambda x: x[1][1] if not (x[1][0].startswith(\"__\") or type(x[1][1])!=str) else None, enumerate(SPEAKER_TOKENS.__dict__.items())))].index(\"{speaker_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(SPEAKER_TOKENS.__dict__.items())[12][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
