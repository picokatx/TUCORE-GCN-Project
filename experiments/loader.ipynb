{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import add_path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\dgl\\dgl.dll\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BertForSequenceClassification(\n",
       "  (tucoregcn_bert): TUCOREGCN_Bert(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (speaker_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (turnAttention): MultiHeadAttention(\n",
       "      (w_qs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_ks): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_vs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (out_lin): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (GCN_layers): ModuleList(\n",
       "      (0-1): 2 x RelGraphConvLayer(\n",
       "        (activation): ReLU()\n",
       "        (conv): HeteroGraphConv(\n",
       "          (mods): ModuleDict(\n",
       "            (speaker): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (dialog): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (entity): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.6, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (LSTM_layers): ModuleList(\n",
       "      (0-1): 2 x TurnLevelLSTM(\n",
       "        (lstm): LSTM(768, 768, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (bilstm2hiddnesize): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=6912, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from models.BERT.tucoregcn_bert_pytorch import TUCOREGCN_BertForSequenceClassification, TUCOREGCN_BertConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tucore_gcn_bert = TUCOREGCN_BertForSequenceClassification(TUCOREGCN_BertConfig.from_json_file(\"../models/BERT/tucoregcn_bert_mlc.json\"))\n",
    "tucore_gcn_bert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BertForSequenceClassification(\n",
       "  (tucoregcn_bert): TUCOREGCN_Bert(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (speaker_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (turnAttention): MultiHeadAttention(\n",
       "      (w_qs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_ks): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_vs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (out_lin): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (GCN_layers): ModuleList(\n",
       "      (0-1): 2 x RelGraphConvLayer(\n",
       "        (activation): ReLU()\n",
       "        (conv): HeteroGraphConv(\n",
       "          (mods): ModuleDict(\n",
       "            (speaker): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (dialog): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (entity): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.6, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (LSTM_layers): ModuleList(\n",
       "      (0-1): 2 x TurnLevelLSTM(\n",
       "        (lstm): LSTM(768, 768, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (bilstm2hiddnesize): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=6912, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tucore_gcn_bert.load_state_dict(torch.load(\"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\", map_location=\"cuda\"))\n",
    "tucore_gcn_bert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../datasets/DialogRE.\n",
      "load preprocessed data from ../datasets/DialogRE/dev_BERT.pkl.\n"
     ]
    }
   ],
   "source": [
    "from data import TUCOREGCNDataset, TUCOREGCNDataloader\n",
    "dev_set = TUCOREGCNDataset(src_file=\"../datasets/DialogRE\", save_file=\"../datasets/DialogRE/dev_BERT.pkl\", max_seq_length=512, tokenizer=tokenizer, n_class=36, encoder_type='BERT')\n",
    "dev_loader = TUCOREGCNDataloader(dataset=dev_set, batch_size=2, shuffle=False, relation_num=36, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, batch = next(enumerate(dev_loader))\n",
    "entry_idx = 0\n",
    "input_ids = batch[\"input_ids\"].to(\"cuda:0\")\n",
    "segment_ids = batch[\"segment_ids\"].to(\"cuda:0\")\n",
    "input_mask = batch[\"input_masks\"].to(\"cuda:0\")\n",
    "speaker_ids = batch[\"speaker_ids\"].to(\"cuda:0\")\n",
    "graphs = batch[\"graphs\"]\n",
    "mention_id = batch[\"mention_ids\"].to(\"cuda:0\")\n",
    "turn_mask = batch[\"turn_masks\"].to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([2, 512]), 'token_type_ids': torch.Size([2, 512]), 'attention_mask': torch.Size([2, 512]), 'speaker_ids': torch.Size([2, 512]), 'graphs': [Graph(num_nodes={'node': 23},\n",
      "      num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 24, ('node', 'speaker', 'node'): 114},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')]), Graph(num_nodes={'node': 23},\n",
      "      num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 10, ('node', 'speaker', 'node'): 114},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])], 'mention_ids': torch.Size([2, 512]), 'turn_mask': torch.Size([2, 512, 512])}\n",
      "{'input_ids': tensor([[ 101,    3, 1024,  ...,  102,    3,  102],\n",
      "        [ 101, 5882, 1015,  ...,  102, 4005,  102]], device='cuda:0'), 'token_type_ids': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0'), 'speaker_ids': tensor([[ 0, 12, 12,  ...,  0, 12,  0],\n",
      "        [ 0,  1,  1,  ...,  0,  0,  0]], device='cuda:0'), 'graphs': [Graph(num_nodes={'node': 23},\n",
      "      num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 24, ('node', 'speaker', 'node'): 114},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')]), Graph(num_nodes={'node': 23},\n",
      "      num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 10, ('node', 'speaker', 'node'): 114},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])], 'mention_ids': tensor([[ 0,  1,  1,  ...,  0, 22,  0],\n",
      "        [ 0,  1,  1,  ...,  0, 22,  0]], device='cuda:0'), 'turn_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]]], device='cuda:0')}\n",
      "tensor([[ 101,    3, 1024,  ...,  102,    3,  102],\n",
      "        [ 101, 5882, 1015,  ...,  102, 4005,  102]], device='cuda:0')\n",
      "--------------------------------------------\n",
      "Embedding(30522, 768, padding_idx=0)\n",
      "tensor([[ 101,    3, 1024,  ...,  102,    3,  102],\n",
      "        [ 101, 5882, 1015,  ...,  102, 4005,  102]], device='cuda:0')\n",
      "tensor([[[ 0.0014, -0.0219, -0.0065,  ..., -0.0128,  0.0082,  0.0029],\n",
      "         [-0.0044, -0.0150, -0.0468,  ..., -0.0415, -0.0076, -0.0100],\n",
      "         [-0.0779, -0.0012, -0.0039,  ...,  0.0356, -0.0328, -0.0579],\n",
      "         ...,\n",
      "         [ 0.0293, -0.0381, -0.0373,  ...,  0.0112,  0.0277,  0.0164],\n",
      "         [-0.0044, -0.0150, -0.0468,  ..., -0.0415, -0.0076, -0.0100],\n",
      "         [ 0.0293, -0.0381, -0.0373,  ...,  0.0112,  0.0277,  0.0164]],\n",
      "\n",
      "        [[ 0.0014, -0.0219, -0.0065,  ..., -0.0128,  0.0082,  0.0029],\n",
      "         [-0.0309, -0.0126, -0.0756,  ..., -0.0587,  0.0371,  0.0649],\n",
      "         [ 0.0620,  0.0342, -0.0009,  ..., -0.0185, -0.0228, -0.0308],\n",
      "         ...,\n",
      "         [ 0.0293, -0.0381, -0.0373,  ...,  0.0112,  0.0277,  0.0164],\n",
      "         [-0.0100, -0.0251,  0.0011,  ...,  0.0114,  0.0014, -0.0043],\n",
      "         [ 0.0293, -0.0381, -0.0373,  ...,  0.0112,  0.0277,  0.0164]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "--------------------------------------------\n",
      "{'embedding_output': torch.Size([2, 512, 768]), 'v': device(type='cuda', index=0), 'extended_attention_mask': torch.Size([2, 1, 1, 512]), 'head_mask': 12}\n",
      "{'embedding_output': tensor([ 3.6673e-02, -0.0000e+00, -3.5258e-02,  2.4717e-01, -5.7690e-02,\n",
      "        -9.3414e-02,  1.9168e-01, -0.0000e+00, -2.4750e-01, -7.9356e-02,\n",
      "         3.4935e-02, -6.9908e-02, -1.2963e-01, -1.3736e-01,  1.2290e-01,\n",
      "        -1.9656e-01, -9.1912e-02, -2.1030e-02,  5.3281e-01,  2.0914e-01,\n",
      "        -1.4777e-01, -2.6016e-01,  3.7708e-02, -3.4372e-01,  3.4917e-01,\n",
      "        -5.7217e-01, -5.0762e-01, -4.6619e-01, -0.0000e+00,  9.5567e-02,\n",
      "         6.4346e-02, -4.8730e-01, -0.0000e+00, -4.3523e-01, -2.9832e-01,\n",
      "        -1.5287e-01,  3.6939e-02, -1.4034e-03,  4.7887e-01,  4.1623e-02,\n",
      "        -3.8560e-01, -6.7123e-02, -2.7984e-01,  1.0240e-01,  3.5605e-01,\n",
      "         9.9036e-03,  1.3125e-01,  1.5932e-02, -4.4216e-01,  1.2226e-01,\n",
      "        -5.5556e-01, -2.0888e-01, -3.5134e-02,  3.4076e-02,  1.7256e-01,\n",
      "        -3.3335e-01, -1.0523e-01, -2.6584e-01,  3.8233e-01, -4.6806e-01,\n",
      "         5.3670e-01,  3.0179e-01,  2.7513e-01, -1.0629e-01, -1.9807e-01,\n",
      "        -5.2946e-02,  4.8496e-02,  6.7846e-02, -0.0000e+00, -4.0891e-01,\n",
      "         8.6273e-02,  2.7979e-01, -0.0000e+00,  6.3168e-02, -1.7343e-01,\n",
      "         5.3385e-01,  2.5156e-01, -1.9124e-02,  2.2393e-01,  0.0000e+00,\n",
      "        -2.1094e-01, -0.0000e+00,  3.0266e-02,  3.3922e-01, -3.7089e-01,\n",
      "         3.1788e-01,  2.6748e-02, -0.0000e+00,  2.6652e-01,  5.8362e-02,\n",
      "         6.7077e-01, -2.4580e-01, -7.0559e-02,  3.0124e-01,  9.8481e-02,\n",
      "        -4.5312e-01, -4.1149e-01, -1.2404e-01,  3.2690e-02,  1.9237e-01,\n",
      "        -1.2532e-01, -0.0000e+00, -6.2812e-01, -5.1166e-01, -4.9939e-01,\n",
      "        -1.9785e-01, -3.9101e-01,  3.5958e-01,  1.1876e-01, -3.2260e-01,\n",
      "        -1.6596e-01,  2.3744e-01,  1.8846e-01,  4.7892e-01, -1.8691e-01,\n",
      "        -1.8353e-01,  7.1114e-02, -1.7106e-01,  2.5915e-01,  1.6064e-01,\n",
      "        -3.9002e-01, -2.8565e-01, -6.0818e-01, -5.2433e-01, -8.7178e-01,\n",
      "         3.4240e-01, -1.9395e-01, -2.1313e-01, -5.3338e-02, -1.9758e-01,\n",
      "         1.4647e-01,  0.0000e+00,  2.8637e-01, -1.6184e-01,  6.0504e-01,\n",
      "         0.0000e+00, -2.6296e-01,  7.5380e-02,  2.1278e-01,  2.5390e-02,\n",
      "         3.4450e-02,  1.4595e-01,  3.8759e-02,  7.2716e-02,  3.9197e-02,\n",
      "         8.8756e-02, -0.0000e+00,  1.8478e-02,  9.8416e-01, -2.6364e-01,\n",
      "        -6.8942e-02, -9.4029e-02, -5.7478e-02,  3.2285e-01, -5.7656e-01,\n",
      "         2.0220e-01, -1.5339e-01, -4.9238e-02, -0.0000e+00, -4.8073e-01,\n",
      "        -7.9179e-02,  0.0000e+00,  1.5304e+00,  4.2460e-02,  4.1123e-01,\n",
      "        -0.0000e+00, -4.8167e-01,  3.8574e-01, -1.2822e-01,  4.1354e-02,\n",
      "         0.0000e+00, -6.9776e-01,  3.2668e-01, -2.1474e-01, -2.1818e-01,\n",
      "        -3.7206e-01,  4.2261e-01, -1.7783e-01, -1.3444e-01,  5.6890e-02,\n",
      "         2.7172e-01, -3.0584e-01, -5.0901e-02,  9.8006e-02,  0.0000e+00,\n",
      "         7.3175e-02, -0.0000e+00,  5.4373e-01,  3.8749e-01, -3.6800e-01,\n",
      "         0.0000e+00,  1.5671e-02, -1.1466e-01,  9.1756e-02, -1.8630e-01,\n",
      "         9.2053e-02,  3.1286e-01,  9.7085e-01,  9.8562e-01,  2.7487e-01,\n",
      "         7.0534e-02,  1.2059e-02, -0.0000e+00,  0.0000e+00, -3.2997e-02,\n",
      "        -0.0000e+00,  2.0434e-01, -3.9233e-01,  9.6061e-02, -0.0000e+00,\n",
      "         1.6593e-01,  1.5613e+00,  1.3652e-01, -2.2649e-01, -0.0000e+00,\n",
      "        -2.9989e-01,  3.6224e-01,  4.3377e-02,  4.2538e-01,  5.5105e-02,\n",
      "        -0.0000e+00,  5.9939e-01, -8.2835e-02,  3.3487e-01,  5.6623e-02,\n",
      "         2.0217e-01, -1.3256e-01,  6.7352e-02, -1.6691e-01, -4.8287e-01,\n",
      "        -3.2467e-01,  3.6327e-01, -3.3465e-02, -1.3227e-01, -1.9528e-01,\n",
      "         0.0000e+00, -1.8343e-01, -1.6908e-01,  2.5411e-01, -3.6206e-01,\n",
      "        -6.9136e-02, -1.0556e-01,  0.0000e+00,  4.2614e-01, -0.0000e+00,\n",
      "         1.7047e+00, -7.2369e-02,  3.0487e-01, -2.8156e-01, -2.1558e-01,\n",
      "         4.5144e-01, -1.1532e-01,  2.8843e-01,  3.4739e-01, -2.3893e-01,\n",
      "         4.1995e-01, -1.9893e-01, -2.1765e-01, -0.0000e+00,  9.7945e-02,\n",
      "         1.0037e-01,  1.1542e-01,  4.2539e-01, -1.7717e-01,  5.3722e-02,\n",
      "        -8.9200e-02, -2.2667e-01, -7.1440e-02,  0.0000e+00,  1.9450e-01,\n",
      "        -1.2310e-01, -1.1707e-01, -2.7133e-01, -7.7989e-02,  0.0000e+00,\n",
      "        -1.7497e-01,  3.2849e-01, -2.5761e-02, -1.1611e+00,  3.5545e-02,\n",
      "        -1.9288e-01, -1.9158e-02, -1.5735e-01,  0.0000e+00, -7.6583e-02,\n",
      "        -0.0000e+00,  4.4132e-01, -1.5308e-01, -2.1728e-01,  2.1403e-01,\n",
      "        -2.2240e-01,  2.5858e-01,  2.6659e-01, -0.0000e+00,  2.5358e-01,\n",
      "        -2.3741e-01, -1.1511e-01,  5.3251e-01,  3.1558e-01,  3.0029e-01,\n",
      "        -2.4493e-01,  8.1528e-02,  1.1085e-01, -5.0980e-01, -0.0000e+00,\n",
      "        -3.2700e-01,  2.1723e-01,  3.4705e-01, -0.0000e+00,  1.4022e-01,\n",
      "         2.2886e-01, -1.5593e-01,  0.0000e+00,  5.6011e-01,  2.2322e-01,\n",
      "        -8.8007e-02, -1.0460e+00, -3.1143e-01, -2.4731e-02, -1.6763e-01,\n",
      "        -4.4498e-02,  5.2424e-01, -0.0000e+00, -1.5999e-01, -3.7545e-01,\n",
      "        -1.6200e-01, -2.9818e-01,  2.0411e-01, -1.2067e-01,  3.4077e-01,\n",
      "         2.1312e-01,  2.8242e-01, -3.4300e-02,  4.5213e-01,  3.7109e-01,\n",
      "        -4.8276e-01,  4.2099e-01, -2.5846e-01, -1.8025e-01,  2.0597e-01,\n",
      "         1.4123e-01, -6.4074e-01, -2.4672e-01,  0.0000e+00, -3.0993e-01,\n",
      "         0.0000e+00,  5.9002e-01,  1.3557e-01,  7.7427e-01, -3.0883e-01,\n",
      "         7.6105e-02, -2.0606e-01, -2.4717e-01,  1.0588e-01,  2.4015e-01,\n",
      "         1.6469e-01, -2.1821e-01,  3.6529e-04, -9.4440e-03, -1.2894e-01,\n",
      "         3.3365e-01,  3.0236e-01, -8.6335e-01, -4.0614e-01,  4.3224e-02,\n",
      "         1.3183e-03,  2.7049e-01, -3.6126e-01,  5.9900e-02,  1.5677e-01,\n",
      "        -7.5442e-02, -8.9264e-01, -6.7276e-03,  6.4889e-02, -2.3772e-01,\n",
      "         1.6583e-01, -2.2118e-02,  9.5446e-03,  2.4386e-01,  1.9682e-01,\n",
      "        -6.3895e-03, -7.8453e-02, -2.1207e-01, -3.8751e-01,  2.8768e-01,\n",
      "         7.7323e-02,  1.5986e-01, -3.3745e-01, -0.0000e+00,  6.7994e-02,\n",
      "         3.2835e-01, -7.3038e-02,  1.7355e-01,  8.5978e-01, -0.0000e+00,\n",
      "        -0.0000e+00,  2.0653e-01,  1.1400e-02, -8.5777e-03, -3.8499e-01,\n",
      "         1.7895e-01, -1.8414e-02,  0.0000e+00, -0.0000e+00, -9.3864e-01,\n",
      "        -3.9450e-01,  0.0000e+00,  1.4413e-01,  1.6788e-01,  1.8263e-01,\n",
      "         6.7385e-02, -0.0000e+00, -2.1318e-01, -7.6329e-02,  1.3133e-01,\n",
      "        -3.2569e-02,  1.2758e-01,  0.0000e+00,  5.0566e-01,  0.0000e+00,\n",
      "        -1.0879e-01,  4.7817e-01, -7.8468e-01,  3.7313e-01,  1.3494e+01,\n",
      "         1.9113e+00,  1.1156e-01, -9.9732e-02,  2.4408e-01,  2.3526e-01,\n",
      "         9.9004e-02,  2.8632e-01, -1.9142e-01,  2.7226e-01,  2.7606e+00,\n",
      "        -6.2966e-01,  3.0511e-01, -5.9308e-01,  1.8053e-01,  0.0000e+00,\n",
      "         5.6255e-02,  1.3233e-01, -0.0000e+00,  0.0000e+00, -4.3440e-01,\n",
      "         2.2629e-01, -7.4939e-02, -1.8720e-01, -0.0000e+00, -5.6370e-01,\n",
      "        -1.0642e+00,  9.6842e-02,  6.3902e-02, -0.0000e+00, -1.6287e-01,\n",
      "        -3.0723e-01, -8.6809e-02,  3.1557e-01,  1.8886e-02, -1.8404e-01,\n",
      "        -1.3579e+00,  1.1675e-01, -4.7689e-01,  4.8738e-02, -8.8720e-01,\n",
      "        -1.6927e-02, -4.9423e-01, -1.7129e-01,  0.0000e+00, -1.3759e-01,\n",
      "        -2.1684e-01, -3.5895e-01, -3.3188e-01, -2.1886e-01,  5.8968e-02,\n",
      "         2.9211e-02,  4.6414e-01, -2.3440e-01, -3.3028e-01, -1.2967e-01,\n",
      "         2.4057e-01, -7.3822e-01, -4.1440e-01,  0.0000e+00, -9.5189e-02,\n",
      "        -0.0000e+00,  3.8751e-01,  1.3621e-01,  0.0000e+00, -5.7732e-01,\n",
      "        -2.2431e-01, -4.9425e-01, -1.2992e-01, -1.9304e-01, -0.0000e+00,\n",
      "         1.2743e-01, -1.5979e-01, -1.1040e-01,  1.6074e-01,  9.9148e-02,\n",
      "        -0.0000e+00, -3.0580e-01, -3.3348e-01,  1.8232e-01, -7.1071e-01,\n",
      "         1.4909e-01, -0.0000e+00,  2.5419e-01, -6.3174e-01, -0.0000e+00,\n",
      "         2.5530e-01, -0.0000e+00, -2.4059e-01, -1.5088e-01,  3.6181e-01,\n",
      "         2.2308e-01, -8.5856e-02,  1.0390e-01,  0.0000e+00,  0.0000e+00,\n",
      "        -7.4115e-02, -3.0854e-01,  4.1610e-02, -5.1516e-02,  9.7044e-02,\n",
      "        -3.9537e-01, -6.0683e-02, -4.5912e-01,  1.6831e-02, -2.6995e-01,\n",
      "        -3.7903e-01, -7.4137e-02, -0.0000e+00, -2.9041e-01, -5.4088e-02,\n",
      "        -7.6297e-02, -9.3597e-02,  2.4751e-01, -0.0000e+00,  3.3896e-02,\n",
      "         5.4124e-02,  5.8633e-03, -7.9066e-02,  3.3809e-01, -0.0000e+00,\n",
      "        -2.2051e-01,  4.5891e-01, -2.2266e-01,  1.2248e-01, -8.7045e-02,\n",
      "        -1.9717e-02, -5.6382e-01, -2.8758e-01,  1.6175e-01, -0.0000e+00,\n",
      "         1.2575e-01, -9.7524e-02,  6.9074e-01,  1.0914e-01, -1.2496e-01,\n",
      "         4.9179e-01,  2.2840e-01, -1.3856e-01, -7.7401e-02, -2.9090e-01,\n",
      "         7.6876e-02,  0.0000e+00,  8.4206e-02, -7.8146e-01, -3.3116e-01,\n",
      "        -3.7474e-01, -8.2589e-02, -1.0115e-01, -6.2747e-01,  2.0730e-01,\n",
      "        -4.0620e-01, -5.3215e-02, -2.3011e-01, -2.3932e-02, -1.0332e+00,\n",
      "         0.0000e+00, -1.3370e+00,  0.0000e+00, -6.1876e-02, -4.4927e-01,\n",
      "        -8.0296e-02,  5.0048e-02,  2.5717e-01,  3.3311e-02, -2.3139e-01,\n",
      "        -3.1908e-01,  1.5972e-01,  0.0000e+00, -3.6291e-01,  3.2023e-02,\n",
      "         1.6791e-01, -3.3133e-01, -3.5564e-01,  4.7413e-01, -5.8519e-01,\n",
      "         2.2069e-02, -7.7416e-02,  2.2750e-02, -3.7961e-01, -2.2373e-01,\n",
      "        -0.0000e+00,  0.0000e+00, -1.9326e-01, -1.5185e-01, -1.3006e-01,\n",
      "        -6.1580e-02,  3.2775e-01, -4.5862e-02, -1.8900e-01,  3.4568e-01,\n",
      "        -5.1148e-01, -1.9688e-01, -3.8906e-01,  5.4904e-02, -3.4327e-01,\n",
      "         0.0000e+00, -2.4183e-01,  6.4419e-03,  3.1556e-01, -2.7452e-01,\n",
      "         1.2422e-01, -4.2030e-01,  0.0000e+00, -1.7305e-01,  0.0000e+00,\n",
      "        -1.0430e-01,  2.5556e-01,  6.7887e-03,  2.2842e-01, -2.2817e-01,\n",
      "        -7.7777e-02,  5.4506e-01,  1.2020e-01, -2.7637e-01, -1.9990e-01,\n",
      "        -8.7777e-02, -4.2024e-02, -4.5002e-02, -7.3429e-04, -3.0557e-01,\n",
      "        -7.0099e-02,  2.0164e-01,  0.0000e+00, -1.9215e-01, -1.9832e-01,\n",
      "        -1.6100e-01, -4.0626e-01, -2.7088e-01, -1.3574e-01, -9.6387e-03,\n",
      "        -1.1558e-01, -8.0887e-02, -0.0000e+00,  5.7941e-02, -6.2847e-01,\n",
      "        -2.3986e-04,  1.8188e-01,  1.2591e-01, -1.0989e-01, -6.6876e-01,\n",
      "         6.2915e-01,  1.6544e+00,  5.1697e-03, -2.8373e-02,  5.6980e-02,\n",
      "        -4.6602e-01, -6.7493e-01, -3.4296e-01,  1.2989e-01, -7.6938e-02,\n",
      "        -5.2058e-02, -2.2240e-01, -4.6323e-01, -2.6919e-01,  1.6207e-01,\n",
      "         3.4440e-01,  2.1395e-01, -2.7859e-01, -4.0380e-01,  4.2877e-02,\n",
      "        -1.8333e-01,  3.9863e-02,  3.4156e-01, -1.9136e-01,  3.9208e-02,\n",
      "        -7.3920e-01,  4.3663e-01,  5.6073e-01,  2.0796e-02, -1.3351e-01,\n",
      "         1.5148e-02, -1.3041e-01, -0.0000e+00, -7.4573e-01, -1.0421e-01,\n",
      "         4.4751e-02,  0.0000e+00, -1.9006e-02,  2.9469e-01, -1.2821e-01,\n",
      "         3.6539e-01,  1.0366e+00, -1.5012e-01,  1.6509e-01, -1.3819e-01,\n",
      "         2.7633e-01, -1.6915e-01, -3.5624e-02,  1.3373e-01,  1.3056e-01,\n",
      "        -2.1405e-01,  1.3699e-01, -0.0000e+00,  1.1508e-01,  6.6258e-02,\n",
      "         2.3397e-01, -9.5083e-01,  3.2930e-01, -2.0649e-01, -2.9661e-01,\n",
      "        -3.6018e-01, -0.0000e+00,  1.7283e-01,  1.5153e-01, -9.3848e-01,\n",
      "        -5.4659e-01, -3.4724e-02,  1.0372e-01, -4.5190e-01, -1.9472e-01,\n",
      "        -9.4216e-02,  3.2108e-01, -2.9454e-01, -7.9248e-02,  1.9217e-01,\n",
      "        -2.9674e-01, -4.2432e-02,  5.4366e-01,  2.2231e-01, -2.2198e-01,\n",
      "        -1.1804e-01,  1.0234e-02, -2.2114e-01,  1.4149e-01,  2.3712e-01,\n",
      "        -0.0000e+00, -2.2539e-01,  0.0000e+00,  0.0000e+00,  5.3381e-04,\n",
      "        -1.5634e-02,  4.9855e-02, -3.9432e-01,  7.3744e-01,  2.0468e-01,\n",
      "         4.1738e-01, -4.3296e-01,  1.7370e-02,  1.0198e-01, -2.4102e-01,\n",
      "        -2.7506e-01,  1.0848e-01, -7.4886e-02], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)}\n",
      "{'extended_attention_mask': tensor([[[[-0., -0., -0.,  ..., -0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.,  ..., -0., -0., -0.]]]], device='cuda:0')}\n",
      "{'head_mask': [None, None, None, None, None, None, None, None, None, None, None, None]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ -4.8410,  -3.6483,  -3.9309,  -2.8273,  -3.2758,  -3.6839,  -0.7073,\n",
       "          -4.5233,  -2.9430,  -6.5784,  -2.1321,  -5.3965,  -2.6291,  -2.7485,\n",
       "          -3.1896,  -3.5896,  -2.8359,  -7.4138, -10.3252,  -7.4138,  -9.1247,\n",
       "          -7.0680,  -6.9347,  -6.0047,  -7.4065,  -7.6085,  -8.2772,  -6.4487,\n",
       "          -7.6898,  -4.4147,  -5.4266,  -7.9759, -11.8575,  -6.3400,  -5.2046,\n",
       "          -5.1611],\n",
       "        [ -8.7905,  -7.0357, -11.0352,  -5.5062,  -7.9179,  -6.2430,  -5.4437,\n",
       "         -11.2843,  -7.1623, -12.9512,  -7.1544, -10.8570,  -9.5506,  -5.7982,\n",
       "          -9.5459,  -8.6775,  -7.7129,  -7.4305, -14.6312,  -8.3071,  -8.4468,\n",
       "          -9.2358,  -9.7711,  -7.3290, -10.2544, -15.4221, -11.8138, -10.5894,\n",
       "           5.5214,  -4.1414,  -5.3511,  -7.3361, -11.7879,  -9.8088,  -9.9927,\n",
       "          -8.2075]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=tensor([[[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1112, 0.1117,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1053, 0.1005,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1357, 0.1219,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1203, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1218, 0.1356,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1256, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1216,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1344,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1303, 0.1374,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1236, 0.1368,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1142, 0.1182,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1296, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]]],\n",
       "\n",
       "\n",
       "        [[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1035, 0.1129,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1113,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0968, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1011, 0.1166,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1034, 0.0995,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1168, 0.0866,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0965,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0986, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0916, 0.1256,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1070, 0.1045,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1140, 0.1025,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]]]],\n",
       "       device='cuda:0', grad_fn=<NativeDropoutBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tucore_gcn_bert(\n",
    "\tinput_ids,\n",
    "\tsegment_ids,\n",
    "\tinput_mask,\n",
    "\tspeaker_ids,\n",
    "\tgraphs,\n",
    "\tmention_id,\n",
    "\tNone,\n",
    "\tturn_mask\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 28], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\dgl\\dgl.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'SpeakerBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import add_path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from models.BERT.speaker_tokens import SpeakerBertTokenizer\n",
    "from models.BERT.tucorecgcn_bert_pytorch_pipeline import ConversationalSequenceClassificationPipeline\n",
    "from models.BERT.tucoregcn_bert_pytorch import TUCOREGCN_BertForSequenceClassification, TUCOREGCN_BertConfig\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"conversational-sequence-classification\",\n",
    "    pipeline_class=ConversationalSequenceClassificationPipeline\n",
    ")\n",
    "speaker_tokenizer = SpeakerBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TUCOREGCN_BertForSequenceClassification(TUCOREGCN_BertConfig.from_json_file(\"../models/BERT/tucoregcn_bert_mlc.json\"))\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(\"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\"))\n",
    "model.cuda()\n",
    "classifier = pipeline(\"conversational-sequence-classification\", model=model, tokenizer=speaker_tokenizer, device=\"cuda:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([1, 512]), 'token_type_ids': torch.Size([1, 512]), 'attention_mask': torch.Size([1, 512]), 'speaker_ids': torch.Size([1, 512]), 'graphs': [Graph(num_nodes={'node': 5},\n",
      "      num_edges={('node', 'dialog', 'node'): 4, ('node', 'entity', 'node'): 8, ('node', 'speaker', 'node'): 1},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])], 'mention_ids': torch.Size([1, 512]), 'turn_mask': torch.Size([1, 512, 512])}\n",
      "{'input_ids': tensor([[ 101,   11, 2129, 5149,  999, 1045, 1005, 1049, 4834, 3240, 1010, 4834,\n",
      "         3240, 1996, 6546,  999,   12, 7592, 4834, 3240, 1012,  102,   11,  102,\n",
      "           12,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'speaker_ids': tensor([[ 0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10,  0,  9,  0, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0'), 'graphs': [Graph(num_nodes={'node': 5},\n",
      "      num_edges={('node', 'dialog', 'node'): 4, ('node', 'entity', 'node'): 8, ('node', 'speaker', 'node'): 1},\n",
      "      metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])], 'mention_ids': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 3, 0,\n",
      "         4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'turn_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,   11, 2129, 5149,  999, 1045, 1005, 1049, 4834, 3240, 1010, 4834,\n",
      "         3240, 1996, 6546,  999,   12, 7592, 4834, 3240, 1012,  102,   11,  102,\n",
      "           12,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0')\n",
      "tensor([[ 101,   11, 2129, 5149,  999, 1045, 1005, 1049, 4834, 3240, 1010, 4834,\n",
      "         3240, 1996, 6546,  999,   12, 7592, 4834, 3240, 1012,  102,   11,  102,\n",
      "           12,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0')\n",
      "--------------------------------------------\n",
      "Embedding(30522, 768, padding_idx=0)\n",
      "tensor([[ 101,   11, 2129, 5149,  999, 1045, 1005, 1049, 4834, 3240, 1010, 4834,\n",
      "         3240, 1996, 6546,  999,   12, 7592, 4834, 3240, 1012,  102,   11,  102,\n",
      "           12,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0')\n",
      "tensor([[[ 0.0014, -0.0219, -0.0065,  ..., -0.0128,  0.0082,  0.0029],\n",
      "         [ 0.0014,  0.0654, -0.0433,  ..., -0.0451,  0.0149, -0.0147],\n",
      "         [-0.0184, -0.0689,  0.0206,  ...,  0.0275,  0.0285, -0.0917],\n",
      "         ...,\n",
      "         [-0.0314, -0.0045,  0.0182,  ..., -0.0309,  0.0204, -0.0345],\n",
      "         [-0.0314, -0.0045,  0.0182,  ..., -0.0309,  0.0204, -0.0345],\n",
      "         [-0.0314, -0.0045,  0.0182,  ..., -0.0309,  0.0204, -0.0345]]],\n",
      "       device='cuda:0')\n",
      "--------------------------------------------\n",
      "{'embedding_output': torch.Size([1, 512, 768]), 'v': device(type='cuda', index=0), 'extended_attention_mask': torch.Size([1, 1, 1, 512]), 'head_mask': 12}\n",
      "{'embedding_output': tensor([ 3.6673e-02, -7.2584e-02, -3.5258e-02,  2.4717e-01, -5.7690e-02,\n",
      "        -9.3414e-02,  1.9168e-01, -3.6063e-01, -2.4750e-01, -7.9356e-02,\n",
      "         3.4935e-02, -6.9908e-02, -1.2963e-01, -1.3736e-01,  1.2290e-01,\n",
      "        -0.0000e+00, -9.1912e-02, -2.1030e-02,  5.3281e-01,  2.0914e-01,\n",
      "        -1.4777e-01, -2.6016e-01,  3.7708e-02, -3.4372e-01,  3.4917e-01,\n",
      "        -5.7217e-01, -5.0762e-01, -4.6619e-01, -1.4343e-01,  9.5567e-02,\n",
      "         6.4346e-02, -0.0000e+00, -3.1297e-01, -4.3523e-01, -2.9832e-01,\n",
      "        -1.5287e-01,  3.6939e-02, -1.4034e-03,  4.7887e-01,  0.0000e+00,\n",
      "        -3.8560e-01, -6.7123e-02, -2.7984e-01,  1.0240e-01,  3.5605e-01,\n",
      "         0.0000e+00,  1.3125e-01,  1.5932e-02, -4.4216e-01,  1.2226e-01,\n",
      "        -5.5556e-01, -2.0888e-01, -3.5134e-02,  3.4076e-02,  1.7256e-01,\n",
      "        -3.3335e-01, -1.0523e-01, -2.6584e-01,  3.8233e-01, -4.6806e-01,\n",
      "         5.3670e-01,  3.0179e-01,  2.7513e-01, -1.0629e-01, -0.0000e+00,\n",
      "        -5.2946e-02,  4.8496e-02,  6.7846e-02, -8.7807e-02, -0.0000e+00,\n",
      "         8.6273e-02,  2.7979e-01, -1.7693e-01,  6.3168e-02, -1.7343e-01,\n",
      "         5.3385e-01,  2.5156e-01, -1.9124e-02,  2.2393e-01,  0.0000e+00,\n",
      "        -2.1094e-01, -3.6770e-01,  3.0266e-02,  3.3922e-01, -3.7089e-01,\n",
      "         0.0000e+00,  2.6748e-02, -7.9741e-02,  2.6652e-01,  5.8362e-02,\n",
      "         6.7077e-01, -2.4580e-01, -7.0559e-02,  3.0124e-01,  9.8481e-02,\n",
      "        -4.5312e-01, -4.1149e-01, -1.2404e-01,  3.2690e-02,  1.9237e-01,\n",
      "        -1.2532e-01, -4.5723e-01, -0.0000e+00, -5.1166e-01, -4.9939e-01,\n",
      "        -1.9785e-01, -3.9101e-01,  3.5958e-01,  1.1876e-01, -3.2260e-01,\n",
      "        -1.6596e-01,  2.3744e-01,  1.8846e-01,  4.7892e-01, -0.0000e+00,\n",
      "        -1.8353e-01,  7.1114e-02, -1.7106e-01,  2.5915e-01,  1.6064e-01,\n",
      "        -3.9002e-01, -2.8565e-01, -6.0818e-01, -5.2433e-01, -8.7178e-01,\n",
      "         3.4240e-01, -1.9395e-01, -2.1313e-01, -5.3338e-02, -1.9758e-01,\n",
      "         1.4647e-01,  4.6054e-01,  2.8637e-01, -1.6184e-01,  6.0504e-01,\n",
      "         0.0000e+00, -2.6296e-01,  7.5380e-02,  2.1278e-01,  2.5390e-02,\n",
      "         3.4450e-02,  1.4595e-01,  3.8759e-02,  7.2716e-02,  3.9197e-02,\n",
      "         8.8756e-02, -6.9209e-02,  1.8478e-02,  9.8416e-01, -2.6364e-01,\n",
      "        -6.8942e-02, -9.4029e-02, -5.7478e-02,  3.2285e-01, -5.7656e-01,\n",
      "         2.0220e-01, -1.5339e-01, -4.9238e-02, -2.7550e-01, -4.8073e-01,\n",
      "        -7.9179e-02,  1.6400e-02,  1.5304e+00,  4.2460e-02,  4.1123e-01,\n",
      "        -5.0117e-02, -4.8167e-01,  3.8574e-01, -1.2822e-01,  0.0000e+00,\n",
      "         2.2314e-01, -6.9776e-01,  3.2668e-01, -2.1474e-01, -2.1818e-01,\n",
      "        -3.7206e-01,  4.2261e-01, -1.7783e-01, -1.3444e-01,  5.6890e-02,\n",
      "         2.7172e-01, -3.0584e-01, -5.0901e-02,  9.8006e-02,  3.0250e-01,\n",
      "         7.3175e-02, -0.0000e+00,  5.4373e-01,  3.8749e-01, -3.6800e-01,\n",
      "         7.2236e-03,  1.5671e-02, -1.1466e-01,  9.1756e-02, -1.8630e-01,\n",
      "         9.2053e-02,  3.1286e-01,  9.7085e-01,  9.8562e-01,  2.7487e-01,\n",
      "         7.0534e-02,  1.2059e-02, -9.0245e-02,  7.3592e-01, -0.0000e+00,\n",
      "        -2.4517e-03,  2.0434e-01, -0.0000e+00,  9.6061e-02, -2.6830e-01,\n",
      "         1.6593e-01,  1.5613e+00,  0.0000e+00, -2.2649e-01, -2.9218e-02,\n",
      "        -2.9989e-01,  0.0000e+00,  4.3377e-02,  4.2538e-01,  5.5105e-02,\n",
      "        -1.0048e+00,  5.9939e-01, -8.2835e-02,  3.3487e-01,  5.6623e-02,\n",
      "         2.0217e-01, -1.3256e-01,  6.7352e-02, -1.6691e-01, -4.8287e-01,\n",
      "        -3.2467e-01,  3.6327e-01, -0.0000e+00, -1.3227e-01, -1.9528e-01,\n",
      "         5.5047e-02, -1.8343e-01, -1.6908e-01,  2.5411e-01, -3.6206e-01,\n",
      "        -6.9136e-02, -1.0556e-01,  1.7062e-01,  4.2614e-01, -0.0000e+00,\n",
      "         1.7047e+00, -7.2369e-02,  3.0487e-01, -2.8156e-01, -2.1558e-01,\n",
      "         4.5144e-01, -1.1532e-01,  2.8843e-01,  3.4739e-01, -2.3893e-01,\n",
      "         4.1995e-01, -1.9893e-01, -2.1765e-01, -5.9621e-01,  9.7945e-02,\n",
      "         1.0037e-01,  1.1542e-01,  4.2539e-01, -1.7717e-01,  5.3722e-02,\n",
      "        -8.9200e-02, -2.2667e-01, -7.1440e-02,  4.7037e-01,  1.9450e-01,\n",
      "        -0.0000e+00, -1.1707e-01, -2.7133e-01, -7.7989e-02,  6.4529e-02,\n",
      "        -1.7497e-01,  3.2849e-01, -2.5761e-02, -1.1611e+00,  3.5545e-02,\n",
      "        -1.9288e-01, -1.9158e-02, -0.0000e+00,  1.9239e-01, -7.6583e-02,\n",
      "        -0.0000e+00,  4.4132e-01, -1.5308e-01, -2.1728e-01,  0.0000e+00,\n",
      "        -2.2240e-01,  2.5858e-01,  2.6659e-01, -7.3016e-02,  2.5358e-01,\n",
      "        -2.3741e-01, -1.1511e-01,  5.3251e-01,  3.1558e-01,  3.0029e-01,\n",
      "        -2.4493e-01,  8.1528e-02,  1.1085e-01, -0.0000e+00, -2.8852e-01,\n",
      "        -3.2700e-01,  2.1723e-01,  3.4705e-01, -2.1966e-01,  1.4022e-01,\n",
      "         2.2886e-01, -1.5593e-01,  6.8395e-01,  5.6011e-01,  2.2322e-01,\n",
      "        -8.8007e-02, -1.0460e+00, -3.1143e-01, -0.0000e+00, -1.6763e-01,\n",
      "        -0.0000e+00,  5.2424e-01, -2.2274e-01, -1.5999e-01, -3.7545e-01,\n",
      "        -0.0000e+00, -2.9818e-01,  2.0411e-01, -1.2067e-01,  3.4077e-01,\n",
      "         2.1312e-01,  2.8242e-01, -3.4300e-02,  4.5213e-01,  3.7109e-01,\n",
      "        -4.8276e-01,  4.2099e-01, -2.5846e-01, -1.8025e-01,  2.0597e-01,\n",
      "         1.4123e-01, -6.4074e-01, -2.4672e-01,  1.3293e-01, -3.0993e-01,\n",
      "         1.2980e-01,  5.9002e-01,  1.3557e-01,  7.7427e-01, -3.0883e-01,\n",
      "         7.6105e-02, -2.0606e-01, -2.4717e-01,  1.0588e-01,  0.0000e+00,\n",
      "         1.6469e-01, -0.0000e+00,  3.6529e-04, -9.4440e-03, -1.2894e-01,\n",
      "         3.3365e-01,  3.0236e-01, -8.6335e-01, -0.0000e+00,  4.3224e-02,\n",
      "         1.3183e-03,  2.7049e-01, -3.6126e-01,  5.9900e-02,  0.0000e+00,\n",
      "        -7.5442e-02, -0.0000e+00, -6.7276e-03,  6.4889e-02, -0.0000e+00,\n",
      "         1.6583e-01, -2.2118e-02,  9.5446e-03,  2.4386e-01,  1.9682e-01,\n",
      "        -6.3895e-03, -7.8453e-02, -2.1207e-01, -3.8751e-01,  2.8768e-01,\n",
      "         7.7323e-02,  1.5986e-01, -3.3745e-01, -4.0467e-02,  6.7994e-02,\n",
      "         3.2835e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "        -1.7224e-01,  2.0653e-01,  1.1400e-02, -8.5777e-03, -3.8499e-01,\n",
      "         1.7895e-01, -1.8414e-02,  2.3375e-01, -6.8610e-02, -0.0000e+00,\n",
      "        -3.9450e-01,  1.6789e-01,  1.4413e-01,  1.6788e-01,  1.8263e-01,\n",
      "         6.7385e-02, -3.0284e-01, -2.1318e-01, -7.6329e-02,  1.3133e-01,\n",
      "        -3.2569e-02,  1.2758e-01,  1.8403e-01,  5.0566e-01,  2.1459e-01,\n",
      "        -1.0879e-01,  4.7817e-01, -7.8468e-01,  3.7313e-01,  1.3494e+01,\n",
      "         1.9113e+00,  1.1156e-01, -9.9732e-02,  2.4408e-01,  2.3526e-01,\n",
      "         9.9004e-02,  2.8632e-01, -1.9142e-01,  2.7226e-01,  2.7606e+00,\n",
      "        -6.2966e-01,  3.0511e-01, -5.9308e-01,  0.0000e+00,  2.6964e-01,\n",
      "         5.6255e-02,  1.3233e-01, -1.9157e-01,  5.8311e-01, -4.3440e-01,\n",
      "         2.2629e-01, -7.4939e-02, -1.8720e-01, -8.2913e-02, -5.6370e-01,\n",
      "        -1.0642e+00,  9.6842e-02,  6.3902e-02, -2.9640e-01, -1.6287e-01,\n",
      "        -3.0723e-01, -8.6809e-02,  3.1557e-01,  1.8886e-02, -1.8404e-01,\n",
      "        -1.3579e+00,  1.1675e-01, -4.7689e-01,  4.8738e-02, -8.8720e-01,\n",
      "        -0.0000e+00, -0.0000e+00, -1.7129e-01,  1.3097e-01, -1.3759e-01,\n",
      "        -2.1684e-01, -3.5895e-01, -3.3188e-01, -2.1886e-01,  5.8968e-02,\n",
      "         2.9211e-02,  4.6414e-01, -0.0000e+00, -0.0000e+00, -1.2967e-01,\n",
      "         2.4057e-01, -7.3822e-01, -4.1440e-01,  1.5865e-01, -9.5189e-02,\n",
      "        -2.6074e-01,  3.8751e-01,  0.0000e+00,  0.0000e+00, -5.7732e-01,\n",
      "        -2.2431e-01, -0.0000e+00, -1.2992e-01, -1.9304e-01, -8.2167e-02,\n",
      "         1.2743e-01, -1.5979e-01, -1.1040e-01,  1.6074e-01,  9.9148e-02,\n",
      "        -2.7650e-01, -3.0580e-01, -3.3348e-01,  1.8232e-01, -7.1071e-01,\n",
      "         1.4909e-01, -2.4436e-01,  0.0000e+00, -6.3174e-01, -1.5366e-01,\n",
      "         2.5530e-01, -2.4094e-01, -2.4059e-01, -1.5088e-01,  3.6181e-01,\n",
      "         2.2308e-01, -8.5856e-02,  1.0390e-01,  2.5428e-01,  2.3133e-01,\n",
      "        -7.4115e-02, -0.0000e+00,  4.1610e-02, -5.1516e-02,  9.7044e-02,\n",
      "        -3.9537e-01, -0.0000e+00, -4.5912e-01,  1.6831e-02, -2.6995e-01,\n",
      "        -3.7903e-01, -7.4137e-02, -3.9954e-02, -2.9041e-01, -5.4088e-02,\n",
      "        -7.6297e-02, -0.0000e+00,  2.4751e-01, -3.8723e-01,  3.3896e-02,\n",
      "         5.4124e-02,  5.8633e-03, -7.9066e-02,  3.3809e-01, -3.1179e-01,\n",
      "        -2.2051e-01,  0.0000e+00, -0.0000e+00,  1.2248e-01, -8.7045e-02,\n",
      "        -1.9717e-02, -5.6382e-01, -2.8758e-01,  1.6175e-01, -2.0714e-01,\n",
      "         1.2575e-01, -9.7524e-02,  6.9074e-01,  1.0914e-01, -1.2496e-01,\n",
      "         4.9179e-01,  2.2840e-01, -1.3856e-01, -7.7401e-02, -2.9090e-01,\n",
      "         7.6876e-02,  3.8148e-01,  8.4206e-02, -7.8146e-01, -3.3116e-01,\n",
      "        -3.7474e-01, -8.2589e-02, -1.0115e-01, -6.2747e-01,  2.0730e-01,\n",
      "        -4.0620e-01, -5.3215e-02, -2.3011e-01, -2.3932e-02, -1.0332e+00,\n",
      "         4.5334e-01, -1.3370e+00,  1.0210e-01, -6.1876e-02, -4.4927e-01,\n",
      "        -8.0296e-02,  5.0048e-02,  2.5717e-01,  3.3311e-02, -2.3139e-01,\n",
      "        -3.1908e-01,  1.5972e-01,  9.3791e-02, -3.6291e-01,  3.2023e-02,\n",
      "         1.6791e-01, -3.3133e-01, -3.5564e-01,  4.7413e-01, -5.8519e-01,\n",
      "         2.2069e-02, -7.7416e-02,  0.0000e+00, -3.7961e-01, -2.2373e-01,\n",
      "        -1.4066e-01,  1.8436e-01, -1.9326e-01, -1.5185e-01, -1.3006e-01,\n",
      "        -6.1580e-02,  0.0000e+00, -0.0000e+00, -0.0000e+00,  3.4568e-01,\n",
      "        -5.1148e-01, -1.9688e-01, -3.8906e-01,  5.4904e-02, -3.4327e-01,\n",
      "         2.3204e-01, -2.4183e-01,  6.4419e-03,  3.1556e-01, -2.7452e-01,\n",
      "         1.2422e-01, -4.2030e-01,  4.1188e-01, -1.7305e-01,  4.1216e-01,\n",
      "        -1.0430e-01,  0.0000e+00,  6.7887e-03,  2.2842e-01, -2.2817e-01,\n",
      "        -7.7777e-02,  5.4506e-01,  1.2020e-01, -2.7637e-01, -1.9990e-01,\n",
      "        -8.7777e-02, -4.2024e-02, -4.5002e-02, -7.3429e-04, -3.0557e-01,\n",
      "        -7.0099e-02,  2.0164e-01,  0.0000e+00, -1.9215e-01, -0.0000e+00,\n",
      "        -1.6100e-01, -4.0626e-01, -2.7088e-01, -1.3574e-01, -9.6387e-03,\n",
      "        -1.1558e-01, -8.0887e-02, -4.2602e-01,  5.7941e-02, -6.2847e-01,\n",
      "        -2.3986e-04,  1.8188e-01,  1.2591e-01, -0.0000e+00, -6.6876e-01,\n",
      "         6.2915e-01,  1.6544e+00,  5.1697e-03, -2.8373e-02,  5.6980e-02,\n",
      "        -4.6602e-01, -6.7493e-01, -3.4296e-01,  1.2989e-01, -7.6938e-02,\n",
      "        -5.2058e-02, -2.2240e-01, -0.0000e+00, -2.6919e-01,  1.6207e-01,\n",
      "         3.4440e-01,  2.1395e-01, -2.7859e-01, -4.0380e-01,  4.2877e-02,\n",
      "        -1.8333e-01,  3.9863e-02,  3.4156e-01, -1.9136e-01,  3.9208e-02,\n",
      "        -0.0000e+00,  4.3663e-01,  5.6073e-01,  2.0796e-02, -1.3351e-01,\n",
      "         1.5148e-02, -1.3041e-01, -1.7842e-01, -7.4573e-01, -1.0421e-01,\n",
      "         4.4751e-02,  4.9132e-02, -1.9006e-02,  2.9469e-01, -1.2821e-01,\n",
      "         3.6539e-01,  1.0366e+00, -1.5012e-01,  1.6509e-01, -1.3819e-01,\n",
      "         2.7633e-01, -1.6915e-01, -3.5624e-02,  1.3373e-01,  1.3056e-01,\n",
      "        -2.1405e-01,  1.3699e-01, -8.9484e-03,  1.1508e-01,  6.6258e-02,\n",
      "         2.3397e-01, -9.5083e-01,  3.2930e-01, -2.0649e-01, -2.9661e-01,\n",
      "        -3.6018e-01, -4.1358e-02,  1.7283e-01,  1.5153e-01, -9.3848e-01,\n",
      "        -5.4659e-01, -3.4724e-02,  1.0372e-01, -4.5190e-01, -1.9472e-01,\n",
      "        -9.4216e-02,  3.2108e-01, -2.9454e-01, -7.9248e-02,  1.9217e-01,\n",
      "        -0.0000e+00, -0.0000e+00,  5.4366e-01,  0.0000e+00, -2.2198e-01,\n",
      "        -1.1804e-01,  1.0234e-02, -2.2114e-01,  1.4149e-01,  2.3712e-01,\n",
      "        -3.5958e-01, -2.2539e-01,  4.2175e-02,  1.8223e-03,  5.3381e-04,\n",
      "        -1.5634e-02,  4.9855e-02, -0.0000e+00,  7.3744e-01,  2.0468e-01,\n",
      "         4.1738e-01, -4.3296e-01,  1.7370e-02,  1.0198e-01, -2.4102e-01,\n",
      "        -2.7506e-01,  1.0848e-01, -7.4886e-02], device='cuda:0')}\n",
      "{'extended_attention_mask': tensor([[[[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "           -0., -0., -0., -0., -0., -0.]]]], device='cuda:0')}\n",
      "{'head_mask': [None, None, None, None, None, None, None, None, None, None, None, None]}\n",
      "{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\models\\BERT\\tucorecgcn_bert_pytorch_pipeline.py:323: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': ['LABEL_14'],\n",
       " 'score': [0.33398374915122986],\n",
       " 'logits': [[-6.00899600982666,\n",
       "   -4.544610500335693,\n",
       "   -6.698363304138184,\n",
       "   -4.116959095001221,\n",
       "   -5.802485466003418,\n",
       "   -5.37537145614624,\n",
       "   -5.542641639709473,\n",
       "   -8.46031665802002,\n",
       "   -5.55460786819458,\n",
       "   -7.048425197601318,\n",
       "   -4.928355693817139,\n",
       "   -6.537173271179199,\n",
       "   -2.317607879638672,\n",
       "   -3.280219793319702,\n",
       "   -1.6410108804702759,\n",
       "   -6.400602340698242,\n",
       "   -1.9220244884490967,\n",
       "   -9.677318572998047,\n",
       "   -13.731180191040039,\n",
       "   -11.12903881072998,\n",
       "   -12.00004768371582,\n",
       "   -7.024866580963135,\n",
       "   -9.708110809326172,\n",
       "   -5.761183261871338,\n",
       "   -7.499666690826416,\n",
       "   -9.901050567626953,\n",
       "   -12.585268020629883,\n",
       "   -5.523062229156494,\n",
       "   -5.182820796966553,\n",
       "   -3.547401189804077,\n",
       "   -7.338206768035889,\n",
       "   -7.698788642883301,\n",
       "   -10.658377647399902,\n",
       "   -6.6471848487854,\n",
       "   -7.222543239593506,\n",
       "   -8.640364646911621]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.BERT.tucoregcn_bert_pytorch_processor import SpeakerRelation, Conversation, Message\n",
    "\n",
    "c = Conversation(\n",
    "\tmessages=[\n",
    "\t\tMessage(\"Speaker 1\", \"Howdy! I'm Flowey, Flowey the Flower!\"),\n",
    "\t\tMessage(\"Speaker 2\", \"Hello Flowey.\"),\n",
    "\t],\n",
    "\tspeaker_relations=[\n",
    "\t\tSpeakerRelation(\"Speaker 1\", \"Speaker 2\", [3])\n",
    "\t]\n",
    ")\n",
    "classifier(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([np.array([1,2,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucoregcn_bert.bert.embeddings.word_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.position_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.token_type_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.speaker_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.LayerNorm.weight\n",
      "tucoregcn_bert.bert.embeddings.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.pooler.dense.weight\n",
      "tucoregcn_bert.bert.pooler.dense.bias\n",
      "tucoregcn_bert.turnAttention.w_qs.weight\n",
      "tucoregcn_bert.turnAttention.w_ks.weight\n",
      "tucoregcn_bert.turnAttention.w_vs.weight\n",
      "tucoregcn_bert.turnAttention.fc.weight\n",
      "tucoregcn_bert.turnAttention.layer_norm.weight\n",
      "tucoregcn_bert.turnAttention.layer_norm.bias\n",
      "tucoregcn_bert.GCN_layers.0.weight\n",
      "tucoregcn_bert.GCN_layers.0.h_bias\n",
      "tucoregcn_bert.GCN_layers.0.loop_weight\n",
      "tucoregcn_bert.GCN_layers.1.weight\n",
      "tucoregcn_bert.GCN_layers.1.h_bias\n",
      "tucoregcn_bert.GCN_layers.1.loop_weight\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.weight\n",
      "tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.bias\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.weight\n",
      "tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(tucore_gcn_bert.state_dict().keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "sequence_outputs = output[\"sequence_outputs\"]\n",
    "pooled_outputs = output[\"pooled_outputs\"]\n",
    "mention_ids = batch[\"mention_ids\"].to(\"cuda:0\")\n",
    "graphs = batch[\"graphs\"]\n",
    "#initialize some variables\n",
    "features = None\n",
    "num_batch_turn = []\n",
    "slen = input_ids.size(1)\n",
    "# Iterate over all inputs to be processed\n",
    "for i in range(len(graphs)):\n",
    "\tsequence_output = sequence_outputs[i]\n",
    "\tmention_id = mention_ids[i]\n",
    "\tpooled_output = pooled_outputs[i]\n",
    "\t# Find the last turn (in the case of tucoregcn, it is the masked speaker dictionary)\n",
    "\tmention_num = torch.max(mention_id)\n",
    "\t# Create a mention matrix idx of mention_num*slen\n",
    "\tnum_batch_turn.append(mention_num + 1)\n",
    "\tmention_index = (\n",
    "\t\t(torch.arange(mention_num) + 1).unsqueeze(1).expand(-1, slen)\n",
    "\t)\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tmention_index = mention_index.cuda()\n",
    "\t# Create a mentions matrix of slen*mention_num\n",
    "\tmentions = mention_id.unsqueeze(0).expand(mention_num, -1)\n",
    "\t# Generate truth matrix of each speaker's dialogue over the entire conversation\n",
    "\tselect_metrix = (mention_index == mentions).float()\n",
    "\t# Factor into the one-hot encoding, the total number of words in each speaker's dialogue\n",
    "\tword_total_numbers = (\n",
    "\t\ttorch.sum(select_metrix, dim=-1).unsqueeze(-1).expand(-1, slen)\n",
    "\t)\n",
    "\tselect_metrix = torch.where(\n",
    "\t\tword_total_numbers > 0,\n",
    "\t\tselect_metrix / word_total_numbers,\n",
    "\t\tselect_metrix,\n",
    "\t)\n",
    "\t# Apply one hot encoding to sequence_output to selectively obtain features\n",
    "\tx = torch.mm(select_metrix, sequence_output)\n",
    "\tx = torch.cat((pooled_output.unsqueeze(0), x), dim=0)\n",
    "\t# Iteratively concatenate features from all sequence_outputs\n",
    "\tif features is None:\n",
    "\t\tfeatures = x\n",
    "\telse:\n",
    "\t\tfeatures = torch.cat((features, x), dim=0)\n",
    "# Batch dgl graphs into 1 graph for efficient computation\n",
    "graph_big = dgl.batch(graphs)\n",
    "output_features = [features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(23, device='cuda:0'), tensor(23, device='cuda:0')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batch_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tucore_gcn_bert.tucoregcn_bert.LSTM_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0296, -0.1093, -0.0949,  ...,  0.0403, -0.0775,  0.0609],\n",
       "        [ 0.0269, -0.1400, -0.0430,  ...,  0.0400, -0.0286,  0.0603],\n",
       "        [ 0.0338, -0.0955, -0.0860,  ...,  0.0678, -0.0762,  0.0849],\n",
       "        ...,\n",
       "        [ 0.0447, -0.0806, -0.0882,  ...,  0.0112, -0.0611,  0.0586],\n",
       "        [-0.0022, -0.0971, -0.0838,  ...,  0.0282, -0.0546,  0.0250],\n",
       "        [ 0.0282, -0.0570, -0.0888,  ...,  0.0342, -0.0482,  0.0617]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features = []\n",
    "new_features.append(features[0])\n",
    "lstm_out = lstm(features[0 + 1 : 0 + idx - 2].unsqueeze(0))\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 4.5792e-01, -1.1729e-02,  3.2297e-01,  ..., -3.7483e-01,\n",
       "           4.0892e-01, -7.1791e-01],\n",
       "         [-4.3282e-01, -1.8977e-01,  7.0841e-01,  ...,  7.7599e-01,\n",
       "          -5.2889e-01, -1.2678e+00],\n",
       "         [-1.0421e+00, -5.9411e-01,  1.3200e+00,  ...,  2.7433e-01,\n",
       "          -4.1637e-01, -1.2198e+00],\n",
       "         ...,\n",
       "         [ 2.4836e-01, -7.2986e-01,  1.8437e-01,  ...,  6.4670e-01,\n",
       "          -1.1878e+00, -2.2512e-01],\n",
       "         [ 1.4898e-01, -6.1494e-01, -5.7474e-01,  ...,  1.6153e+00,\n",
       "          -2.0723e+00, -3.3880e-01],\n",
       "         [ 1.2348e+00, -4.7497e-01, -5.3483e-01,  ..., -1.4764e-03,\n",
       "          -1.1713e+00, -2.1761e-01]], device='cuda:0', grad_fn=<CatBackward0>)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'list' is an illegal expression for augmented assignment (2151216791.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[97], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    [4,7,8]+=[2]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'list' is an illegal expression for augmented assignment\n"
     ]
    }
   ],
   "source": [
    "[4,7,8]+=[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "sequence_output = output[\"sequence_outputs\"][0]\n",
    "pooled_output = output[\"pooled_outputs\"][0]\n",
    "mention_id = batch[\"mention_ids\"].to(\"cuda:0\")[0]\n",
    "\n",
    "#initialize some variables\n",
    "slen = input_ids.size(1)\n",
    "# Find the last turn (in the case of tucoregcn, it is the masked speaker dictionary)\n",
    "mention_num = torch.max(mention_id)\n",
    "# Create a mention matrix idx of mention_num*slen\n",
    "num_batch_turn = mention_num + 1\n",
    "mention_index = (\n",
    "\t(torch.arange(mention_num) + 1).unsqueeze(1).expand(-1, slen)\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "\tmention_index = mention_index.cuda()\n",
    "# Create a mentions matrix of slen*mention_num\n",
    "mentions = mention_id.unsqueeze(0).expand(mention_num, -1)\n",
    "# Generate 1 hot encoding of each speaker's dialogue\n",
    "select_metrix = (mention_index == mentions).float()\n",
    "# Factor into the one-hot encoding, the total number of words in each speaker's dialogue\n",
    "word_total_numbers = (\n",
    "\ttorch.sum(select_metrix, dim=-1).unsqueeze(-1).expand(-1, slen)\n",
    ")\n",
    "select_metrix = torch.where(\n",
    "\tword_total_numbers > 0,\n",
    "\tselect_metrix / word_total_numbers,\n",
    "\tselect_metrix,\n",
    ")\n",
    "# Apply one hot encoding to sequence_output to selectively obtain features\n",
    "features = torch.mm(select_metrix, sequence_output)\n",
    "features = torch.cat((pooled_output.unsqueeze(0), features), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val, ind = torch.max(torch.LongTensor([[1,2,3],[1,2,4]]),dim=1)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 22], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 6, 6, 6, 6, 6, 6, 6],\n",
       "        [7, 7, 7, 7, 7, 7, 7, 7],\n",
       "        [8, 8, 8, 8, 8, 8, 8, 8]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2500, 0.2500,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(5) + 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.LongTensor([6,7,8])\n",
    "result = torch.arange(0, torch.max(temp)+1).expand(len(temp), -1)\n",
    "result#>temp.expand(torch.max(temp)+1,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [8]],\n",
       "\n",
       "        [[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [8]],\n",
       "\n",
       "        [[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [8]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result>torch.LongTensor([6,7,8]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 2,  2,  2,  ...,  2,  2,  2],\n",
       "        [ 3,  3,  3,  ...,  3,  3,  3],\n",
       "        ...,\n",
       "        [20, 20, 20,  ..., 20, 20, 20],\n",
       "        [21, 21, 21,  ..., 21, 21, 21],\n",
       "        [22, 22, 22,  ..., 22, 22, 22]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"mention_ids\"].to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m mention_nums, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(mention_ids,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m num_batch_turns \u001b[38;5;241m=\u001b[39m mention_nums \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m mention_index \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 12\u001b[0m \t(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmention_nums\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, slen)\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m mention_index\n",
      "\u001b[1;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "sequence_outputs = output[\"sequence_outputs\"]\n",
    "pooled_outputs = output[\"pooled_outputs\"]\n",
    "graphs = batch[\"graphs\"]\n",
    "mention_ids = batch[\"mention_ids\"].to(\"cuda:0\")\n",
    "mention_ids\n",
    "#initialize some variables\n",
    "slen = input_ids.size(1)\n",
    "# Find the last turn (in the case of tucoregcn, it is the masked speaker dictionary)\n",
    "mention_nums, _ = torch.max(mention_ids,dim=1)\n",
    "num_batch_turns = mention_nums + 1\n",
    "'''\n",
    "mention_index = (\n",
    "\t(torch.arange(mention_nums,dim=1) + 1).unsqueeze(1).expand(-1, slen)\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "\tmention_index = mention_index.cuda()'''\n",
    "mentions = mention_ids.unsqueeze(1).expand(-1, mention_num, -1)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        ...,\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_id.unsqueeze(0).expand(mention_num, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        ...,\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_ids.unsqueeze(1).expand(-1, mention_num, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  4.,  4.,  ...,  4.,  4.,  4.],\n",
       "        [ 5.,  5.,  5.,  ...,  5.,  5.,  5.],\n",
       "        [12., 12., 12.,  ..., 12., 12., 12.],\n",
       "        ...,\n",
       "        [75., 75., 75.,  ..., 75., 75., 75.],\n",
       "        [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((mention_index==mentions).float(), dim=-1).unsqueeze(-1).expand(-1, slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "        0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_metrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mention_index==mentions).float()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2500, 0.2500,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(mention_num) + 1).unsqueeze(1).expand(-1, slen).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'hey',\n",
       " '!',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'hey',\n",
       " '.',\n",
       " 'speaker',\n",
       " '3',\n",
       " ':',\n",
       " 'hey',\n",
       " ',',\n",
       " 'man',\n",
       " '.',\n",
       " 'what',\n",
       " \"'\",\n",
       " 's',\n",
       " 'up',\n",
       " '?',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'maybe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'me',\n",
       " '.',\n",
       " 'my',\n",
       " 'agent',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'know',\n",
       " 'why',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'show',\n",
       " 'up',\n",
       " 'at',\n",
       " 'the',\n",
       " 'audition',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'i',\n",
       " 'had',\n",
       " 'today',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'she',\n",
       " 'gets',\n",
       " 'me',\n",
       " 'in',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'how',\n",
       " 'could',\n",
       " 'you',\n",
       " 'not',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'message',\n",
       " '?',\n",
       " '!',\n",
       " 'speaker',\n",
       " '3',\n",
       " ':',\n",
       " 'well',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'tell',\n",
       " 'ya',\n",
       " 'i',\n",
       " 'do',\n",
       " 'enjoy',\n",
       " 'guilt',\n",
       " ',',\n",
       " 'but',\n",
       " ',',\n",
       " 'ah',\n",
       " ',',\n",
       " 'it',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'me',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'yes',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " '!',\n",
       " 'it',\n",
       " 'was',\n",
       " 'him',\n",
       " '!',\n",
       " 'uh',\n",
       " 'huh',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'me',\n",
       " '!',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'how',\n",
       " 'is',\n",
       " 'it',\n",
       " 'you',\n",
       " '?',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'well',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'all',\n",
       " 'so',\n",
       " 'crazy',\n",
       " ',',\n",
       " 'you',\n",
       " 'know',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'chandler',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'closet',\n",
       " ',',\n",
       " 'counting',\n",
       " 'to',\n",
       " '10',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'was',\n",
       " 'up',\n",
       " 'to',\n",
       " '7',\n",
       " 'and',\n",
       " 'i',\n",
       " 'hadn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'found',\n",
       " 'a',\n",
       " 'place',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'yet',\n",
       " '.',\n",
       " 'i',\n",
       " '-',\n",
       " 'i',\n",
       " '-',\n",
       " 'i',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " 'wrote',\n",
       " 'it',\n",
       " 'all',\n",
       " 'down',\n",
       " 'on',\n",
       " 'my',\n",
       " 'hand',\n",
       " '.',\n",
       " 'see',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'yep',\n",
       " ',',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'my',\n",
       " 'audition',\n",
       " '.',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'see',\n",
       " ',',\n",
       " 'now',\n",
       " 'this',\n",
       " 'is',\n",
       " 'why',\n",
       " 'i',\n",
       " 'keep',\n",
       " 'note',\n",
       " '##pad',\n",
       " '##s',\n",
       " 'everywhere',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'yep',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'why',\n",
       " 'we',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'invite',\n",
       " 'you',\n",
       " 'to',\n",
       " 'play',\n",
       " '.',\n",
       " 'speaker',\n",
       " '5',\n",
       " ':',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'great',\n",
       " 'tragedy',\n",
       " 'here',\n",
       " '?',\n",
       " 'you',\n",
       " 'go',\n",
       " 'get',\n",
       " 'yourself',\n",
       " 'another',\n",
       " 'appointment',\n",
       " '.',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'well',\n",
       " ',',\n",
       " 'este',\n",
       " '##lle',\n",
       " 'tried',\n",
       " ',',\n",
       " 'you',\n",
       " 'know',\n",
       " '.',\n",
       " 'the',\n",
       " 'casting',\n",
       " 'director',\n",
       " 'told',\n",
       " 'her',\n",
       " 'that',\n",
       " 'i',\n",
       " 'missed',\n",
       " 'my',\n",
       " 'chance',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'that',\n",
       " 'is',\n",
       " 'unfair',\n",
       " '.',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'call',\n",
       " 'her',\n",
       " 'and',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'it',\n",
       " 'was',\n",
       " 'totally',\n",
       " 'my',\n",
       " 'fault',\n",
       " '.',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'ph',\n",
       " '##ee',\n",
       " '##bs',\n",
       " ',',\n",
       " 'you',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'do',\n",
       " 'that',\n",
       " '.',\n",
       " 'the',\n",
       " 'casting',\n",
       " 'director',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'friends',\n",
       " ',',\n",
       " 'she',\n",
       " 'only',\n",
       " 'talks',\n",
       " 'to',\n",
       " 'agents',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'what',\n",
       " 'a',\n",
       " 'sad',\n",
       " 'little',\n",
       " 'life',\n",
       " 'she',\n",
       " 'must',\n",
       " 'lead',\n",
       " '.',\n",
       " 'okay',\n",
       " ',',\n",
       " 'o',\n",
       " '##oh',\n",
       " '.',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'what',\n",
       " ',',\n",
       " 'what',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'what',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'no',\n",
       " ',',\n",
       " 'no',\n",
       " ',',\n",
       " 'no',\n",
       " ',',\n",
       " 'i',\n",
       " 'know',\n",
       " ',',\n",
       " 'i',\n",
       " 'know',\n",
       " ',',\n",
       " 'o',\n",
       " '##oh',\n",
       " '.',\n",
       " \"'\",\n",
       " 'hi',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'kate',\n",
       " '##lynn',\n",
       " ',',\n",
       " 'from',\n",
       " 'phoebe',\n",
       " 'buff',\n",
       " '##ay',\n",
       " \"'\",\n",
       " 's',\n",
       " 'office',\n",
       " '.',\n",
       " 'um',\n",
       " ',',\n",
       " 'is',\n",
       " 'um',\n",
       " ',',\n",
       " 'ann',\n",
       " 'there',\n",
       " 'for',\n",
       " 'phoebe',\n",
       " ',',\n",
       " 'she',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'know',\n",
       " 'what',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'about',\n",
       " '.',\n",
       " \"'\",\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'hang',\n",
       " 'up',\n",
       " ',',\n",
       " 'hang',\n",
       " 'up',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " \"'\",\n",
       " 'annie',\n",
       " '!',\n",
       " 'hi',\n",
       " '.',\n",
       " 'listen',\n",
       " 'we',\n",
       " 'got',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'joey',\n",
       " 'tri',\n",
       " '##bb',\n",
       " '##iani',\n",
       " ',',\n",
       " 'apparently',\n",
       " 'he',\n",
       " 'missed',\n",
       " 'his',\n",
       " 'audition',\n",
       " '.',\n",
       " 'who',\n",
       " 'did',\n",
       " 'you',\n",
       " 'speak',\n",
       " 'to',\n",
       " 'in',\n",
       " 'my',\n",
       " 'office',\n",
       " '?',\n",
       " 'este',\n",
       " '##lle',\n",
       " ',',\n",
       " 'no',\n",
       " ',',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'what',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'her',\n",
       " '.',\n",
       " 'no',\n",
       " '.',\n",
       " 'all',\n",
       " 'right',\n",
       " ',',\n",
       " 'so',\n",
       " 'your',\n",
       " 'husband',\n",
       " 'leaves',\n",
       " 'and',\n",
       " 'burns',\n",
       " 'down',\n",
       " 'the',\n",
       " '[SEP]',\n",
       " 'este',\n",
       " '##lle',\n",
       " '[SEP]',\n",
       " 'agent',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] speaker 1 : hey! speaker 2 : hey. speaker 3 : hey, man. what ' s up? speaker 1 : maybe you can tell me. my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today. the first good thing she gets me in weeks. how could you not give me the message?! speaker 3 : well, i ' ll tell ya i do enjoy guilt, but, ah, it wasn ' t me. speaker 2 : yes, it was! it was him! uh huh! okay, it was me! speaker 1 : how is it you? speaker 2 : well, it was just, it was all so crazy, you know. i mean, chandler was in the closet, counting to 10, and he was up to 7 and i hadn ' t found a place to hide yet. i - i - i meant to tell you, and i wrote it all down on my hand. see, all of it. speaker 1 : yep, that ' s my audition. speaker 4 : see, now this is why i keep notepads everywhere. speaker 2 : yep, and that ' s why we don ' t invite you to play. speaker 5 : what is the great tragedy here? you go get yourself another appointment. speaker 1 : well, estelle tried, you know. the casting director told her that i missed my chance. speaker 2 : that is unfair. i ' ll call her and tell her it was totally my fault. speaker 1 : pheebs, you can ' t do that. the casting director doesn ' t talk to friends, she only talks to agents. speaker 2 : what a sad little life she must lead. okay, ooh. speaker 1 : what, what are you doing? what are you doing? speaker 2 : no, no, no, i know, i know, ooh. ' hi, this is katelynn, from phoebe buffay ' s office. um, is um, ann there for phoebe, she ' ll know what it ' s about. ' speaker 1 : hang up, hang up. speaker 2 : ' annie! hi. listen we got a problem with joey tribbiani, apparently he missed his audition. who did you speak to in my office? estelle, no, i don ' t know what i ' m going to do with her. no. all right, so your husband leaves and burns down the [SEP] estelle [SEP] agent [SEP]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 1, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,\n",
       "         7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
       "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
       "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "        14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17,\n",
       "        17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20,  0, 21, 21,  0, 22,  0], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\", map_location=\"cuda\")\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace(\"tucoregcn_bert.turnAttention.fc.weight\", \"tucoregcn_bert.turnAttention.out_lin.weight\")] = state_dict.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucoregcn_bert.bert.embeddings.word_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.position_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.token_type_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.speaker_embeddings.weight\n",
      "tucoregcn_bert.bert.embeddings.LayerNorm.weight\n",
      "tucoregcn_bert.bert.embeddings.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.query.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.query.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.key.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.key.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.value.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.self.value.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.dense.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.dense.bias\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "tucoregcn_bert.bert.pooler.dense.weight\n",
      "tucoregcn_bert.bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "tucoregcn_bert.turnAttention.w_qs.weight\n",
      "tucoregcn_bert.turnAttention.w_ks.weight\n",
      "tucoregcn_bert.turnAttention.w_vs.weight\n",
      "tucoregcn_bert.turnAttention.out_lin.weight\n",
      "tucoregcn_bert.turnAttention.layer_norm.weight\n",
      "tucoregcn_bert.turnAttention.layer_norm.bias\n",
      "tucoregcn_bert.GCN_layers.0.weight\n",
      "tucoregcn_bert.GCN_layers.0.h_bias\n",
      "tucoregcn_bert.GCN_layers.0.loop_weight\n",
      "tucoregcn_bert.GCN_layers.1.weight\n",
      "tucoregcn_bert.GCN_layers.1.h_bias\n",
      "tucoregcn_bert.GCN_layers.1.loop_weight\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.weight\n",
      "tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.bias\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1_reverse\n",
      "tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.weight\n",
      "tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(state_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, \"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
