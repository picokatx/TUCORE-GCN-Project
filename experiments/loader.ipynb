{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import add_path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\dgl\\dgl.dll\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BertForSequenceClassification(\n",
       "  (tucoregcn_bert): TUCOREGCN_Bert(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (speaker_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (turnAttention): MultiHeadAttention(\n",
       "      (w_qs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_ks): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_vs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (fc): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (GCN_layers): ModuleList(\n",
       "      (0-1): 2 x RelGraphConvLayer(\n",
       "        (activation): ReLU()\n",
       "        (conv): HeteroGraphConv(\n",
       "          (mods): ModuleDict(\n",
       "            (speaker): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (dialog): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (entity): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.6, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (LSTM_layers): ModuleList(\n",
       "      (0-1): 2 x TurnLevelLSTM(\n",
       "        (lstm): LSTM(768, 768, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (bilstm2hiddnesize): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=6912, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from models.BERT.tucoregcn_bert_pytorch import TUCOREGCN_BertForSequenceClassification, TUCOREGCN_BertConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tucore_gcn_bert = TUCOREGCN_BertForSequenceClassification(TUCOREGCN_BertConfig.from_json_file(\"../models/BERT/tucoregcn_bert_mlc.json\"))\n",
    "tucore_gcn_bert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BertForSequenceClassification(\n",
       "  (tucoregcn_bert): TUCOREGCN_Bert(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (speaker_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (turnAttention): MultiHeadAttention(\n",
       "      (w_qs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_ks): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (w_vs): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (fc): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (GCN_layers): ModuleList(\n",
       "      (0-1): 2 x RelGraphConvLayer(\n",
       "        (activation): ReLU()\n",
       "        (conv): HeteroGraphConv(\n",
       "          (mods): ModuleDict(\n",
       "            (speaker): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (dialog): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (entity): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.6, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (LSTM_layers): ModuleList(\n",
       "      (0-1): 2 x TurnLevelLSTM(\n",
       "        (lstm): LSTM(768, 768, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (bilstm2hiddnesize): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=6912, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tucore_gcn_bert.load_state_dict(torch.load(\"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\", map_location=\"cuda\"))\n",
    "tucore_gcn_bert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../datasets/DialogRE/dev.json.\n",
      "load preprocessed data from ../datasets/DialogRE/dev_BERT.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'graph_output': tensor([[ 0.4662, -0.0104,  0.6122,  ...,  0.0000,  0.0000,  0.0288],\n",
       "         [ 0.2737,  0.0785,  0.2930,  ...,  2.1276,  0.4612,  3.3512]],\n",
       "        device='cuda:0', grad_fn=<StackBackward0>),\n",
       " 'graphs': [Graph(num_nodes={'node': 23},\n",
       "        num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 24, ('node', 'speaker', 'node'): 114},\n",
       "        metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')]),\n",
       "  Graph(num_nodes={'node': 23},\n",
       "        num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 10, ('node', 'speaker', 'node'): 114},\n",
       "        metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])],\n",
       " 'sequence_outputs': tensor([[[ 0.1739, -0.7403, -0.0519,  ...,  0.7922, -1.2172, -0.3233],\n",
       "          [ 0.0245, -0.1028,  0.4226,  ...,  0.7190, -0.4245, -0.9714],\n",
       "          [-0.4540, -0.1118,  0.0846,  ...,  1.1896, -1.2328, -1.0474],\n",
       "          ...,\n",
       "          [ 0.0523, -0.8307, -0.3889,  ...,  1.1827, -1.3276, -1.1553],\n",
       "          [ 0.0808, -0.3089,  0.4071,  ...,  1.2374, -0.9535,  0.3380],\n",
       "          [ 0.5711,  0.3974, -0.9195,  ...,  0.9180, -1.7780, -0.9887]],\n",
       " \n",
       "         [[ 0.5009, -0.4036,  0.1385,  ...,  1.0761, -1.1157, -0.4147],\n",
       "          [-0.3613, -0.8027,  1.0473,  ...,  0.6288, -0.5909, -1.2540],\n",
       "          [-0.5186, -1.0668,  0.6148,  ...,  0.4841, -0.2674, -1.9428],\n",
       "          ...,\n",
       "          [ 1.1622, -0.9924, -0.6364,  ..., -0.4431, -1.9439, -0.8221],\n",
       "          [ 0.2128, -0.9493, -1.1259,  ...,  0.1920, -0.9890,  0.4414],\n",
       "          [ 1.2156, -0.5195, -0.9432,  ...,  0.9291, -1.1872, -0.3703]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'pooled_outputs': tensor([[ 0.4662, -0.0104,  0.6122,  ..., -0.4204,  0.1608, -0.7047],\n",
       "         [ 0.2737,  0.0785,  0.2930,  ..., -0.5418,  0.4474, -0.5716]],\n",
       "        device='cuda:0', grad_fn=<TanhBackward0>),\n",
       " 'attn': tensor([[[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1147, 0.1079,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1197, 0.1170,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1289, 0.1228,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1310, 0.1195,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1307, 0.1204,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1301, 0.1302,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1290, 0.1298,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1254, 0.1337,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1343, 0.1293,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.1219,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1035, 0.1129,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1162, 0.1177,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]]],\n",
       " \n",
       " \n",
       "         [[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1065, 0.1212,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1131, 0.1155,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1079, 0.0944,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1111, 0.1015,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1141, 0.1051,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1006, 0.0936,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1032, 0.1011,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0977, 0.1112,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0953, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]],\n",
       " \n",
       "          [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1063, 0.1223,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.1113, 0.1266,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 1.1111, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1111, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.1111]]]],\n",
       "        device='cuda:0', grad_fn=<NativeDropoutBackward0>)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import TUCOREGCNDataset, TUCOREGCNDataloader\n",
    "dev_set = TUCOREGCNDataset(src_file=\"../datasets/DialogRE/dev.json\", save_file=\"../datasets/DialogRE/dev_BERT.pkl\", max_seq_length=512, tokenizer=tokenizer, n_class=36, encoder_type='BERT')\n",
    "dev_loader = TUCOREGCNDataloader(dataset=dev_set, batch_size=2, shuffle=False, relation_num=36, max_length=512)\n",
    "idx, batch = next(enumerate(dev_loader))\n",
    "entry_idx = 0\n",
    "input_ids = batch[\"input_ids\"].to(\"cuda:0\")\n",
    "segment_ids = batch[\"segment_ids\"].to(\"cuda:0\")\n",
    "input_mask = batch[\"input_masks\"].to(\"cuda:0\")\n",
    "speaker_ids = batch[\"speaker_ids\"].to(\"cuda:0\")\n",
    "graphs = batch[\"graphs\"]\n",
    "mention_id = batch[\"mention_ids\"].to(\"cuda:0\")\n",
    "turn_mask = batch[\"turn_masks\"].to(\"cuda:0\")\n",
    "output = tucore_gcn_bert(\n",
    "\tinput_ids,\n",
    "\tsegment_ids,\n",
    "\tinput_mask,\n",
    "\tspeaker_ids,\n",
    "\tgraphs,\n",
    "\tmention_id,\n",
    "\tNone,\n",
    "\tturn_mask\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "sequence_output = output[\"sequence_outputs\"][0]\n",
    "pooled_output = output[\"pooled_outputs\"][0]\n",
    "mention_id = batch[\"mention_ids\"].to(\"cuda:0\")[0]\n",
    "\n",
    "#initialize some variables\n",
    "slen = input_ids.size(1)\n",
    "# Find the last turn (in the case of tucoregcn, it is the masked speaker dictionary)\n",
    "mention_num = torch.max(mention_id)\n",
    "# Create a mention matrix idx of mention_num*slen\n",
    "num_batch_turn = mention_num + 1\n",
    "mention_index = (\n",
    "\t(torch.arange(mention_num) + 1).unsqueeze(1).expand(-1, slen)\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "\tmention_index = mention_index.cuda()\n",
    "# Create a mentions matrix of slen*mention_num\n",
    "mentions = mention_id.unsqueeze(0).expand(mention_num, -1)\n",
    "# Generate 1 hot encoding of each speaker's dialogue\n",
    "select_metrix = (mention_index == mentions).float()\n",
    "# Factor into the one-hot encoding, the total number of words in each speaker's dialogue\n",
    "word_total_numbers = (\n",
    "\ttorch.sum(select_metrix, dim=-1).unsqueeze(-1).expand(-1, slen)\n",
    ")\n",
    "select_metrix = torch.where(\n",
    "\tword_total_numbers > 0,\n",
    "\tselect_metrix / word_total_numbers,\n",
    "\tselect_metrix,\n",
    ")\n",
    "# Apply one hot encoding to sequence_output to selectively obtain features\n",
    "features = torch.mm(select_metrix, sequence_output)\n",
    "features = torch.cat((pooled_output.unsqueeze(0), features), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val, ind = torch.max(torch.LongTensor([[1,2,3],[1,2,4]]),dim=1)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 22], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 6, 6, 6, 6, 6, 6, 6],\n",
       "        [7, 7, 7, 7, 7, 7, 7, 7],\n",
       "        [8, 8, 8, 8, 8, 8, 8, 8]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.LongTensor([6,7,8])\n",
    "result = torch.arange(0, torch.max(temp)+1).expand(len(temp), -1)\n",
    "result>temp.expand(torch.max(temp)+1,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m mention_nums, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(mention_ids,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m num_batch_turns \u001b[38;5;241m=\u001b[39m mention_nums \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m mention_index \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 12\u001b[0m \t(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmention_nums\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, slen)\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m mention_index\n",
      "\u001b[1;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "sequence_outputs = output[\"sequence_outputs\"]\n",
    "pooled_outputs = output[\"pooled_outputs\"]\n",
    "graphs = batch[\"graphs\"]\n",
    "mention_ids = batch[\"mention_ids\"].to(\"cuda:0\")\n",
    "mention_ids\n",
    "#initialize some variables\n",
    "slen = input_ids.size(1)\n",
    "# Find the last turn (in the case of tucoregcn, it is the masked speaker dictionary)\n",
    "mention_nums, _ = torch.max(mention_ids,dim=1)\n",
    "num_batch_turns = mention_nums + 1\n",
    "'''\n",
    "mention_index = (\n",
    "\t(torch.arange(mention_nums,dim=1) + 1).unsqueeze(1).expand(-1, slen)\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "\tmention_index = mention_index.cuda()'''\n",
    "mentions = mention_ids.unsqueeze(1).expand(-1, mention_num, -1)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        ...,\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_id.unsqueeze(0).expand(mention_num, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        ...,\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0],\n",
       "        [ 0,  1,  1,  ...,  0, 22,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_ids.unsqueeze(1).expand(-1, mention_num, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  4.,  4.,  ...,  4.,  4.,  4.],\n",
       "        [ 5.,  5.,  5.,  ...,  5.,  5.,  5.],\n",
       "        [12., 12., 12.,  ..., 12., 12., 12.],\n",
       "        ...,\n",
       "        [75., 75., 75.,  ..., 75., 75., 75.],\n",
       "        [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((mention_index==mentions).float(), dim=-1).unsqueeze(-1).expand(-1, slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "        0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_metrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mention_index==mentions).float()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2500, 0.2500,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(mention_num) + 1).unsqueeze(1).expand(-1, slen).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'hey',\n",
       " '!',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'hey',\n",
       " '.',\n",
       " 'speaker',\n",
       " '3',\n",
       " ':',\n",
       " 'hey',\n",
       " ',',\n",
       " 'man',\n",
       " '.',\n",
       " 'what',\n",
       " \"'\",\n",
       " 's',\n",
       " 'up',\n",
       " '?',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'maybe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'me',\n",
       " '.',\n",
       " 'my',\n",
       " 'agent',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'know',\n",
       " 'why',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'show',\n",
       " 'up',\n",
       " 'at',\n",
       " 'the',\n",
       " 'audition',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'i',\n",
       " 'had',\n",
       " 'today',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'she',\n",
       " 'gets',\n",
       " 'me',\n",
       " 'in',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'how',\n",
       " 'could',\n",
       " 'you',\n",
       " 'not',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'message',\n",
       " '?',\n",
       " '!',\n",
       " 'speaker',\n",
       " '3',\n",
       " ':',\n",
       " 'well',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'tell',\n",
       " 'ya',\n",
       " 'i',\n",
       " 'do',\n",
       " 'enjoy',\n",
       " 'guilt',\n",
       " ',',\n",
       " 'but',\n",
       " ',',\n",
       " 'ah',\n",
       " ',',\n",
       " 'it',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'me',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'yes',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " '!',\n",
       " 'it',\n",
       " 'was',\n",
       " 'him',\n",
       " '!',\n",
       " 'uh',\n",
       " 'huh',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'me',\n",
       " '!',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'how',\n",
       " 'is',\n",
       " 'it',\n",
       " 'you',\n",
       " '?',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'well',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'all',\n",
       " 'so',\n",
       " 'crazy',\n",
       " ',',\n",
       " 'you',\n",
       " 'know',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'chandler',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'closet',\n",
       " ',',\n",
       " 'counting',\n",
       " 'to',\n",
       " '10',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'was',\n",
       " 'up',\n",
       " 'to',\n",
       " '7',\n",
       " 'and',\n",
       " 'i',\n",
       " 'hadn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'found',\n",
       " 'a',\n",
       " 'place',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'yet',\n",
       " '.',\n",
       " 'i',\n",
       " '-',\n",
       " 'i',\n",
       " '-',\n",
       " 'i',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " 'wrote',\n",
       " 'it',\n",
       " 'all',\n",
       " 'down',\n",
       " 'on',\n",
       " 'my',\n",
       " 'hand',\n",
       " '.',\n",
       " 'see',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'yep',\n",
       " ',',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'my',\n",
       " 'audition',\n",
       " '.',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'see',\n",
       " ',',\n",
       " 'now',\n",
       " 'this',\n",
       " 'is',\n",
       " 'why',\n",
       " 'i',\n",
       " 'keep',\n",
       " 'note',\n",
       " '##pad',\n",
       " '##s',\n",
       " 'everywhere',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'yep',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'why',\n",
       " 'we',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'invite',\n",
       " 'you',\n",
       " 'to',\n",
       " 'play',\n",
       " '.',\n",
       " 'speaker',\n",
       " '5',\n",
       " ':',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'great',\n",
       " 'tragedy',\n",
       " 'here',\n",
       " '?',\n",
       " 'you',\n",
       " 'go',\n",
       " 'get',\n",
       " 'yourself',\n",
       " 'another',\n",
       " 'appointment',\n",
       " '.',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'well',\n",
       " ',',\n",
       " 'este',\n",
       " '##lle',\n",
       " 'tried',\n",
       " ',',\n",
       " 'you',\n",
       " 'know',\n",
       " '.',\n",
       " 'the',\n",
       " 'casting',\n",
       " 'director',\n",
       " 'told',\n",
       " 'her',\n",
       " 'that',\n",
       " 'i',\n",
       " 'missed',\n",
       " 'my',\n",
       " 'chance',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'that',\n",
       " 'is',\n",
       " 'unfair',\n",
       " '.',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'call',\n",
       " 'her',\n",
       " 'and',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'it',\n",
       " 'was',\n",
       " 'totally',\n",
       " 'my',\n",
       " 'fault',\n",
       " '.',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'ph',\n",
       " '##ee',\n",
       " '##bs',\n",
       " ',',\n",
       " 'you',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'do',\n",
       " 'that',\n",
       " '.',\n",
       " 'the',\n",
       " 'casting',\n",
       " 'director',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'friends',\n",
       " ',',\n",
       " 'she',\n",
       " 'only',\n",
       " 'talks',\n",
       " 'to',\n",
       " 'agents',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'what',\n",
       " 'a',\n",
       " 'sad',\n",
       " 'little',\n",
       " 'life',\n",
       " 'she',\n",
       " 'must',\n",
       " 'lead',\n",
       " '.',\n",
       " 'okay',\n",
       " ',',\n",
       " 'o',\n",
       " '##oh',\n",
       " '.',\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'what',\n",
       " ',',\n",
       " 'what',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'what',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " 'no',\n",
       " ',',\n",
       " 'no',\n",
       " ',',\n",
       " 'no',\n",
       " ',',\n",
       " 'i',\n",
       " 'know',\n",
       " ',',\n",
       " 'i',\n",
       " 'know',\n",
       " ',',\n",
       " 'o',\n",
       " '##oh',\n",
       " '.',\n",
       " \"'\",\n",
       " 'hi',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'kate',\n",
       " '##lynn',\n",
       " ',',\n",
       " 'from',\n",
       " 'phoebe',\n",
       " 'buff',\n",
       " '##ay',\n",
       " \"'\",\n",
       " 's',\n",
       " 'office',\n",
       " '.',\n",
       " 'um',\n",
       " ',',\n",
       " 'is',\n",
       " 'um',\n",
       " ',',\n",
       " 'ann',\n",
       " 'there',\n",
       " 'for',\n",
       " 'phoebe',\n",
       " ',',\n",
       " 'she',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'know',\n",
       " 'what',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'about',\n",
       " '.',\n",
       " \"'\",\n",
       " '[unused2]',\n",
       " ':',\n",
       " 'hang',\n",
       " 'up',\n",
       " ',',\n",
       " 'hang',\n",
       " 'up',\n",
       " '.',\n",
       " 'speaker',\n",
       " '2',\n",
       " ':',\n",
       " \"'\",\n",
       " 'annie',\n",
       " '!',\n",
       " 'hi',\n",
       " '.',\n",
       " 'listen',\n",
       " 'we',\n",
       " 'got',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'joey',\n",
       " 'tri',\n",
       " '##bb',\n",
       " '##iani',\n",
       " ',',\n",
       " 'apparently',\n",
       " 'he',\n",
       " 'missed',\n",
       " 'his',\n",
       " 'audition',\n",
       " '.',\n",
       " 'who',\n",
       " 'did',\n",
       " 'you',\n",
       " 'speak',\n",
       " 'to',\n",
       " 'in',\n",
       " 'my',\n",
       " 'office',\n",
       " '?',\n",
       " 'este',\n",
       " '##lle',\n",
       " ',',\n",
       " 'no',\n",
       " ',',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'what',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'her',\n",
       " '.',\n",
       " 'no',\n",
       " '.',\n",
       " 'all',\n",
       " 'right',\n",
       " ',',\n",
       " 'so',\n",
       " 'your',\n",
       " 'husband',\n",
       " 'leaves',\n",
       " 'and',\n",
       " 'burns',\n",
       " 'down',\n",
       " 'the',\n",
       " 'apartment',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " 'does',\n",
       " 'not',\n",
       " 'stop',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'este',\n",
       " '##lle',\n",
       " '[SEP]',\n",
       " '[unused2]',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 1, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,\n",
       "         7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
       "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
       "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "        14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17,\n",
       "        17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20,  0, 21, 21,  0, 22,  0], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\", map_location=\"cuda\")\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace(\"tucoregcn_bert.classifier.\", \"classifier.\")] = state_dict.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, \"../TUCOREGCN_BERT_DialogRE/tucoregcn_pytorch_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
