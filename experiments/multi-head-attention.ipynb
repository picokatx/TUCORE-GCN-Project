{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\projects\\\\affect\\\\TUCORE-GCN',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv',\n",
       " '',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path[0] = \"d:\\\\projects\\\\affect\\\\TUCORE-GCN\"\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\dgl\\dgl.dll\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from models.BERT.TUCOREGCN_BERT import TUCOREGCN_BERT, BertConfig\n",
    "from data import TUCOREGCNDataset, TUCOREGCNDataloader\n",
    "from transformers.pipelines import pipeline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tucore_gcn_bert = TUCOREGCN_BERT(BertConfig.from_json_file(\"../pre-trained_model/BERT/bert_config.json\"), num_labels=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BERTEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (speaker_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): BERTLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BERTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BERTLayer(\n",
       "          (attention): BERTAttention(\n",
       "            (self): BERTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BERTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BERTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=6912, out_features=36, bias=True)\n",
       "  (turnAttention): MultiHeadAttention(\n",
       "    (w_qs): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (w_ks): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (w_vs): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (fc): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (GCN_layers): ModuleList(\n",
       "    (0-1): 2 x RelGraphConvLayer(\n",
       "      (activation): ReLU()\n",
       "      (conv): HeteroGraphConv(\n",
       "        (mods): ModuleDict(\n",
       "          (speaker): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          (dialog): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          (entity): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.6, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (LSTM_layers): ModuleList(\n",
       "    (0-1): 2 x TurnLevelLSTM(\n",
       "      (lstm): LSTM(768, 768, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "      (bilstm2hiddnesize): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tucore_gcn_bert.load_state_dict(torch.load(\"../TUCOREGCN_BERT_DialogRE/model_best.pt\", map_location=\"cuda\"))\n",
    "tucore_gcn_bert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dialogue = torch.LongTensor([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS]Hi! I'm Flowey, Flowey the flower![SEP]\"))])\n",
    "test_speaker = torch.LongTensor([0]*len(test_dialogue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../datasets/DialogRE/dev.json.\n",
      "load preprocessed data from ../datasets/DialogRE/dev_BERT.pkl.\n"
     ]
    }
   ],
   "source": [
    "dev_set = TUCOREGCNDataset(src_file=\"../datasets/DialogRE/dev.json\", save_file=\"../datasets/DialogRE/dev_BERT.pkl\", max_seq_length=512, tokenizer=tokenizer, n_class=36, encoder_type='BERT')\n",
    "dev_loader = TUCOREGCNDataloader(dataset=dev_set, batch_size=1, shuffle=False, relation_num=36, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,     3,  1024,  4931,   999,  5882,  1016,  1024,  4931,  1012,\n",
       "           5882,  1017,  1024,  4931,  1010,  2158,  1012,  2054,  1005,  1055,\n",
       "           2039,  1029,     3,  1024,  2672,  2017,  2064,  2425,  2033,  1012,\n",
       "           2026,  4005,  2052,  2066,  2000,  2113,  2339,  1045,  2134,  1005,\n",
       "           1056,  2265,  2039,  2012,  1996, 14597,  1045,  2134,  1005,  1056,\n",
       "           2113,  1045,  2018,  2651,  1012,  1996,  2034,  2204,  2518,  2016,\n",
       "           4152,  2033,  1999,  3134,  1012,  2129,  2071,  2017,  2025,  2507,\n",
       "           2033,  1996,  4471,  1029,   999,  5882,  1017,  1024,  2092,  1010,\n",
       "           1045,  1005,  2222,  2425,  8038,  1045,  2079,  5959,  8056,  1010,\n",
       "           2021,  1010,  6289,  1010,  2009,  2347,  1005,  1056,  2033,  1012,\n",
       "           5882,  1016,  1024,  2748,  1010,  2009,  2001,   999,  2009,  2001,\n",
       "           2032,   999,  7910,  9616,   999,  3100,  1010,  2009,  2001,  2033,\n",
       "            999,     3,  1024,  2129,  2003,  2009,  2017,  1029,  5882,  1016,\n",
       "           1024,  2092,  1010,  2009,  2001,  2074,  1010,  2009,  2001,  2035,\n",
       "           2061,  4689,  1010,  2017,  2113,  1012,  1045,  2812,  1010, 13814,\n",
       "           2001,  1999,  1996,  9346,  1010, 10320,  2000,  2184,  1010,  1998,\n",
       "           2002,  2001,  2039,  2000,  1021,  1998,  1045,  2910,  1005,  1056,\n",
       "           2179,  1037,  2173,  2000,  5342,  2664,  1012,  1045,  1011,  1045,\n",
       "           1011,  1045,  3214,  2000,  2425,  2017,  1010,  1998,  1045,  2626,\n",
       "           2009,  2035,  2091,  2006,  2026,  2192,  1012,  2156,  1010,  2035,\n",
       "           1997,  2009,  1012,     3,  1024, 15624,  1010,  2008,  1005,  1055,\n",
       "           2026, 14597,  1012,  5882,  1018,  1024,  2156,  1010,  2085,  2023,\n",
       "           2003,  2339,  1045,  2562,  3602, 15455,  2015,  7249,  1012,  5882,\n",
       "           1016,  1024, 15624,  1010,  1998,  2008,  1005,  1055,  2339,  2057,\n",
       "           2123,  1005,  1056, 13260,  2017,  2000,  2377,  1012,  5882,  1019,\n",
       "           1024,  2054,  2003,  1996,  2307, 10576,  2182,  1029,  2017,  2175,\n",
       "           2131,  4426,  2178,  6098,  1012,     3,  1024,  2092,  1010, 28517,\n",
       "           6216,  2699,  1010,  2017,  2113,  1012,  1996,  9179,  2472,  2409,\n",
       "           2014,  2008,  1045,  4771,  2026,  3382,  1012,  5882,  1016,  1024,\n",
       "           2008,  2003, 15571,  1012,  1045,  1005,  2222,  2655,  2014,  1998,\n",
       "           2425,  2014,  2009,  2001,  6135,  2026,  6346,  1012,     3,  1024,\n",
       "           6887,  4402,  5910,  1010,  2017,  2064,  1005,  1056,  2079,  2008,\n",
       "           1012,  1996,  9179,  2472,  2987,  1005,  1056,  2831,  2000,  2814,\n",
       "           1010,  2016,  2069,  7566,  2000,  6074,  1012,  5882,  1016,  1024,\n",
       "           2054,  1037,  6517,  2210,  2166,  2016,  2442,  2599,  1012,  3100,\n",
       "           1010,  1051, 11631,  1012,     3,  1024,  2054,  1010,  2054,  2024,\n",
       "           2017,  2725,  1029,  2054,  2024,  2017,  2725,  1029,  5882,  1016,\n",
       "           1024,  2053,  1010,  2053,  1010,  2053,  1010,  1045,  2113,  1010,\n",
       "           1045,  2113,  1010,  1051, 11631,  1012,  1005,  7632,  1010,  2023,\n",
       "           2003,  5736, 27610,  1010,  2013, 18188, 23176,  4710,  1005,  1055,\n",
       "           2436,  1012,  8529,  1010,  2003,  8529,  1010,  5754,  2045,  2005,\n",
       "          18188,  1010,  2016,  1005,  2222,  2113,  2054,  2009,  1005,  1055,\n",
       "           2055,  1012,  1005,     3,  1024,  6865,  2039,  1010,  6865,  2039,\n",
       "           1012,  5882,  1016,  1024,  1005,  8194,   999,  7632,  1012,  4952,\n",
       "           2057,  2288,  1037,  3291,  2007,  9558, 13012, 10322, 25443,  1010,\n",
       "           4593,  2002,  4771,  2010, 14597,  1012,  2040,  2106,  2017,  3713,\n",
       "           2000,  1999,  2026,  2436,  1029, 28517,  6216,  1010,  2053,  1010,\n",
       "           1045,  2123,  1005,  1056,  2113,  2054,  1045,  1005,  1049,  2183,\n",
       "           2000,  2079,  2007,  2014,  1012,  2053,  1012,  2035,  2157,  1010,\n",
       "           2061,  2115,  3129,  3727,  1998,  7641,  2091,  1996,  4545,  1010,\n",
       "           1996,  2088,  2515,  2025,  2644,  1012,   102, 28517,  6216,   102,\n",
       "              3,   102]], device='cuda:0'),\n",
       " 'segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'input_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'mention_ids': tensor([[ 0,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "           3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "           4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "           4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "           4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "           5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "           6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,\n",
       "           7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "           8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "           8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "           8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "           8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
       "          10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
       "          11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
       "          13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
       "          14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "          14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "          15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "          16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17,\n",
       "          17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "          18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "          18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "          18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20,\n",
       "          20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "          20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "          20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "          20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "          20, 20,  0, 21, 21,  0, 22,  0]], device='cuda:0'),\n",
       " 'speaker_ids': tensor([[ 0, 12, 12, 12, 12,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "           3,  3,  3,  3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "           3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, 12, 12, 12, 12, 12,\n",
       "          12, 12,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  4,  4,  4,\n",
       "           4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  5,  5,  5,  5,\n",
       "           5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, 12, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2, 12, 12, 12, 12, 12, 12, 12, 12,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  0,  0,  0,  0, 12,  0]], device='cuda:0'),\n",
       " 'label_ids': tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        device='cuda:0'),\n",
       " 'turn_masks': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 1, 1,  ..., 0, 0, 0],\n",
       "          [0, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 1, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 1, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 1]]], device='cuda:0'),\n",
       " 'graphs': [Graph(num_nodes={'node': 23},\n",
       "        num_edges={('node', 'dialog', 'node'): 40, ('node', 'entity', 'node'): 24, ('node', 'speaker', 'node'): 114},\n",
       "        metagraph=[('node', 'node', 'dialog'), ('node', 'node', 'entity'), ('node', 'node', 'speaker')])]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, batch = next(enumerate(dev_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "entry_idx = 0\n",
    "input_ids = batch[\"input_ids\"].to(\"cuda:0\")\n",
    "segment_ids = batch[\"segment_ids\"].to(\"cuda:0\")\n",
    "input_mask = batch[\"input_masks\"].to(\"cuda:0\")\n",
    "speaker_ids = batch[\"speaker_ids\"].to(\"cuda:0\")\n",
    "graphs = batch[\"graphs\"]\n",
    "mention_id = batch[\"mention_ids\"].to(\"cuda:0\")\n",
    "turn_mask = batch[\"turn_masks\"].to(\"cuda:0\")\n",
    "logits = tucore_gcn_bert(\n",
    "\tinput_ids,\n",
    "\tsegment_ids,\n",
    "\tinput_mask,\n",
    "\tspeaker_ids,\n",
    "\tgraphs,\n",
    "\tmention_id,\n",
    "\tNone,\n",
    "\tturn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.3505,  -2.7431,  -3.9807,  -3.2567,  -6.0214,  -3.5813,  -1.4219,\n",
       "          -4.4033,  -4.2782,  -4.9335,  -3.5914,  -6.0117,  -2.1588,  -3.8887,\n",
       "          -2.5400,  -4.5949,  -2.5093,  -6.7579, -12.2926,  -7.4864,  -9.5331,\n",
       "          -7.5495,  -8.5769,  -6.3480,  -8.5716,  -7.8853, -10.4961,  -5.8990,\n",
       "          -8.7028,  -6.0993,  -7.7270,  -7.2681, -12.7441,  -5.5965,  -6.7239,\n",
       "          -5.9486]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_outputs, pooled_outputs = tucore_gcn_bert.bert(test_dialogue, test_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2006,  0.1525,  0.4920,  ..., -0.0985, -0.3281, -0.0443],\n",
       "         [ 0.3134,  0.0418,  0.7631,  ...,  0.9421, -0.6366,  0.4469],\n",
       "         [ 0.1686,  0.0200,  0.4321,  ...,  0.3830, -0.4554, -0.3702],\n",
       "         ...,\n",
       "         [-1.1565,  0.3951,  0.5653,  ...,  0.3693, -0.8601,  0.6407],\n",
       "         [-0.4329,  0.6883,  0.4808,  ...,  0.0828, -0.3398, -0.2907],\n",
       "         [ 0.8026, -0.3991, -0.7652,  ...,  0.6569, -0.4829, -0.7216]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDPA_DROPOUT = nn.Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdpa_forward(temperature, q, k, v):\n",
    "    attn = torch.matmul(q / temperature, k.transpose(2, 3))\n",
    "    attn = SDPA_DROPOUT(F.softmax(attn, dim=-1))\n",
    "    output = torch.matmul(attn, v)\n",
    "    return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2006,  0.1525,  0.4920,  ..., -0.0985, -0.3281, -0.0443],\n",
       "          [ 0.3134,  0.0418,  0.7631,  ...,  0.9421, -0.6366,  0.4469],\n",
       "          [ 0.1686,  0.0200,  0.4321,  ...,  0.3830, -0.4554, -0.3702],\n",
       "          ...,\n",
       "          [-1.1565,  0.3951,  0.5653,  ...,  0.3693, -0.8601,  0.6407],\n",
       "          [-0.4329,  0.6883,  0.4808,  ...,  0.0828, -0.3398, -0.2907],\n",
       "          [ 0.8026, -0.3991, -0.7652,  ...,  0.6569, -0.4829, -0.7216]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.2006,  0.1525,  0.4920,  ..., -0.0985, -0.3281, -0.0443],\n",
       "          [ 0.3134,  0.0418,  0.7631,  ...,  0.9421, -0.6366,  0.4469],\n",
       "          [ 0.1686,  0.0200,  0.4321,  ...,  0.3830, -0.4554, -0.3702],\n",
       "          ...,\n",
       "          [-1.1565,  0.3951,  0.5653,  ...,  0.3693, -0.8601,  0.6407],\n",
       "          [-0.4329,  0.6883,  0.4808,  ...,  0.0828, -0.3398, -0.2907],\n",
       "          [ 0.8026, -0.3991, -0.7652,  ...,  0.6569, -0.4829, -0.7216]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.2006,  0.1525,  0.4920,  ..., -0.0985, -0.3281, -0.0443],\n",
       "          [ 0.3134,  0.0418,  0.7631,  ...,  0.9421, -0.6366,  0.4469],\n",
       "          [ 0.1686,  0.0200,  0.4321,  ...,  0.3830, -0.4554, -0.3702],\n",
       "          ...,\n",
       "          [-1.1565,  0.3951,  0.5653,  ...,  0.3693, -0.8601,  0.6407],\n",
       "          [-0.4329,  0.6883,  0.4808,  ...,  0.0828, -0.3398, -0.2907],\n",
       "          [ 0.8026, -0.3991, -0.7652,  ...,  0.6569, -0.4829, -0.7216]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = sequence_outputs\n",
    "K,V = Q,Q\n",
    "Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEAD = 12\n",
    "M_SIZE = 768\n",
    "K_SIZE = int(768/12)\n",
    "V_SIZE = int(768/12)\n",
    "TEMPERATURE = K_SIZE ** 0.5\n",
    "\n",
    "sz_b, len_q, len_k, len_v = Q.size(0), Q.size(1), K.size(1), V.size(1)\n",
    "residual = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 12, 64])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.view(sz_b, len_q, N_HEAD, K_SIZE).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 15, 64])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.view(sz_b, len_q, N_HEAD, K_SIZE).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q.view(sz_b, len_q, N_HEAD, K_SIZE).transpose(1, 2)\n",
    "k = K.view(sz_b, len_k, N_HEAD, K_SIZE).transpose(1, 2)\n",
    "v = V.view(sz_b, len_v, N_HEAD, V_SIZE).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2006,  0.1525,  0.4920,  ..., -0.2199,  0.0624, -0.3040],\n",
       "          [ 0.3134,  0.0418,  0.7631,  ..., -0.6378, -0.4597, -0.1262],\n",
       "          [ 0.1686,  0.0200,  0.4321,  ..., -0.6865,  0.1268,  0.4269],\n",
       "          ...,\n",
       "          [-1.1565,  0.3951,  0.5653,  ..., -0.7774, -0.1663, -0.9071],\n",
       "          [-0.4329,  0.6883,  0.4808,  ..., -0.4828,  0.5765, -0.4630],\n",
       "          [ 0.8026, -0.3991, -0.7652,  ..., -0.5097,  0.6836, -0.4744]],\n",
       "\n",
       "         [[ 0.5684,  0.0188, -0.0326,  ...,  0.2229,  0.0637,  0.1624],\n",
       "          [ 1.1151,  0.1873,  0.3643,  ...,  0.1153,  0.5826,  1.2940],\n",
       "          [-0.7175,  0.6431,  0.1659,  ...,  0.5098,  0.2153,  0.1611],\n",
       "          ...,\n",
       "          [ 1.1745,  0.4817,  0.7727,  ...,  1.9266, -0.2642,  1.2091],\n",
       "          [-0.2577,  0.3078,  0.5520,  ...,  0.4616,  0.1806,  0.4137],\n",
       "          [ 0.4694,  1.3367, -0.8174,  ...,  0.8301, -0.9760, -0.3622]],\n",
       "\n",
       "         [[ 0.1766, -0.1704,  0.0158,  ...,  0.5301, -0.3743, -0.0368],\n",
       "          [ 0.3716,  0.1228,  0.3375,  ...,  0.2964, -0.4685,  0.4249],\n",
       "          [ 0.6439, -0.0643,  0.3599,  ...,  0.1169, -0.9806,  0.8661],\n",
       "          ...,\n",
       "          [ 0.3193, -0.8451,  0.5717,  ...,  0.5643, -1.1341,  0.2854],\n",
       "          [ 0.3018, -0.1127,  0.1438,  ...,  0.3990, -0.4488,  0.6552],\n",
       "          [ 1.1051,  0.2355,  0.6284,  ...,  1.1832, -0.8740,  0.9700]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1791, -0.0603, -0.1646,  ...,  0.5485, -0.2763, -0.2457],\n",
       "          [ 0.9181, -0.2444, -1.1076,  ...,  0.7472,  0.8063,  0.8661],\n",
       "          [ 0.6091, -0.0982, -0.3312,  ..., -0.0868, -0.4525,  0.2298],\n",
       "          ...,\n",
       "          [ 1.1166,  0.7457,  0.2559,  ...,  0.2634, -0.5087, -0.3251],\n",
       "          [ 0.5568,  0.1395, -0.4373,  ...,  0.5584, -0.2468, -0.1626],\n",
       "          [-0.2586,  1.4848, -0.2360,  ...,  0.6164, -1.0593, -0.0452]],\n",
       "\n",
       "         [[ 0.3853,  0.3205, -0.5712,  ..., -0.6148, -0.3415,  0.0091],\n",
       "          [ 0.9156,  0.7002, -0.8910,  ..., -1.3002, -1.0720, -0.2843],\n",
       "          [-0.0223,  1.1600, -0.2748,  ..., -0.6116, -0.8750, -0.2292],\n",
       "          ...,\n",
       "          [ 0.5560,  0.4166, -0.9542,  ..., -0.1986, -0.2642, -0.2473],\n",
       "          [ 0.2429,  0.7041, -0.3451,  ..., -1.1848, -1.0460,  0.1856],\n",
       "          [-0.0717,  1.0029,  0.1719,  ..., -0.8058, -0.2687,  0.5307]],\n",
       "\n",
       "         [[ 0.0090,  0.2925, -0.4547,  ..., -0.0985, -0.3281, -0.0443],\n",
       "          [ 0.1011,  1.4887, -0.1361,  ...,  0.9421, -0.6366,  0.4469],\n",
       "          [-0.0430,  0.6857, -0.0808,  ...,  0.3830, -0.4554, -0.3702],\n",
       "          ...,\n",
       "          [ 0.0707,  1.1245,  0.3476,  ...,  0.3693, -0.8601,  0.6407],\n",
       "          [-0.0848,  0.6983, -0.2684,  ...,  0.0828, -0.3398, -0.2907],\n",
       "          [ 0.5199,  0.6629,  0.0568,  ...,  0.6569, -0.4829, -0.7216]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, attn = sdpa_forward(TEMPERATURE, q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0233, -0.0534,  0.2859,  ..., -0.4817,  0.0962, -0.0733],\n",
       "          [ 0.1556, -0.0062,  0.5206,  ..., -0.5524, -0.1295, -0.0493],\n",
       "          [ 0.0332, -0.0585,  0.3798,  ..., -0.5612,  0.0697,  0.0855],\n",
       "          ...,\n",
       "          [-0.3925,  0.0644,  0.3694,  ..., -0.6206,  0.0074, -0.3413],\n",
       "          [-0.0812,  0.0196,  0.3462,  ..., -0.5348,  0.1730, -0.0779],\n",
       "          [ 0.7348, -0.3897, -0.6663,  ..., -0.5537,  0.6545, -0.4399]],\n",
       "\n",
       "         [[ 0.4249,  0.3681,  0.0167,  ...,  0.4800,  0.1476,  0.2384],\n",
       "          [ 0.8529,  0.3746,  0.2464,  ...,  0.4189,  0.3877,  0.7853],\n",
       "          [ 0.1192,  0.5297,  0.1609,  ...,  0.5179,  0.2174,  0.2897],\n",
       "          ...,\n",
       "          [ 0.7541,  0.4675,  0.3605,  ...,  1.0866, -0.0232,  0.6736],\n",
       "          [ 0.3347,  0.4776,  0.2096,  ...,  0.5629,  0.1878,  0.3301],\n",
       "          [ 0.5412,  1.1516, -0.5498,  ...,  0.8206, -0.6543, -0.1567]],\n",
       "\n",
       "         [[ 0.7262, -0.3649,  0.0430,  ...,  0.4110, -0.4739,  0.4733],\n",
       "          [ 0.5596, -0.1258,  0.1775,  ...,  0.3773, -0.4915,  0.4824],\n",
       "          [ 0.6160, -0.3077,  0.0855,  ...,  0.2088, -0.4916,  0.5093],\n",
       "          ...,\n",
       "          [ 0.5734, -0.5547,  0.3138,  ...,  0.5285, -0.8346,  0.4013],\n",
       "          [ 0.6516, -0.2739,  0.0902,  ...,  0.3856, -0.5393,  0.5226],\n",
       "          [ 0.9466,  0.0016,  0.4276,  ...,  0.8798, -0.7448,  0.7799]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3532,  0.0664, -0.3325,  ...,  0.3392, -0.0355, -0.0716],\n",
       "          [ 0.7489, -0.1246, -0.8687,  ...,  0.5629,  0.4352,  0.4257],\n",
       "          [ 0.5483, -0.0232, -0.4203,  ...,  0.3262, -0.0905, -0.0111],\n",
       "          ...,\n",
       "          [ 0.2687, -0.0483, -0.3224,  ...,  0.2401, -0.0036, -0.0204],\n",
       "          [ 0.2795,  0.0574, -0.3376,  ...,  0.2869, -0.0943, -0.0402],\n",
       "          [ 0.0360,  0.9131, -0.3231,  ...,  0.5641, -0.6440, -0.0958]],\n",
       "\n",
       "         [[ 0.3206,  0.8053, -0.6360,  ..., -0.5637, -0.3862, -0.1826],\n",
       "          [ 0.5757,  0.9223, -0.7791,  ..., -0.9299, -0.7454, -0.2367],\n",
       "          [ 0.2596,  1.0455, -0.5900,  ..., -0.6627, -0.6759, -0.2107],\n",
       "          ...,\n",
       "          [ 0.4303,  0.9007, -0.8206,  ..., -0.5975, -0.4832, -0.2305],\n",
       "          [ 0.3388,  0.9670, -0.6692,  ..., -0.7943, -0.6391, -0.1626],\n",
       "          [-0.0060,  1.0181,  0.0181,  ..., -0.7767, -0.2993,  0.4216]],\n",
       "\n",
       "         [[ 0.1180,  0.4781, -0.0493,  ...,  0.4408, -0.4523, -0.3077],\n",
       "          [ 0.0733,  1.2725, -0.1151,  ...,  0.8605, -0.6178,  0.2391],\n",
       "          [ 0.0980,  0.8204, -0.0566,  ...,  0.6590, -0.5844, -0.2371],\n",
       "          ...,\n",
       "          [ 0.1467,  0.7405,  0.0586,  ...,  0.5898, -0.5835, -0.0238],\n",
       "          [ 0.1127,  0.7754, -0.0620,  ...,  0.6201, -0.5902, -0.2610],\n",
       "          [ 0.0580,  0.3537,  0.0134,  ...,  0.3601, -0.3125, -0.1117]]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.2006,   0.7793,   0.4920,  ...,  -5.0510,  -0.3281,   1.3801],\n",
       "         [  8.9824,   0.2507,   0.7631,  ..., -21.3476,  -6.6887,  -6.6413],\n",
       "         [ -0.0618,   1.6801,   3.0003,  ...,   0.3830,  -0.6904,  -0.3702],\n",
       "         ...,\n",
       "         [ -0.1799,   0.3951,   0.5653,  ...,  -2.7048,  -0.8601,   0.6407],\n",
       "         [ -1.4788,   1.7556,  -5.6796,  ...,   0.0828,  -1.2728,  -0.2907],\n",
       "         [ 10.8724,   0.4165,  -2.6196,  ...,   0.6569,   2.3774,  -0.7216]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "q = F.dropout(q)\n",
    "q += residual\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size, v_size, n_head = self.k_size, self.v_size, self.n_head\n",
    "sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "residual = q\n",
    "\n",
    "# Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "# Separate different heads: b x lq x n x dv\n",
    "q = self.w_qs(q).view(sz_b, len_q, n_head, k_size)\n",
    "k = self.w_ks(k).view(sz_b, len_k, n_head, k_size)\n",
    "v = self.w_vs(v).view(sz_b, len_v, n_head, v_size)\n",
    "\n",
    "# Transpose for attention dot product: b x n x lq x dv\n",
    "q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "if mask is not None:\n",
    "\tmask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "# Transpose to move the head dimension back: b x lq x n x dv\n",
    "# Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "q = self.dropout(self.fc(q))\n",
    "q += residual\n",
    "\n",
    "q = self.layer_norm(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "w_qs = F.linear(M_SIZE, N_HEAD * K_SIZE, bias=False)\n",
    "w_ks = nn.Linear(M_SIZE, N_HEAD * K_SIZE, bias=False)\n",
    "w_vs = nn.Linear(M_SIZE, N_HEAD * V_SIZE, bias=False)\n",
    "fc = nn.Linear(N_HEAD * V_SIZE, M_SIZE, bias=False)\n",
    "\n",
    "\n",
    "#attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, k_size, v_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.k_size = k_size\n",
    "        self.v_size = v_size\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * k_size, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * k_size, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * v_size, bias=False)\n",
    "        self.fc = nn.Linear(n_head * v_size, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=k_size ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        k_size, v_size, n_head = self.k_size, self.v_size, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, k_size)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, k_size)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, v_size)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
