{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDPA_DROPOUT = nn.Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdpa_forward(self, temperature, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q / temperature, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = SDPA_DROPOUT(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"hidden_size\": 768, \"hidden_act\": \"gelu\", \"initializer_range\": 0.02, \"vocab_size\": 30522, \"hidden_dropout_prob\": 0.1, \"num_attention_heads\": 12, \"type_vocab_size\": 2, \"max_position_embeddings\": 512, \"num_hidden_layers\": 12, \"intermediate_size\": 3072, \"attention_probs_dropout_prob\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEAD = 12\n",
    "M_SIZE = 768\n",
    "K_SIZE = 768/12\n",
    "V_SIZE = 768/12\n",
    "w_qs = nn.Linear(M_SIZE, N_HEAD * K_SIZE, bias=False)\n",
    "w_ks = nn.Linear(M_SIZE, N_HEAD * K_SIZE, bias=False)\n",
    "w_vs = nn.Linear(M_SIZE, N_HEAD * V_SIZE, bias=False)\n",
    "fc = nn.Linear(N_HEAD * V_SIZE, M_SIZE, bias=False)\n",
    "#attention\n",
    "DROPOUT = nn.Dropout(0.1)\n",
    "LAYER_NORM = nn.LayerNorm(M_SIZE, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHeadAttention(12, 768, 768/12, 768/12, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.n_head = n_head\n",
    "self.k_size = k_size\n",
    "self.v_size = v_size\n",
    "\n",
    "self.w_qs = nn.Linear(d_model, n_head * k_size, bias=False)\n",
    "self.w_ks = nn.Linear(d_model, n_head * k_size, bias=False)\n",
    "self.w_vs = nn.Linear(d_model, n_head * v_size, bias=False)\n",
    "self.fc = nn.Linear(n_head * v_size, d_model, bias=False)\n",
    "\n",
    "self.attention = ScaledDotProductAttention(temperature=k_size ** 0.5)\n",
    "\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size, v_size, n_head = self.k_size, self.v_size, self.n_head\n",
    "sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "residual = q\n",
    "\n",
    "# Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "# Separate different heads: b x lq x n x dv\n",
    "q = self.w_qs(q).view(sz_b, len_q, n_head, k_size)\n",
    "k = self.w_ks(k).view(sz_b, len_k, n_head, k_size)\n",
    "v = self.w_vs(v).view(sz_b, len_v, n_head, v_size)\n",
    "\n",
    "# Transpose for attention dot product: b x n x lq x dv\n",
    "q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "if mask is not None:\n",
    "\tmask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "# Transpose to move the head dimension back: b x lq x n x dv\n",
    "# Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "q = self.dropout(self.fc(q))\n",
    "q += residual\n",
    "\n",
    "q = self.layer_norm(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, k_size, v_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.k_size = k_size\n",
    "        self.v_size = v_size\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * k_size, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * k_size, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * v_size, bias=False)\n",
    "        self.fc = nn.Linear(n_head * v_size, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=k_size ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        k_size, v_size, n_head = self.k_size, self.v_size, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, k_size)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, k_size)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, v_size)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
