{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "import aqlm\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\projects\\\\affect\\\\TUCORE-GCN',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv',\n",
       " '',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages',\n",
       " 'D:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\AQLM\\\\src',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers\")\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "  (emb_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (encoder): BertEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (mixer): MHA(\n",
       "          (Wqkv): FusedDense(in_features=768, out_features=2304, bias=True)\n",
       "          (inner_attn): FlashSelfAttention(\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): FlashCrossAttention(\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_proj): FusedDense(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp): FusedMLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): FusedDense(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flash_attn.models.bert import BertModel, BertConfig\n",
    "bm = BertModel(BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\"))\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tucore_gcn_transformers.berttest import remap_state_dict\n",
    "from tucore_gcn_bert_modelling import TUCOREGCN_BertForSequenceClassification, TUCOREGCN_BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "config = TUCOREGCN_BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "bertmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "new_statedict = remap_state_dict(bertmodel.state_dict(), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "save_file(new_statedict, \"./pre-trained_model/BERT/flash_compatible.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tucoregcn_bert.bert.embeddings.word_embeddings.weight',\n",
       "              tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [-0.0174,  0.0138, -0.0058,  ...,  0.0100, -0.0070, -0.0147],\n",
       "                      [ 0.0276,  0.0204,  0.0104,  ...,  0.0056, -0.0036,  0.0004],\n",
       "                      ...,\n",
       "                      [-0.0285,  0.0349, -0.0096,  ..., -0.0025,  0.0105,  0.0040],\n",
       "                      [ 0.0393, -0.0251,  0.0253,  ..., -0.0045, -0.0162, -0.0289],\n",
       "                      [ 0.0044, -0.0194,  0.0154,  ...,  0.0369,  0.0053,  0.0077]])),\n",
       "             ('tucoregcn_bert.bert.embeddings.position_embeddings.weight',\n",
       "              tensor([[ 0.0098,  0.0252, -0.0081,  ...,  0.0010,  0.0104,  0.0320],\n",
       "                      [ 0.0169,  0.0228, -0.0012,  ...,  0.0107,  0.0369, -0.0190],\n",
       "                      [-0.0305, -0.0021, -0.0096,  ...,  0.0184,  0.0057, -0.0324],\n",
       "                      ...,\n",
       "                      [ 0.0016,  0.0117, -0.0237,  ...,  0.0277,  0.0113,  0.0046],\n",
       "                      [ 0.0157,  0.0423, -0.0162,  ...,  0.0237,  0.0159,  0.0099],\n",
       "                      [ 0.0142,  0.0150,  0.0131,  ..., -0.0274,  0.0030,  0.0013]])),\n",
       "             ('tucoregcn_bert.bert.embeddings.token_type_embeddings.weight',\n",
       "              tensor([[-0.0397, -0.0218, -0.0153,  ..., -0.0160, -0.0014, -0.0011],\n",
       "                      [-0.0192, -0.0205,  0.0161,  ..., -0.0214,  0.0195,  0.0075]])),\n",
       "             ('tucoregcn_bert.bert.embeddings.speaker_embeddings.weight',\n",
       "              tensor([[-0.0335,  0.0111,  0.0079,  ...,  0.0096, -0.0029, -0.0032],\n",
       "                      [-0.0202,  0.0312, -0.0096,  ..., -0.0111, -0.0013, -0.0373],\n",
       "                      [ 0.0017, -0.0209, -0.0123,  ...,  0.0020,  0.0122,  0.0099],\n",
       "                      ...,\n",
       "                      [ 0.0124, -0.0132, -0.0018,  ..., -0.0542, -0.0480,  0.0296],\n",
       "                      [-0.0156, -0.0178,  0.0216,  ...,  0.0256, -0.0278, -0.0079],\n",
       "                      [-0.0249, -0.0009,  0.0051,  ..., -0.0153,  0.0282, -0.0109]])),\n",
       "             ('tucoregcn_bert.bert.embeddings.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.embeddings.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.self.query.weight',\n",
       "              tensor([[-0.0329,  0.0141,  0.0124,  ..., -0.0076,  0.0133,  0.0230],\n",
       "                      [-0.0157,  0.0423,  0.0166,  ..., -0.0136,  0.0171,  0.0102],\n",
       "                      [ 0.0134,  0.0249,  0.0013,  ...,  0.0054, -0.0105, -0.0016],\n",
       "                      ...,\n",
       "                      [-0.0037,  0.0120, -0.0087,  ..., -0.0216, -0.0038, -0.0012],\n",
       "                      [ 0.0150,  0.0166,  0.0302,  ...,  0.0188, -0.0321, -0.0053],\n",
       "                      [ 0.0333,  0.0203,  0.0136,  ..., -0.0223, -0.0109,  0.0319]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.self.key.weight',\n",
       "              tensor([[ 0.0384,  0.0002,  0.0002,  ..., -0.0087,  0.0194,  0.0195],\n",
       "                      [-0.0067, -0.0104,  0.0262,  ...,  0.0425,  0.0194, -0.0138],\n",
       "                      [ 0.0039,  0.0127,  0.0211,  ...,  0.0221,  0.0014, -0.0062],\n",
       "                      ...,\n",
       "                      [-0.0112,  0.0447, -0.0249,  ..., -0.0020, -0.0076,  0.0014],\n",
       "                      [ 0.0061,  0.0106,  0.0150,  ...,  0.0313, -0.0033,  0.0188],\n",
       "                      [-0.0155, -0.0126,  0.0277,  ...,  0.0129, -0.0136, -0.0320]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.self.value.weight',\n",
       "              tensor([[-0.0100, -0.0204, -0.0259,  ..., -0.0065,  0.0185, -0.0051],\n",
       "                      [ 0.0207,  0.0030, -0.0262,  ..., -0.0058, -0.0296, -0.0134],\n",
       "                      [-0.0118, -0.0156, -0.0068,  ..., -0.0048,  0.0134,  0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0088,  0.0029, -0.0151,  ..., -0.0031, -0.0425, -0.0168],\n",
       "                      [-0.0385, -0.0030, -0.0349,  ..., -0.0516,  0.0106, -0.0059],\n",
       "                      [-0.0237, -0.0296, -0.0063,  ...,  0.0137, -0.0140,  0.0053]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.weight',\n",
       "              tensor([[ 0.0263,  0.0289, -0.0134,  ...,  0.0066,  0.0134,  0.0189],\n",
       "                      [ 0.0108,  0.0107, -0.0133,  ...,  0.0305,  0.0008, -0.0380],\n",
       "                      [ 0.0416,  0.0144,  0.0009,  ..., -0.0189, -0.0035,  0.0448],\n",
       "                      ...,\n",
       "                      [ 0.0036, -0.0106,  0.0006,  ...,  0.0154,  0.0268, -0.0098],\n",
       "                      [ 0.0054, -0.0172,  0.0009,  ..., -0.0106,  0.0012, -0.0100],\n",
       "                      [ 0.0318, -0.0013, -0.0141,  ..., -0.0391,  0.0061, -0.0169]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.weight',\n",
       "              tensor([[ 0.0150, -0.0082,  0.0033,  ...,  0.0037, -0.0006, -0.0047],\n",
       "                      [ 0.0296,  0.0065,  0.0009,  ..., -0.0291,  0.0023, -0.0058],\n",
       "                      [-0.0099,  0.0006, -0.0027,  ..., -0.0047, -0.0125,  0.0039],\n",
       "                      ...,\n",
       "                      [-0.0017, -0.0256,  0.0292,  ..., -0.0067, -0.0135, -0.0309],\n",
       "                      [-0.0044,  0.0042, -0.0237,  ..., -0.0279, -0.0241,  0.0148],\n",
       "                      [-0.0291,  0.0268,  0.0289,  ...,  0.0285, -0.0009,  0.0106]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.output.dense.weight',\n",
       "              tensor([[ 0.0181, -0.0246,  0.0178,  ...,  0.0235,  0.0016, -0.0272],\n",
       "                      [-0.0028, -0.0211,  0.0036,  ..., -0.0303,  0.0082,  0.0163],\n",
       "                      [-0.0268,  0.0023, -0.0099,  ...,  0.0066, -0.0002,  0.0035],\n",
       "                      ...,\n",
       "                      [-0.0128,  0.0042, -0.0101,  ...,  0.0410, -0.0233,  0.0044],\n",
       "                      [ 0.0051, -0.0097, -0.0114,  ...,  0.0182, -0.0400, -0.0121],\n",
       "                      [ 0.0159, -0.0124,  0.0186,  ...,  0.0436,  0.0140, -0.0194]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.self.query.weight',\n",
       "              tensor([[-0.0115,  0.0146,  0.0416,  ..., -0.0395, -0.0329, -0.0194],\n",
       "                      [ 0.0093, -0.0041, -0.0034,  ...,  0.0049, -0.0258,  0.0029],\n",
       "                      [ 0.0351,  0.0081, -0.0204,  ...,  0.0018, -0.0390,  0.0064],\n",
       "                      ...,\n",
       "                      [ 0.0304, -0.0029,  0.0171,  ..., -0.0218, -0.0201, -0.0047],\n",
       "                      [ 0.0437,  0.0011, -0.0069,  ..., -0.0090,  0.0150, -0.0220],\n",
       "                      [-0.0055, -0.0145,  0.0098,  ...,  0.0119,  0.0290,  0.0088]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.self.key.weight',\n",
       "              tensor([[-0.0120,  0.0140, -0.0321,  ..., -0.0068, -0.0092, -0.0159],\n",
       "                      [ 0.0402, -0.0089, -0.0281,  ..., -0.0230,  0.0074, -0.0162],\n",
       "                      [ 0.0077, -0.0168, -0.0191,  ..., -0.0009,  0.0424,  0.0389],\n",
       "                      ...,\n",
       "                      [-0.0283, -0.0013, -0.0101,  ..., -0.0043, -0.0099, -0.0233],\n",
       "                      [ 0.0305,  0.0059,  0.0057,  ...,  0.0026, -0.0180,  0.0124],\n",
       "                      [ 0.0252,  0.0115, -0.0138,  ..., -0.0197, -0.0072, -0.0007]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.self.value.weight',\n",
       "              tensor([[ 0.0217, -0.0004, -0.0152,  ...,  0.0185,  0.0056,  0.0188],\n",
       "                      [ 0.0230,  0.0149, -0.0024,  ..., -0.0101, -0.0063, -0.0096],\n",
       "                      [-0.0062,  0.0050, -0.0286,  ...,  0.0259, -0.0193,  0.0176],\n",
       "                      ...,\n",
       "                      [ 0.0122,  0.0253, -0.0031,  ..., -0.0006,  0.0092,  0.0075],\n",
       "                      [ 0.0423,  0.0097, -0.0337,  ...,  0.0150,  0.0068, -0.0075],\n",
       "                      [ 0.0124, -0.0075,  0.0346,  ..., -0.0039,  0.0202, -0.0025]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.weight',\n",
       "              tensor([[ 0.0437, -0.0090, -0.0253,  ..., -0.0038, -0.0271, -0.0192],\n",
       "                      [ 0.0375,  0.0254,  0.0043,  ...,  0.0238, -0.0106, -0.0288],\n",
       "                      [-0.0133, -0.0167, -0.0049,  ..., -0.0078, -0.0165, -0.0215],\n",
       "                      ...,\n",
       "                      [-0.0346,  0.0074,  0.0176,  ...,  0.0074,  0.0155,  0.0237],\n",
       "                      [-0.0200, -0.0098,  0.0124,  ...,  0.0215,  0.0056, -0.0174],\n",
       "                      [ 0.0151,  0.0111, -0.0072,  ...,  0.0079, -0.0371, -0.0081]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.weight',\n",
       "              tensor([[ 0.0043, -0.0352,  0.0148,  ..., -0.0231,  0.0073, -0.0122],\n",
       "                      [-0.0042,  0.0003,  0.0046,  ...,  0.0127, -0.0190, -0.0336],\n",
       "                      [ 0.0246, -0.0212,  0.0194,  ...,  0.0086, -0.0173,  0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0296, -0.0194, -0.0125,  ...,  0.0103,  0.0146, -0.0061],\n",
       "                      [-0.0036, -0.0369, -0.0148,  ..., -0.0238,  0.0040, -0.0089],\n",
       "                      [-0.0190, -0.0214,  0.0023,  ..., -0.0099,  0.0158, -0.0145]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.output.dense.weight',\n",
       "              tensor([[-0.0161,  0.0196, -0.0380,  ..., -0.0202, -0.0002, -0.0035],\n",
       "                      [ 0.0066, -0.0038,  0.0209,  ...,  0.0194,  0.0021,  0.0137],\n",
       "                      [ 0.0052,  0.0174, -0.0147,  ..., -0.0066, -0.0002, -0.0064],\n",
       "                      ...,\n",
       "                      [ 0.0088, -0.0301,  0.0052,  ..., -0.0064,  0.0040,  0.0118],\n",
       "                      [ 0.0176, -0.0041,  0.0103,  ..., -0.0124,  0.0209,  0.0114],\n",
       "                      [-0.0048,  0.0219,  0.0169,  ...,  0.0127, -0.0023,  0.0168]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.self.query.weight',\n",
       "              tensor([[-0.0285,  0.0156, -0.0093,  ...,  0.0068,  0.0062,  0.0043],\n",
       "                      [-0.0148, -0.0071,  0.0108,  ...,  0.0256,  0.0196,  0.0209],\n",
       "                      [-0.0666,  0.0275, -0.0075,  ...,  0.0097,  0.0110, -0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0285,  0.0204,  0.0001,  ...,  0.0175,  0.0083,  0.0078],\n",
       "                      [ 0.0195,  0.0272, -0.0390,  ...,  0.0024,  0.0018,  0.0053],\n",
       "                      [-0.0196,  0.0058,  0.0110,  ..., -0.0181, -0.0024,  0.0449]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.self.key.weight',\n",
       "              tensor([[-0.0026,  0.0170,  0.0193,  ...,  0.0225,  0.0051,  0.0231],\n",
       "                      [-0.0113, -0.0114,  0.0153,  ...,  0.0308, -0.0219, -0.0242],\n",
       "                      [ 0.0135, -0.0080, -0.0081,  ...,  0.0023, -0.0011, -0.0022],\n",
       "                      ...,\n",
       "                      [-0.0266,  0.0116,  0.0290,  ...,  0.0079, -0.0042,  0.0165],\n",
       "                      [-0.0131,  0.0467,  0.0120,  ...,  0.0151,  0.0059,  0.0307],\n",
       "                      [ 0.0147, -0.0319, -0.0188,  ..., -0.0548,  0.0360, -0.0081]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.self.value.weight',\n",
       "              tensor([[-2.4689e-02,  7.0425e-04,  5.8489e-03,  ..., -4.3386e-03,\n",
       "                       -5.2388e-03,  3.8304e-02],\n",
       "                      [ 2.1677e-02,  1.2546e-02,  2.4059e-02,  ..., -2.6071e-02,\n",
       "                       -1.9393e-02, -6.2261e-02],\n",
       "                      [ 1.6238e-03,  1.0944e-02, -2.1031e-05,  ...,  6.6714e-03,\n",
       "                       -4.8142e-02,  2.3336e-02],\n",
       "                      ...,\n",
       "                      [-1.8585e-02,  1.3280e-03, -3.9256e-02,  ...,  3.3025e-03,\n",
       "                        2.5858e-03,  2.1866e-02],\n",
       "                      [ 3.3152e-03, -1.7985e-02,  8.6071e-03,  ..., -2.5713e-02,\n",
       "                        1.1307e-02, -1.2171e-02],\n",
       "                      [-1.6667e-02,  2.7195e-02, -1.3939e-02,  ...,  4.3246e-03,\n",
       "                       -1.4304e-03,  5.7205e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.weight',\n",
       "              tensor([[ 0.0343, -0.0282, -0.0138,  ..., -0.0081,  0.0257,  0.0247],\n",
       "                      [-0.0241,  0.0017, -0.0095,  ...,  0.0078,  0.0013, -0.0137],\n",
       "                      [ 0.0275,  0.0053,  0.0327,  ...,  0.0263,  0.0597, -0.0217],\n",
       "                      ...,\n",
       "                      [ 0.0182, -0.0026, -0.0079,  ...,  0.0368, -0.0298,  0.0130],\n",
       "                      [-0.0169, -0.0022, -0.0156,  ...,  0.0159, -0.0011,  0.0215],\n",
       "                      [-0.0329, -0.0006,  0.0299,  ..., -0.0085,  0.0169,  0.0141]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.weight',\n",
       "              tensor([[ 0.0146, -0.0166,  0.0172,  ...,  0.0140, -0.0103, -0.0309],\n",
       "                      [-0.0035,  0.0238, -0.0017,  ..., -0.0215, -0.0133,  0.0142],\n",
       "                      [-0.0212,  0.0057,  0.0328,  ...,  0.0011,  0.0061,  0.0345],\n",
       "                      ...,\n",
       "                      [ 0.0113,  0.0051, -0.0394,  ..., -0.0016,  0.0076,  0.0043],\n",
       "                      [-0.0155,  0.0173,  0.0100,  ..., -0.0130,  0.0151, -0.0079],\n",
       "                      [-0.0103, -0.0060, -0.0133,  ...,  0.0260,  0.0137,  0.0113]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.output.dense.weight',\n",
       "              tensor([[-0.0161,  0.0093,  0.0086,  ...,  0.0368, -0.0254,  0.0102],\n",
       "                      [-0.0397, -0.0089,  0.0093,  ..., -0.0069,  0.0497, -0.0306],\n",
       "                      [ 0.0316, -0.0040, -0.0147,  ...,  0.0265, -0.0020, -0.0170],\n",
       "                      ...,\n",
       "                      [-0.0107,  0.0003,  0.0442,  ..., -0.0241,  0.0037,  0.0027],\n",
       "                      [-0.0153,  0.0071,  0.0030,  ...,  0.0178,  0.0193,  0.0373],\n",
       "                      [-0.0350, -0.0202,  0.0034,  ..., -0.0057,  0.0377,  0.0133]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.self.query.weight',\n",
       "              tensor([[-0.0545, -0.0080,  0.0210,  ..., -0.0242, -0.0050, -0.0157],\n",
       "                      [-0.0097,  0.0165, -0.0074,  ..., -0.0547, -0.0399, -0.0101],\n",
       "                      [ 0.0199, -0.0047,  0.0341,  ...,  0.0179, -0.0195, -0.0068],\n",
       "                      ...,\n",
       "                      [-0.0201, -0.0297, -0.0185,  ...,  0.0011,  0.0111,  0.0127],\n",
       "                      [-0.0160, -0.0015,  0.0090,  ...,  0.0379,  0.0210,  0.0307],\n",
       "                      [-0.0124, -0.0443, -0.0167,  ..., -0.0033, -0.0137,  0.0105]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.self.key.weight',\n",
       "              tensor([[ 1.9227e-02,  1.2659e-02,  3.6910e-02,  ...,  1.0638e-02,\n",
       "                        2.4613e-02,  4.6647e-02],\n",
       "                      [ 2.7553e-02, -1.6576e-02, -3.6277e-02,  ...,  1.8331e-03,\n",
       "                        7.2558e-03, -2.7841e-02],\n",
       "                      [-2.1968e-05,  2.6007e-02, -9.4741e-03,  ...,  9.9116e-03,\n",
       "                        2.1968e-02, -1.1069e-02],\n",
       "                      ...,\n",
       "                      [ 2.7718e-02, -1.2514e-02,  4.0784e-03,  ..., -3.7833e-03,\n",
       "                        1.8164e-02,  1.2284e-02],\n",
       "                      [-1.4355e-02, -2.6484e-02,  1.4967e-03,  ...,  1.7592e-02,\n",
       "                        4.3401e-03,  5.2161e-02],\n",
       "                      [ 1.2926e-03,  1.1970e-02, -9.1914e-03,  ...,  3.0623e-02,\n",
       "                       -7.9548e-03,  2.5218e-02]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.self.value.weight',\n",
       "              tensor([[ 0.0204,  0.0357, -0.0159,  ...,  0.0041, -0.0428,  0.0240],\n",
       "                      [ 0.0082, -0.0047,  0.0391,  ...,  0.0322,  0.0179,  0.0177],\n",
       "                      [ 0.0203, -0.0032,  0.0134,  ...,  0.0087, -0.0031, -0.0072],\n",
       "                      ...,\n",
       "                      [ 0.0026,  0.0062, -0.0014,  ...,  0.0373,  0.0465, -0.0173],\n",
       "                      [ 0.0411, -0.0184,  0.0188,  ..., -0.0082,  0.0598,  0.0116],\n",
       "                      [ 0.0143,  0.0173, -0.0274,  ..., -0.0098, -0.0349, -0.0073]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.weight',\n",
       "              tensor([[ 0.0031,  0.0206,  0.0047,  ...,  0.0053,  0.0254,  0.0423],\n",
       "                      [ 0.0426, -0.0056,  0.0168,  ...,  0.0184,  0.0219,  0.0267],\n",
       "                      [-0.0110, -0.0035,  0.0244,  ..., -0.0331,  0.0085,  0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0122, -0.0296,  0.0155,  ..., -0.0071, -0.0155, -0.0120],\n",
       "                      [ 0.0304,  0.0051,  0.0303,  ..., -0.0038, -0.0237,  0.0106],\n",
       "                      [-0.0016,  0.0038,  0.0020,  ...,  0.0004,  0.0370, -0.0210]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.weight',\n",
       "              tensor([[-0.0072, -0.0109,  0.0085,  ..., -0.0162,  0.0072,  0.0227],\n",
       "                      [-0.0295, -0.0037,  0.0004,  ...,  0.0259, -0.0405,  0.0152],\n",
       "                      [-0.0301, -0.0137,  0.0234,  ..., -0.0283, -0.0135, -0.0107],\n",
       "                      ...,\n",
       "                      [ 0.0009, -0.0243, -0.0014,  ...,  0.0073,  0.0171,  0.0040],\n",
       "                      [-0.0215,  0.0116, -0.0004,  ...,  0.0036,  0.0110, -0.0021],\n",
       "                      [ 0.0049,  0.0324, -0.0045,  ...,  0.0011, -0.0091,  0.0115]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.output.dense.weight',\n",
       "              tensor([[-0.0002,  0.0193, -0.0225,  ...,  0.0322,  0.0295, -0.0249],\n",
       "                      [ 0.0158,  0.0265,  0.0151,  ...,  0.0179,  0.0442, -0.0123],\n",
       "                      [-0.0314,  0.0063,  0.0033,  ...,  0.0201, -0.0041,  0.0202],\n",
       "                      ...,\n",
       "                      [-0.0100, -0.0091, -0.0251,  ..., -0.0111,  0.0083, -0.0356],\n",
       "                      [-0.0342, -0.0127, -0.0098,  ...,  0.0062,  0.0221,  0.0171],\n",
       "                      [-0.0012,  0.0133, -0.0052,  ...,  0.0043, -0.0270,  0.0091]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.self.query.weight',\n",
       "              tensor([[-0.0600,  0.0125, -0.0139,  ..., -0.0095, -0.0415,  0.0193],\n",
       "                      [ 0.0357,  0.0070,  0.0263,  ...,  0.0017,  0.0221, -0.0017],\n",
       "                      [-0.0266,  0.0038,  0.0387,  ..., -0.0084, -0.0146, -0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0091, -0.0158,  0.0162,  ..., -0.0191,  0.0091, -0.0022],\n",
       "                      [ 0.0064, -0.0308,  0.0201,  ...,  0.0068,  0.0155, -0.0093],\n",
       "                      [-0.0249, -0.0167, -0.0155,  ..., -0.0110, -0.0243,  0.0036]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.self.key.weight',\n",
       "              tensor([[ 6.6978e-03,  1.8415e-02,  4.1203e-03,  ...,  1.1000e-02,\n",
       "                       -8.6623e-04, -4.5808e-03],\n",
       "                      [-1.2120e-02,  5.0739e-03, -6.0475e-03,  ...,  8.8851e-03,\n",
       "                        2.1567e-03,  1.2471e-02],\n",
       "                      [-2.2943e-03,  6.4041e-03, -4.0813e-02,  ..., -1.7381e-03,\n",
       "                        3.4433e-02, -2.8634e-02],\n",
       "                      ...,\n",
       "                      [ 1.4762e-02, -1.1269e-02, -2.3079e-05,  ...,  6.1533e-05,\n",
       "                        3.7944e-02,  2.0506e-04],\n",
       "                      [-3.2333e-02, -8.5884e-03,  1.7399e-02,  ..., -3.2116e-02,\n",
       "                        2.6997e-02,  5.4982e-02],\n",
       "                      [-1.7490e-02,  1.2297e-02, -2.2206e-02,  ..., -6.2859e-03,\n",
       "                        1.9803e-02,  5.3663e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.self.value.weight',\n",
       "              tensor([[ 1.2463e-02,  3.7427e-02,  7.4637e-03,  ..., -2.7938e-02,\n",
       "                       -3.5414e-02,  1.4859e-02],\n",
       "                      [-1.0111e-02,  1.7074e-02,  8.3488e-03,  ...,  1.4077e-02,\n",
       "                       -1.0507e-03,  8.8623e-03],\n",
       "                      [-1.9542e-02, -4.2902e-02, -1.6459e-02,  ...,  1.0872e-02,\n",
       "                       -1.2947e-02, -4.2801e-03],\n",
       "                      ...,\n",
       "                      [-2.8586e-02, -3.2280e-03,  8.4353e-03,  ..., -1.3890e-02,\n",
       "                        2.0393e-02,  5.8486e-05],\n",
       "                      [ 2.3080e-02, -3.8552e-03, -2.4324e-02,  ..., -1.5763e-02,\n",
       "                        5.9632e-03, -6.8862e-03],\n",
       "                      [ 2.8108e-02, -1.5964e-02,  6.8794e-04,  ..., -7.9168e-04,\n",
       "                       -7.6699e-03, -3.2128e-02]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.weight',\n",
       "              tensor([[-0.0021, -0.0113, -0.0044,  ..., -0.0050,  0.0145,  0.0194],\n",
       "                      [-0.0362,  0.0123,  0.0109,  ...,  0.0100, -0.0077, -0.0139],\n",
       "                      [ 0.0495, -0.0110,  0.0213,  ..., -0.0025,  0.0014, -0.0234],\n",
       "                      ...,\n",
       "                      [-0.0146, -0.0320,  0.0057,  ...,  0.0260,  0.0047, -0.0073],\n",
       "                      [-0.0112, -0.0135, -0.0041,  ..., -0.0363, -0.0145, -0.0184],\n",
       "                      [ 0.0009,  0.0027,  0.0326,  ...,  0.0223,  0.0182, -0.0111]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.weight',\n",
       "              tensor([[-0.0014, -0.0027,  0.0130,  ...,  0.0228, -0.0216, -0.0266],\n",
       "                      [ 0.0376,  0.0646,  0.0235,  ...,  0.0077, -0.0014, -0.0057],\n",
       "                      [-0.0050,  0.0243, -0.0371,  ...,  0.0151,  0.0062,  0.0054],\n",
       "                      ...,\n",
       "                      [-0.0079,  0.0138,  0.0124,  ..., -0.0181,  0.0410, -0.0233],\n",
       "                      [-0.0090,  0.0117, -0.0137,  ..., -0.0146,  0.0177,  0.0128],\n",
       "                      [-0.0090,  0.0034, -0.0311,  ...,  0.0043,  0.0052, -0.0050]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.output.dense.weight',\n",
       "              tensor([[ 3.9163e-03, -2.6821e-02,  1.1311e-02,  ...,  1.2818e-03,\n",
       "                        1.6083e-03, -1.2016e-02],\n",
       "                      [-3.5759e-04, -8.7125e-03, -1.2109e-02,  ...,  3.7141e-02,\n",
       "                       -3.7753e-02, -3.1658e-02],\n",
       "                      [-2.8699e-02, -2.2454e-02,  9.3610e-03,  ..., -2.9853e-03,\n",
       "                       -1.5720e-02,  3.3122e-02],\n",
       "                      ...,\n",
       "                      [ 3.0713e-02,  1.1729e-02,  1.5532e-05,  ..., -3.3026e-02,\n",
       "                        9.9977e-03,  1.0202e-02],\n",
       "                      [-3.6739e-03,  2.7488e-03, -2.4345e-02,  ...,  2.7332e-02,\n",
       "                       -3.5959e-03,  9.0202e-03],\n",
       "                      [-5.7439e-03, -2.5409e-02, -3.2055e-02,  ..., -1.3242e-03,\n",
       "                       -1.6904e-02,  4.1367e-02]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.self.query.weight',\n",
       "              tensor([[ 0.0124,  0.0102, -0.0073,  ...,  0.0169,  0.0139,  0.0328],\n",
       "                      [-0.0410, -0.0090,  0.0235,  ..., -0.0338,  0.0116, -0.0023],\n",
       "                      [-0.0282,  0.0050,  0.0229,  ...,  0.0294, -0.0239, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0121,  0.0099, -0.0313,  ...,  0.0066, -0.0005, -0.0149],\n",
       "                      [ 0.0091, -0.0028,  0.0130,  ...,  0.0038,  0.0359, -0.0312],\n",
       "                      [-0.0007, -0.0119, -0.0431,  ..., -0.0037, -0.0259, -0.0037]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.self.key.weight',\n",
       "              tensor([[-2.4947e-03,  6.7835e-04, -2.1242e-03,  ..., -5.9063e-03,\n",
       "                        5.8719e-03,  2.4261e-02],\n",
       "                      [ 8.9639e-03, -3.1850e-02,  6.6322e-03,  ..., -2.9972e-02,\n",
       "                        8.6893e-03,  1.5842e-02],\n",
       "                      [-9.7818e-03,  3.1745e-02, -2.1845e-03,  ..., -9.9026e-03,\n",
       "                        8.8581e-03, -1.3804e-02],\n",
       "                      ...,\n",
       "                      [ 5.8411e-02, -2.6027e-03,  2.5850e-02,  ...,  2.4152e-02,\n",
       "                        4.7786e-03, -4.8512e-05],\n",
       "                      [-1.5725e-02,  2.2028e-02,  2.7181e-02,  ..., -1.3053e-03,\n",
       "                        4.3318e-04, -1.2184e-02],\n",
       "                      [ 1.0202e-02,  3.2034e-02,  3.9081e-04,  ...,  2.5503e-03,\n",
       "                       -1.3275e-02, -2.5895e-02]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.self.value.weight',\n",
       "              tensor([[-0.0241,  0.0272, -0.0163,  ..., -0.0475,  0.0253, -0.0152],\n",
       "                      [ 0.0094,  0.0084, -0.0403,  ..., -0.0089,  0.0004, -0.0178],\n",
       "                      [-0.0156,  0.0095,  0.0157,  ...,  0.0125,  0.0048,  0.0015],\n",
       "                      ...,\n",
       "                      [-0.0126,  0.0187,  0.0066,  ..., -0.0138,  0.0032, -0.0244],\n",
       "                      [ 0.0136,  0.0130, -0.0149,  ...,  0.0026, -0.0076,  0.0220],\n",
       "                      [ 0.0170,  0.0158, -0.0157,  ..., -0.0095,  0.0027, -0.0193]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.weight',\n",
       "              tensor([[ 0.0347,  0.0110, -0.0171,  ..., -0.0074,  0.0292, -0.0170],\n",
       "                      [ 0.0326,  0.0232,  0.0104,  ..., -0.0280,  0.0159,  0.0140],\n",
       "                      [ 0.0326,  0.0245, -0.0128,  ..., -0.0061,  0.0344,  0.0416],\n",
       "                      ...,\n",
       "                      [-0.0350,  0.0194, -0.0265,  ..., -0.0024,  0.0073,  0.0097],\n",
       "                      [ 0.0261, -0.0003,  0.0064,  ..., -0.0011, -0.0035, -0.0161],\n",
       "                      [-0.0030, -0.0291, -0.0346,  ..., -0.0208,  0.0243,  0.0156]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.weight',\n",
       "              tensor([[-0.0046, -0.0025,  0.0301,  ...,  0.0185, -0.0075,  0.0143],\n",
       "                      [-0.0109, -0.0255,  0.0164,  ...,  0.0232, -0.0322, -0.0114],\n",
       "                      [-0.0043,  0.0270, -0.0144,  ...,  0.0056, -0.0181,  0.0222],\n",
       "                      ...,\n",
       "                      [ 0.0119, -0.0159,  0.0210,  ...,  0.0195, -0.0039, -0.0543],\n",
       "                      [-0.0270,  0.0158,  0.0007,  ..., -0.0240, -0.0324,  0.0174],\n",
       "                      [ 0.0223,  0.0246, -0.0076,  ..., -0.0107,  0.0271, -0.0158]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.output.dense.weight',\n",
       "              tensor([[-0.0044,  0.0349, -0.0266,  ..., -0.0153,  0.0123,  0.0044],\n",
       "                      [ 0.0005,  0.0279, -0.0252,  ...,  0.0071, -0.0066,  0.0100],\n",
       "                      [ 0.0108, -0.0327,  0.0248,  ...,  0.0246, -0.0070,  0.0233],\n",
       "                      ...,\n",
       "                      [ 0.0246, -0.0027,  0.0107,  ...,  0.0068,  0.0027,  0.0061],\n",
       "                      [-0.0160, -0.0128,  0.0189,  ...,  0.0247, -0.0120,  0.0182],\n",
       "                      [ 0.0228, -0.0350,  0.0081,  ..., -0.0074, -0.0174,  0.0041]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.self.query.weight',\n",
       "              tensor([[ 0.0234, -0.0289, -0.0254,  ...,  0.0134, -0.0273,  0.0078],\n",
       "                      [-0.0078, -0.0183, -0.0116,  ...,  0.0163,  0.0037, -0.0071],\n",
       "                      [ 0.0298,  0.0100, -0.0019,  ..., -0.0198,  0.0040, -0.0222],\n",
       "                      ...,\n",
       "                      [ 0.0011, -0.0003,  0.0004,  ...,  0.0036, -0.0040,  0.0152],\n",
       "                      [-0.0004,  0.0062,  0.0354,  ..., -0.0096, -0.0021,  0.0019],\n",
       "                      [-0.0321,  0.0064, -0.0217,  ...,  0.0416,  0.0073,  0.0094]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.self.key.weight',\n",
       "              tensor([[-0.0358,  0.0064,  0.0082,  ..., -0.0235,  0.0032,  0.0167],\n",
       "                      [ 0.0058, -0.0231, -0.0042,  ...,  0.0175, -0.0026,  0.0259],\n",
       "                      [-0.0168, -0.0034, -0.0213,  ..., -0.0410,  0.0322, -0.0274],\n",
       "                      ...,\n",
       "                      [ 0.0099,  0.0240, -0.0056,  ..., -0.0028, -0.0230, -0.0218],\n",
       "                      [-0.0225,  0.0160,  0.0064,  ...,  0.0172, -0.0395,  0.0296],\n",
       "                      [ 0.0254,  0.0381,  0.0118,  ...,  0.0120,  0.0141,  0.0034]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.self.value.weight',\n",
       "              tensor([[-0.0074, -0.0042, -0.0121,  ..., -0.0068, -0.0312, -0.0022],\n",
       "                      [ 0.0131, -0.0138, -0.0208,  ..., -0.0070,  0.0074, -0.0010],\n",
       "                      [ 0.0010,  0.0308,  0.0141,  ...,  0.0138,  0.0091,  0.0236],\n",
       "                      ...,\n",
       "                      [ 0.0250,  0.0422,  0.0034,  ..., -0.0346, -0.0405,  0.0149],\n",
       "                      [ 0.0078,  0.0071,  0.0071,  ..., -0.0158, -0.0383,  0.0083],\n",
       "                      [ 0.0102,  0.0206, -0.0078,  ..., -0.0110,  0.0102, -0.0035]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.weight',\n",
       "              tensor([[-0.0069,  0.0103, -0.0246,  ..., -0.0456,  0.0287,  0.0036],\n",
       "                      [-0.0034, -0.0160,  0.0260,  ...,  0.0159, -0.0307,  0.0468],\n",
       "                      [-0.0050, -0.0102,  0.0173,  ..., -0.0350, -0.0142, -0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0026, -0.0101, -0.0049,  ...,  0.0159, -0.0367,  0.0086],\n",
       "                      [ 0.0207, -0.0177, -0.0355,  ..., -0.0034,  0.0194,  0.0095],\n",
       "                      [ 0.0178, -0.0101, -0.0363,  ..., -0.0005,  0.0159,  0.0109]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.weight',\n",
       "              tensor([[-2.0690e-02, -1.1774e-02, -3.3136e-02,  ..., -1.7455e-02,\n",
       "                       -6.5167e-03,  2.5648e-02],\n",
       "                      [-3.4594e-02, -1.9910e-02, -6.0178e-03,  ..., -2.8175e-02,\n",
       "                       -9.7138e-05, -4.1798e-02],\n",
       "                      [-2.6749e-02,  3.0385e-02,  5.1883e-03,  ...,  1.8789e-02,\n",
       "                        3.3543e-02, -5.9402e-03],\n",
       "                      ...,\n",
       "                      [ 7.6658e-03,  2.6856e-02,  5.3730e-03,  ...,  6.7813e-03,\n",
       "                       -2.6045e-02,  1.2177e-02],\n",
       "                      [-3.1651e-02,  3.2871e-02,  5.2048e-03,  ..., -1.2926e-02,\n",
       "                       -4.7656e-03,  1.6316e-02],\n",
       "                      [-1.5417e-02,  2.1490e-02, -1.1659e-02,  ..., -1.6522e-02,\n",
       "                        4.0113e-02, -3.0518e-02]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.output.dense.weight',\n",
       "              tensor([[-0.0258,  0.0089, -0.0064,  ..., -0.0281,  0.0095, -0.0069],\n",
       "                      [ 0.0344, -0.0024, -0.0012,  ..., -0.0215,  0.0069,  0.0151],\n",
       "                      [-0.0157, -0.0075,  0.0311,  ..., -0.0021,  0.0152,  0.0069],\n",
       "                      ...,\n",
       "                      [-0.0377, -0.0521, -0.0187,  ..., -0.0179, -0.0026,  0.0228],\n",
       "                      [-0.0127, -0.0199, -0.0380,  ...,  0.0024, -0.0101, -0.0270],\n",
       "                      [-0.0133,  0.0095,  0.0043,  ...,  0.0141,  0.0131, -0.0145]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.self.query.weight',\n",
       "              tensor([[-0.0237, -0.0198, -0.0029,  ..., -0.0053, -0.0112,  0.0107],\n",
       "                      [ 0.0255, -0.0301, -0.0020,  ..., -0.0039, -0.0058,  0.0242],\n",
       "                      [-0.0204, -0.0322,  0.0375,  ...,  0.0190,  0.0103, -0.0140],\n",
       "                      ...,\n",
       "                      [-0.0183,  0.0008, -0.0432,  ..., -0.0104, -0.0229,  0.0089],\n",
       "                      [-0.0071, -0.0321,  0.0161,  ..., -0.0297, -0.0283,  0.0536],\n",
       "                      [ 0.0070, -0.0130,  0.0221,  ...,  0.0175,  0.0022,  0.0284]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.self.key.weight',\n",
       "              tensor([[-1.0397e-02,  2.5892e-02, -3.0713e-03,  ...,  4.0210e-03,\n",
       "                       -2.4019e-02, -2.3935e-02],\n",
       "                      [-1.5341e-02, -4.3246e-03, -1.7279e-02,  ..., -1.2816e-02,\n",
       "                        1.1714e-02,  1.0425e-02],\n",
       "                      [-1.9462e-02, -1.3687e-03,  3.0209e-03,  ..., -1.5141e-02,\n",
       "                       -3.1891e-02,  1.9052e-02],\n",
       "                      ...,\n",
       "                      [-3.0478e-02, -1.1358e-02,  6.7812e-03,  ...,  5.4798e-03,\n",
       "                        2.9659e-02, -7.0171e-03],\n",
       "                      [ 2.3457e-02,  1.9817e-02, -1.6320e-02,  ..., -1.4095e-02,\n",
       "                        6.0612e-03, -1.3365e-02],\n",
       "                      [ 9.2763e-03, -1.6039e-02, -3.4265e-03,  ..., -2.1178e-02,\n",
       "                        4.0748e-06,  4.1926e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.self.value.weight',\n",
       "              tensor([[-1.6198e-02, -1.5366e-02, -1.7396e-02,  ...,  2.4337e-03,\n",
       "                        4.7632e-03, -1.7394e-02],\n",
       "                      [ 2.2595e-02,  4.2164e-02, -3.0359e-02,  ..., -2.0329e-02,\n",
       "                        1.7161e-03, -1.1679e-02],\n",
       "                      [-3.4401e-02,  2.6343e-03,  4.0046e-02,  ..., -7.5731e-03,\n",
       "                        8.2489e-05, -2.4405e-03],\n",
       "                      ...,\n",
       "                      [ 2.3179e-02, -1.6680e-02,  3.3953e-02,  ...,  1.9620e-02,\n",
       "                       -3.3634e-03, -3.4961e-03],\n",
       "                      [ 3.5457e-02,  2.7402e-02,  9.1187e-03,  ...,  5.4518e-03,\n",
       "                        2.1682e-02, -6.0807e-02],\n",
       "                      [-2.6360e-03, -1.7933e-02, -2.4202e-02,  ..., -7.8280e-03,\n",
       "                       -7.1825e-03, -8.8251e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.weight',\n",
       "              tensor([[-1.2410e-03,  1.4140e-02,  8.2837e-03,  ..., -6.8718e-03,\n",
       "                       -3.5419e-02, -1.0859e-02],\n",
       "                      [-3.4155e-02,  2.8461e-02,  1.1107e-02,  ...,  2.5418e-05,\n",
       "                        3.6704e-02,  4.9691e-03],\n",
       "                      [-1.9828e-02, -2.0757e-02, -5.6947e-03,  ...,  9.7707e-03,\n",
       "                        1.8556e-02,  8.9151e-03],\n",
       "                      ...,\n",
       "                      [-3.3395e-02,  3.7529e-03, -4.9277e-03,  ..., -5.8213e-03,\n",
       "                       -1.4962e-02,  1.7991e-02],\n",
       "                      [ 1.0118e-02,  2.1362e-03, -2.0068e-03,  ..., -2.3142e-02,\n",
       "                        1.0719e-02, -2.1715e-02],\n",
       "                      [ 2.6160e-02, -2.0669e-03,  1.4133e-02,  ...,  6.6850e-03,\n",
       "                       -5.9726e-03,  5.1197e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.weight',\n",
       "              tensor([[-1.3726e-02, -3.9020e-03,  6.2867e-03,  ..., -1.4826e-02,\n",
       "                        5.1892e-02,  3.2340e-02],\n",
       "                      [ 5.9595e-04,  1.2464e-02, -6.9441e-03,  ..., -1.4470e-02,\n",
       "                       -5.9432e-03,  6.2268e-03],\n",
       "                      [ 3.0411e-02,  2.6094e-02,  4.8340e-02,  ..., -7.0617e-05,\n",
       "                        3.3853e-02,  5.3623e-02],\n",
       "                      ...,\n",
       "                      [-1.3760e-02, -1.3305e-03,  1.8511e-02,  ...,  6.8958e-03,\n",
       "                       -3.9727e-02,  1.6934e-02],\n",
       "                      [-7.3908e-03, -1.0130e-02,  4.0545e-02,  ..., -1.0679e-02,\n",
       "                        1.5802e-03, -3.0426e-02],\n",
       "                      [-1.1217e-02, -1.5394e-03,  2.1624e-02,  ..., -7.2988e-03,\n",
       "                       -5.1864e-03,  4.0727e-02]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.output.dense.weight',\n",
       "              tensor([[ 0.0203, -0.0354, -0.0150,  ...,  0.0009, -0.0107,  0.0137],\n",
       "                      [ 0.0105,  0.0144,  0.0348,  ...,  0.0155,  0.0129,  0.0092],\n",
       "                      [-0.0057, -0.0082, -0.0221,  ...,  0.0169, -0.0023,  0.0102],\n",
       "                      ...,\n",
       "                      [-0.0058,  0.0103,  0.0215,  ..., -0.0218, -0.0158,  0.0222],\n",
       "                      [ 0.0048,  0.0151, -0.0485,  ...,  0.0406,  0.0065, -0.0033],\n",
       "                      [ 0.0106,  0.0209, -0.0050,  ..., -0.0035,  0.0002, -0.0396]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.self.query.weight',\n",
       "              tensor([[-0.0344,  0.0131,  0.0146,  ..., -0.0031,  0.0052, -0.0357],\n",
       "                      [-0.0206, -0.0068, -0.0255,  ..., -0.0075,  0.0324, -0.0045],\n",
       "                      [-0.0180, -0.0015, -0.0082,  ...,  0.0265, -0.0161,  0.0017],\n",
       "                      ...,\n",
       "                      [-0.0262, -0.0317,  0.0077,  ..., -0.0129, -0.0090, -0.0220],\n",
       "                      [-0.0071,  0.0279, -0.0189,  ...,  0.0237, -0.0194,  0.0191],\n",
       "                      [ 0.0027, -0.0087, -0.0050,  ..., -0.0107,  0.0103, -0.0080]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.self.key.weight',\n",
       "              tensor([[-0.0119, -0.0460, -0.0192,  ..., -0.0251,  0.0007, -0.0056],\n",
       "                      [-0.0004, -0.0152,  0.0109,  ..., -0.0327, -0.0050, -0.0104],\n",
       "                      [-0.0098,  0.0176, -0.0043,  ...,  0.0127,  0.0075,  0.0197],\n",
       "                      ...,\n",
       "                      [ 0.0133, -0.0250,  0.0005,  ...,  0.0159, -0.0080,  0.0035],\n",
       "                      [-0.0201, -0.0576,  0.0327,  ...,  0.0215,  0.0013,  0.0234],\n",
       "                      [ 0.0093,  0.0056,  0.0336,  ...,  0.0396, -0.0027,  0.0072]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.self.value.weight',\n",
       "              tensor([[ 3.7120e-03,  9.9605e-03,  2.6290e-02,  ..., -1.3759e-02,\n",
       "                        2.0092e-02, -8.7588e-03],\n",
       "                      [ 1.8944e-02,  1.9562e-02, -4.4375e-03,  ...,  3.2424e-03,\n",
       "                       -1.2462e-02,  6.3146e-03],\n",
       "                      [ 2.7770e-03, -2.2686e-02, -3.2686e-02,  ..., -5.2658e-04,\n",
       "                        1.1554e-02, -9.6716e-05],\n",
       "                      ...,\n",
       "                      [ 7.0276e-03,  2.7059e-03, -5.9545e-03,  ...,  2.3068e-02,\n",
       "                        1.9666e-02, -1.3341e-02],\n",
       "                      [-1.7090e-02,  8.4235e-03,  1.2588e-02,  ..., -1.6354e-02,\n",
       "                       -3.6429e-02, -1.3818e-02],\n",
       "                      [ 7.7162e-03,  3.4747e-02,  1.8559e-02,  ...,  2.1155e-02,\n",
       "                       -3.5413e-03, -7.6241e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.weight',\n",
       "              tensor([[-0.0044,  0.0027,  0.0150,  ...,  0.0503,  0.0221, -0.0052],\n",
       "                      [-0.0069,  0.0100,  0.0240,  ...,  0.0197,  0.0181, -0.0248],\n",
       "                      [-0.0030,  0.0020,  0.0185,  ...,  0.0213, -0.0043,  0.0060],\n",
       "                      ...,\n",
       "                      [ 0.0117,  0.0099, -0.0055,  ...,  0.0149,  0.0337,  0.0051],\n",
       "                      [ 0.0041,  0.0151, -0.0008,  ..., -0.0009,  0.0096, -0.0283],\n",
       "                      [-0.0004, -0.0124,  0.0212,  ..., -0.0065, -0.0199,  0.0147]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.weight',\n",
       "              tensor([[ 0.0063, -0.0360, -0.0204,  ...,  0.0216,  0.0194,  0.0039],\n",
       "                      [-0.0279, -0.0375, -0.0399,  ..., -0.0403,  0.0188,  0.0304],\n",
       "                      [ 0.0041, -0.0162,  0.0078,  ...,  0.0070, -0.0302, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0099,  0.0314,  0.0020,  ...,  0.0280,  0.0292, -0.0025],\n",
       "                      [-0.0089,  0.0048, -0.0134,  ...,  0.0164, -0.0393,  0.0097],\n",
       "                      [ 0.0269, -0.0024,  0.0132,  ..., -0.0323, -0.0024, -0.0103]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.output.dense.weight',\n",
       "              tensor([[-0.0162,  0.0104, -0.0318,  ...,  0.0085, -0.0067,  0.0425],\n",
       "                      [-0.0156,  0.0027, -0.0130,  ...,  0.0115,  0.0100,  0.0112],\n",
       "                      [-0.0157, -0.0131, -0.0027,  ...,  0.0238, -0.0075,  0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0128, -0.0123, -0.0032,  ..., -0.0033, -0.0158, -0.0403],\n",
       "                      [ 0.0242, -0.0013,  0.0169,  ..., -0.0311, -0.0026,  0.0050],\n",
       "                      [-0.0107,  0.0183, -0.0049,  ..., -0.0006,  0.0106, -0.0296]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.self.query.weight',\n",
       "              tensor([[-0.0037,  0.0331, -0.0246,  ...,  0.0370, -0.0074, -0.0246],\n",
       "                      [ 0.0165, -0.0241, -0.0284,  ..., -0.0301, -0.0042, -0.0467],\n",
       "                      [-0.0241, -0.0209,  0.0027,  ..., -0.0154, -0.0049, -0.0176],\n",
       "                      ...,\n",
       "                      [ 0.0049,  0.0287, -0.0089,  ..., -0.0206, -0.0048,  0.0291],\n",
       "                      [-0.0214,  0.0120, -0.0003,  ..., -0.0114,  0.0076, -0.0172],\n",
       "                      [ 0.0046,  0.0083, -0.0105,  ..., -0.0101, -0.0304, -0.0075]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.self.key.weight',\n",
       "              tensor([[-2.5334e-03, -2.1107e-05, -2.3048e-02,  ..., -1.9666e-02,\n",
       "                        1.9834e-02, -4.6669e-03],\n",
       "                      [ 1.4788e-03, -2.2779e-02, -9.9004e-03,  ..., -1.0131e-02,\n",
       "                       -1.1412e-02,  4.6656e-03],\n",
       "                      [-2.4621e-02, -9.3573e-03, -1.2110e-02,  ..., -2.6714e-02,\n",
       "                       -1.6471e-02,  1.1099e-02],\n",
       "                      ...,\n",
       "                      [ 2.9801e-02, -3.6305e-02, -1.4332e-02,  ..., -1.9350e-03,\n",
       "                        2.7956e-02,  1.8817e-02],\n",
       "                      [ 7.4263e-03,  1.4403e-02, -1.7948e-02,  ..., -1.8261e-02,\n",
       "                       -5.6261e-04, -1.2184e-02],\n",
       "                      [ 1.8859e-02, -5.3662e-03,  3.5683e-04,  ...,  2.2731e-02,\n",
       "                        2.0589e-02,  4.3320e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.self.value.weight',\n",
       "              tensor([[-0.0290,  0.0311, -0.0089,  ...,  0.0171, -0.0003, -0.0187],\n",
       "                      [ 0.0201,  0.0223,  0.0074,  ..., -0.0277, -0.0271,  0.0005],\n",
       "                      [ 0.0314, -0.0051, -0.0014,  ..., -0.0105,  0.0079, -0.0132],\n",
       "                      ...,\n",
       "                      [-0.0105,  0.0053, -0.0040,  ..., -0.0186,  0.0131, -0.0063],\n",
       "                      [ 0.0174,  0.0002,  0.0197,  ..., -0.0076,  0.0229, -0.0005],\n",
       "                      [-0.0009,  0.0177,  0.0005,  ..., -0.0045,  0.0208,  0.0133]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.weight',\n",
       "              tensor([[ 0.0224,  0.0093,  0.0183,  ...,  0.0181,  0.0143,  0.0191],\n",
       "                      [-0.0377,  0.0171,  0.0286,  ...,  0.0202,  0.0252, -0.0110],\n",
       "                      [-0.0008,  0.0335, -0.0154,  ..., -0.0309, -0.0271, -0.0107],\n",
       "                      ...,\n",
       "                      [-0.0430,  0.0046,  0.0121,  ..., -0.0081, -0.0223, -0.0011],\n",
       "                      [-0.0233,  0.0055,  0.0040,  ...,  0.0116, -0.0307, -0.0038],\n",
       "                      [ 0.0200,  0.0050, -0.0037,  ...,  0.0322, -0.0091,  0.0179]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.weight',\n",
       "              tensor([[-0.0106, -0.0162, -0.0171,  ...,  0.0104, -0.0322, -0.0052],\n",
       "                      [-0.0006,  0.0332,  0.0166,  ..., -0.0203, -0.0023,  0.0178],\n",
       "                      [-0.0112,  0.0059, -0.0080,  ...,  0.0022,  0.0149, -0.0190],\n",
       "                      ...,\n",
       "                      [ 0.0019,  0.0266,  0.0347,  ...,  0.0160, -0.0275, -0.0257],\n",
       "                      [ 0.0048,  0.0136,  0.0220,  ..., -0.0112,  0.0092, -0.0119],\n",
       "                      [ 0.0006,  0.0397,  0.0009,  ...,  0.0236,  0.0304, -0.0023]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.output.dense.weight',\n",
       "              tensor([[-0.0336, -0.0361,  0.0045,  ...,  0.0212,  0.0063,  0.0017],\n",
       "                      [ 0.0180, -0.0174,  0.0296,  ..., -0.0062,  0.0101, -0.0126],\n",
       "                      [-0.0048, -0.0142,  0.0387,  ..., -0.0056,  0.0165, -0.0142],\n",
       "                      ...,\n",
       "                      [-0.0180,  0.0069, -0.0038,  ...,  0.0120,  0.0078,  0.0100],\n",
       "                      [-0.0312,  0.0028, -0.0125,  ..., -0.0332,  0.0225,  0.0018],\n",
       "                      [ 0.0207,  0.0060,  0.0152,  ..., -0.0096,  0.0191,  0.0080]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.self.query.weight',\n",
       "              tensor([[-0.0085,  0.0100,  0.0037,  ..., -0.0065,  0.0080,  0.0081],\n",
       "                      [ 0.0034,  0.0043, -0.0120,  ...,  0.0244,  0.0076, -0.0207],\n",
       "                      [-0.0062,  0.0089, -0.0095,  ..., -0.0236, -0.0023, -0.0219],\n",
       "                      ...,\n",
       "                      [-0.0362, -0.0303,  0.0282,  ...,  0.0237, -0.0091, -0.0024],\n",
       "                      [-0.0067, -0.0155,  0.0011,  ...,  0.0226, -0.0216, -0.0002],\n",
       "                      [ 0.0242,  0.0211, -0.0157,  ..., -0.0385,  0.0468,  0.0223]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.self.key.weight',\n",
       "              tensor([[-0.0234,  0.0014,  0.0118,  ..., -0.0045, -0.0045,  0.0093],\n",
       "                      [ 0.0227,  0.0021, -0.0014,  ...,  0.0067, -0.0190, -0.0176],\n",
       "                      [ 0.0400,  0.0157,  0.0083,  ..., -0.0248, -0.0219, -0.0285],\n",
       "                      ...,\n",
       "                      [ 0.0264, -0.0361, -0.0159,  ..., -0.0258, -0.0231,  0.0032],\n",
       "                      [ 0.0042, -0.0089, -0.0079,  ...,  0.0115,  0.0208, -0.0105],\n",
       "                      [ 0.0127, -0.0116, -0.0435,  ..., -0.0195,  0.0205,  0.0080]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.self.value.weight',\n",
       "              tensor([[ 0.0210,  0.0110, -0.0027,  ...,  0.0082, -0.0216, -0.0102],\n",
       "                      [-0.0269,  0.0116, -0.0005,  ..., -0.0247,  0.0105,  0.0119],\n",
       "                      [ 0.0021, -0.0187,  0.0232,  ..., -0.0192,  0.0027, -0.0310],\n",
       "                      ...,\n",
       "                      [ 0.0095,  0.0278,  0.0129,  ...,  0.0151, -0.0127, -0.0298],\n",
       "                      [-0.0071, -0.0307,  0.0350,  ...,  0.0107, -0.0181, -0.0206],\n",
       "                      [-0.0032,  0.0125,  0.0196,  ..., -0.0066,  0.0135, -0.0295]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.weight',\n",
       "              tensor([[-0.0300, -0.0095, -0.0477,  ...,  0.0224,  0.0041,  0.0219],\n",
       "                      [-0.0098,  0.0070, -0.0092,  ...,  0.0075, -0.0060,  0.0090],\n",
       "                      [-0.0227,  0.0386,  0.0118,  ..., -0.0013,  0.0092,  0.0350],\n",
       "                      ...,\n",
       "                      [-0.0089, -0.0374,  0.0179,  ...,  0.0135, -0.0074, -0.0057],\n",
       "                      [ 0.0155, -0.0127, -0.0127,  ..., -0.0324, -0.0045, -0.0060],\n",
       "                      [-0.0134,  0.0102, -0.0284,  ...,  0.0041, -0.0045, -0.0166]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.weight',\n",
       "              tensor([[-0.0497, -0.0107, -0.0137,  ...,  0.0246,  0.0179, -0.0190],\n",
       "                      [-0.0077, -0.0181, -0.0136,  ...,  0.0121,  0.0258, -0.0070],\n",
       "                      [ 0.0059,  0.0273, -0.0029,  ..., -0.0148, -0.0120, -0.0135],\n",
       "                      ...,\n",
       "                      [ 0.0039,  0.0153,  0.0147,  ...,  0.0178,  0.0048,  0.0075],\n",
       "                      [-0.0175, -0.0015,  0.0007,  ..., -0.0052, -0.0044,  0.0327],\n",
       "                      [ 0.0057,  0.0257,  0.0099,  ..., -0.0037,  0.0521,  0.0284]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.output.dense.weight',\n",
       "              tensor([[ 2.5483e-02, -1.3195e-03, -4.0893e-02,  ...,  2.1216e-02,\n",
       "                       -1.8157e-02, -1.5581e-02],\n",
       "                      [-3.0191e-02, -3.0639e-02,  3.9092e-02,  ..., -2.8376e-03,\n",
       "                       -6.7593e-02,  2.3117e-03],\n",
       "                      [ 3.4853e-02,  2.5827e-02, -4.0274e-03,  ..., -1.2433e-02,\n",
       "                        4.4922e-03,  2.8622e-02],\n",
       "                      ...,\n",
       "                      [-2.3693e-02,  3.6981e-02, -2.2886e-02,  ...,  7.4784e-03,\n",
       "                        1.3532e-02, -7.8801e-05],\n",
       "                      [ 1.6187e-02,  5.5857e-03, -1.5335e-02,  ...,  1.2349e-02,\n",
       "                        3.7336e-02,  2.4810e-02],\n",
       "                      [ 1.5955e-02, -2.2002e-02, -1.3945e-02,  ..., -1.1985e-02,\n",
       "                        1.3431e-02, -6.6740e-03]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.self.query.weight',\n",
       "              tensor([[-0.0276, -0.0313,  0.0173,  ..., -0.0037,  0.0007, -0.0469],\n",
       "                      [ 0.0046,  0.0279, -0.0151,  ...,  0.0307, -0.0016, -0.0087],\n",
       "                      [ 0.0042, -0.0047,  0.0188,  ...,  0.0108, -0.0156, -0.0080],\n",
       "                      ...,\n",
       "                      [ 0.0018, -0.0076, -0.0353,  ..., -0.0122, -0.0233,  0.0042],\n",
       "                      [-0.0176, -0.0136, -0.0033,  ..., -0.0213, -0.0042, -0.0405],\n",
       "                      [ 0.0242,  0.0116, -0.0512,  ...,  0.0003,  0.0316,  0.0305]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.self.query.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.self.key.weight',\n",
       "              tensor([[ 0.0123,  0.0211, -0.0021,  ...,  0.0035, -0.0136, -0.0015],\n",
       "                      [ 0.0096, -0.0032, -0.0262,  ...,  0.0214,  0.0015,  0.0253],\n",
       "                      [ 0.0114,  0.0259, -0.0077,  ..., -0.0016,  0.0402, -0.0102],\n",
       "                      ...,\n",
       "                      [-0.0067,  0.0042, -0.0207,  ...,  0.0315,  0.0210,  0.0097],\n",
       "                      [-0.0188,  0.0107,  0.0339,  ..., -0.0089, -0.0106, -0.0121],\n",
       "                      [-0.0011, -0.0346,  0.0041,  ...,  0.0477,  0.0114, -0.0098]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.self.key.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.self.value.weight',\n",
       "              tensor([[ 0.0044,  0.0168,  0.0232,  ...,  0.0061,  0.0048,  0.0150],\n",
       "                      [ 0.0370, -0.0250, -0.0227,  ..., -0.0208, -0.0031,  0.0295],\n",
       "                      [ 0.0209,  0.0279,  0.0247,  ..., -0.0111,  0.0284,  0.0051],\n",
       "                      ...,\n",
       "                      [-0.0172,  0.0061,  0.0140,  ..., -0.0181, -0.0061, -0.0154],\n",
       "                      [ 0.0018, -0.0202,  0.0169,  ...,  0.0206,  0.0063,  0.0058],\n",
       "                      [-0.0154, -0.0053, -0.0006,  ...,  0.0154,  0.0644,  0.0005]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.self.value.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.weight',\n",
       "              tensor([[-0.0256, -0.0008, -0.0044,  ..., -0.0326,  0.0227, -0.0232],\n",
       "                      [ 0.0399,  0.0041,  0.0383,  ..., -0.0293, -0.0425,  0.0197],\n",
       "                      [-0.0384, -0.0079, -0.0003,  ...,  0.0093, -0.0305,  0.0074],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0132, -0.0077,  ..., -0.0313, -0.0069, -0.0022],\n",
       "                      [-0.0204, -0.0131,  0.0182,  ..., -0.0188, -0.0228,  0.0371],\n",
       "                      [-0.0100,  0.0148,  0.0049,  ..., -0.0073,  0.0069, -0.0152]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.weight',\n",
       "              tensor([[ 0.0363, -0.0132,  0.0234,  ..., -0.0343, -0.0380, -0.0045],\n",
       "                      [ 0.0231,  0.0103, -0.0294,  ...,  0.0080, -0.0104,  0.0158],\n",
       "                      [ 0.0279,  0.0208,  0.0254,  ..., -0.0177, -0.0171, -0.0080],\n",
       "                      ...,\n",
       "                      [-0.0143,  0.0420, -0.0022,  ..., -0.0025,  0.0088, -0.0106],\n",
       "                      [-0.0361, -0.0043,  0.0030,  ...,  0.0158,  0.0142, -0.0201],\n",
       "                      [ 0.0183, -0.0344,  0.0120,  ...,  0.0097,  0.0178,  0.0425]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.output.dense.weight',\n",
       "              tensor([[-0.0068,  0.0454, -0.0073,  ..., -0.0262, -0.0230, -0.0017],\n",
       "                      [ 0.0060,  0.0130,  0.0270,  ...,  0.0133,  0.0300, -0.0272],\n",
       "                      [ 0.0009, -0.0023, -0.0068,  ..., -0.0024, -0.0203,  0.0016],\n",
       "                      ...,\n",
       "                      [-0.0147,  0.0046, -0.0170,  ...,  0.0159, -0.0189,  0.0316],\n",
       "                      [ 0.0258,  0.0395, -0.0286,  ..., -0.0235,  0.0063,  0.0341],\n",
       "                      [ 0.0140, -0.0197,  0.0272,  ..., -0.0107, -0.0128,  0.0077]])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.output.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.bert.pooler.dense.weight',\n",
       "              tensor([[ 0.0066, -0.0162, -0.0117,  ..., -0.0062,  0.0010,  0.0054],\n",
       "                      [ 0.0008, -0.0054,  0.0051,  ..., -0.0026, -0.0093, -0.0070],\n",
       "                      [-0.0128,  0.0077,  0.0027,  ..., -0.0229, -0.0137, -0.0202],\n",
       "                      ...,\n",
       "                      [-0.0115,  0.0019, -0.0232,  ..., -0.0102,  0.0343,  0.0051],\n",
       "                      [-0.0023,  0.0071, -0.0532,  ...,  0.0458,  0.0068,  0.0191],\n",
       "                      [-0.0367, -0.0032, -0.0393,  ..., -0.0028, -0.0071, -0.0169]])),\n",
       "             ('tucoregcn_bert.bert.pooler.dense.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.turnAttention.w_qs.weight',\n",
       "              tensor([[ 0.0002, -0.0004,  0.0144,  ..., -0.0172,  0.0296,  0.0319],\n",
       "                      [ 0.0113, -0.0081, -0.0044,  ...,  0.0249, -0.0386,  0.0081],\n",
       "                      [ 0.0067, -0.0056, -0.0131,  ..., -0.0001,  0.0268, -0.0037],\n",
       "                      ...,\n",
       "                      [ 0.0203,  0.0249, -0.0047,  ..., -0.0119, -0.0442, -0.0204],\n",
       "                      [-0.0004, -0.0198,  0.0069,  ..., -0.0213, -0.0022, -0.0107],\n",
       "                      [ 0.0239,  0.0010, -0.0006,  ...,  0.0074,  0.0453,  0.0395]])),\n",
       "             ('tucoregcn_bert.turnAttention.w_ks.weight',\n",
       "              tensor([[ 0.0085, -0.0208,  0.0027,  ..., -0.0315,  0.0085, -0.0026],\n",
       "                      [ 0.0081, -0.0047,  0.0380,  ..., -0.0116, -0.0119, -0.0011],\n",
       "                      [-0.0164, -0.0050,  0.0122,  ..., -0.0160, -0.0103,  0.0198],\n",
       "                      ...,\n",
       "                      [-0.0122,  0.0131,  0.0220,  ...,  0.0126,  0.0005, -0.0165],\n",
       "                      [-0.0211, -0.0269,  0.0091,  ...,  0.0279,  0.0414,  0.0044],\n",
       "                      [ 0.0221,  0.0044, -0.0291,  ...,  0.0035,  0.0065,  0.0206]])),\n",
       "             ('tucoregcn_bert.turnAttention.w_vs.weight',\n",
       "              tensor([[-0.0026,  0.0028, -0.0092,  ..., -0.0059,  0.0135, -0.0060],\n",
       "                      [-0.0088,  0.0220, -0.0247,  ...,  0.0012,  0.0022,  0.0250],\n",
       "                      [ 0.0102, -0.0220, -0.0254,  ...,  0.0270,  0.0017, -0.0221],\n",
       "                      ...,\n",
       "                      [ 0.0267, -0.0155,  0.0128,  ..., -0.0134, -0.0111, -0.0256],\n",
       "                      [ 0.0011,  0.0113, -0.0247,  ...,  0.0095,  0.0261, -0.0170],\n",
       "                      [ 0.0273, -0.0192,  0.0088,  ...,  0.0118, -0.0330, -0.0048]])),\n",
       "             ('tucoregcn_bert.turnAttention.out_lin.weight',\n",
       "              tensor([[-0.0082, -0.0065,  0.0050,  ..., -0.0201, -0.0058,  0.0281],\n",
       "                      [-0.0048, -0.0196, -0.0251,  ...,  0.0065, -0.0289,  0.0134],\n",
       "                      [ 0.0140, -0.0089,  0.0264,  ..., -0.0041, -0.0171,  0.0140],\n",
       "                      ...,\n",
       "                      [ 0.0003, -0.0078, -0.0142,  ...,  0.0092,  0.0063, -0.0080],\n",
       "                      [ 0.0342, -0.0054, -0.0187,  ..., -0.0041, -0.0005, -0.0087],\n",
       "                      [ 0.0245, -0.0140, -0.0424,  ...,  0.0231, -0.0130, -0.0294]])),\n",
       "             ('tucoregcn_bert.turnAttention.layer_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('tucoregcn_bert.turnAttention.layer_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.GCN_layers.0.weight',\n",
       "              tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "              \n",
       "                      [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "              \n",
       "                      [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]]])),\n",
       "             ('tucoregcn_bert.GCN_layers.0.h_bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.GCN_layers.0.loop_weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.GCN_layers.1.weight',\n",
       "              tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "              \n",
       "                      [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "              \n",
       "                      [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]]])),\n",
       "             ('tucoregcn_bert.GCN_layers.1.h_bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.GCN_layers.1.loop_weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.weight',\n",
       "              tensor([[-0.0018, -0.0159,  0.0011,  ...,  0.0132, -0.0213, -0.0236],\n",
       "                      [ 0.0073, -0.0241,  0.0263,  ..., -0.0039, -0.0096, -0.0264],\n",
       "                      [-0.0050,  0.0080,  0.0281,  ..., -0.0110, -0.0334, -0.0279],\n",
       "                      ...,\n",
       "                      [-0.0178, -0.0141,  0.0438,  ..., -0.0309, -0.0173, -0.0265],\n",
       "                      [ 0.0235,  0.0065,  0.0492,  ...,  0.0153,  0.0071, -0.0245],\n",
       "                      [ 0.0086,  0.0141, -0.0293,  ..., -0.0045,  0.0391,  0.0262]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1_reverse',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.weight',\n",
       "              tensor([[ 0.0190,  0.0097, -0.0035,  ..., -0.0165,  0.0495, -0.0487],\n",
       "                      [-0.0003, -0.0101, -0.0237,  ..., -0.0070, -0.0086,  0.0016],\n",
       "                      [ 0.0153,  0.0157, -0.0119,  ..., -0.0045,  0.0112, -0.0079],\n",
       "                      ...,\n",
       "                      [ 0.0227, -0.0112, -0.0054,  ...,  0.0061,  0.0024, -0.0233],\n",
       "                      [-0.0063,  0.0395,  0.0069,  ...,  0.0266, -0.0318,  0.0169],\n",
       "                      [-0.0018, -0.0049, -0.0068,  ...,  0.0115,  0.0036, -0.0006]])),\n",
       "             ('tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('classifier.weight',\n",
       "              tensor([[-0.0208, -0.0236,  0.0290,  ...,  0.0250,  0.0114, -0.0005],\n",
       "                      [ 0.0051, -0.0417, -0.0145,  ...,  0.0033, -0.0160,  0.0021],\n",
       "                      [ 0.0063,  0.0227, -0.0257,  ...,  0.0002, -0.0193,  0.0156],\n",
       "                      ...,\n",
       "                      [ 0.0040, -0.0154, -0.0329,  ..., -0.0284, -0.0147,  0.0262],\n",
       "                      [-0.0121, -0.0129, -0.0255,  ..., -0.0459,  0.0413,  0.0158],\n",
       "                      [ 0.0222,  0.0202, -0.0130,  ..., -0.0305, -0.0037, -0.0007]])),\n",
       "             ('classifier.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertmodel.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5239, 0.5308, 0.5342,  ..., 0.4512, 0.6802, 0.8755],\n",
       "          [0.0210, 0.5815, 0.8091,  ..., 1.0566, 0.7070, 1.1670],\n",
       "          [0.1770, 0.9795, 0.9004,  ..., 0.9893, 0.6558, 1.0498],\n",
       "          ...,\n",
       "          [0.2417, 0.0913, 0.7861,  ..., 0.7217, 0.8843, 0.6719],\n",
       "          [0.4082, 0.9844, 0.6338,  ..., 0.6611, 0.3215, 1.3887],\n",
       "          [0.0468, 0.6470, 0.8389,  ..., 0.4219, 1.0020, 0.7178]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.random.manual_seed(9)\n",
    "# coding=utf-8\n",
    "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n",
    "# and OPT implementations in this library. It has been modified from its\n",
    "# original forms to accommodate minor architectural differences compared\n",
    "# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" PyTorch LLaMA model.\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    SequenceClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "import os\n",
    "\n",
    "if is_flash_attn_2_available():\n",
    "    os.add_dll_directory(os.path.join(os.environ['CUDA_PATH'], 'bin'))\n",
    "    from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
    "    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n",
    "\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"LlamaConfig\"\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LLAMA_ATTENTION_CLASSES, LlamaMLP, LlamaRMSNorm\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*):\n",
    "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
    "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "        if \"padding_mask\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
    "            )\n",
    "        # For fused:\n",
    "        # Dropout -> Add -> LN\n",
    "    \t# -> MHA ->\n",
    "        # Dropout -> Add -> LN\n",
    "        # -> MLP\n",
    "        residual = hidden_states\n",
    "        # LN\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        # LN -> MHA -> Dropout\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # LN -> MHA -> Dropout -> Add\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        # LN -> MHA -> Dropout -> Add -> LN\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        # LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "dl = LlamaDecoderLayer(LlamaConfig.from_json_file(\"./llama_config.json\"), 0).to(\"cuda\", dtype=torch.float16)\n",
    "dl(torch.rand(((1, 512, 2048)), device=\"cuda\", dtype=torch.float16), attention_mask = torch.ones((1, 512), device=\"cuda\", dtype=torch.float16), position_ids = torch.tensor([list(range(0,512))], device=\"cuda\", dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn_bert_no_way import BertModel as FlashBertModel, BertConfig as FlashBertConfig\n",
    "model = FlashBertModel(FlashBertConfig.from_json_file(\"./pre-trained_model/BERT/bert_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "  (emb_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (encoder): BertEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (mixer): MHA(\n",
       "          (Wqkv): LinearResidual(in_features=768, out_features=2304, bias=True)\n",
       "          (inner_attn): SelfAttention(\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): CrossAttention(\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForPreTraining\n",
    "tmodel = AutoModelForPreTraining.from_pretrained('bert-base-uncased')\n",
    "tmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## galore testing + introspecting flashattn for nopad speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'openxla_eval', 'tvm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn.flash_attn_interface import flash_attn_varlen_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 µs ± 52.8 µs per loop (mean ± std. dev. of 64 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 64\n",
    "flash_attn_varlen_func(\n",
    "\ttorch.rand((429,4,64), dtype=torch.float16, device=\"cuda\"),\n",
    "\ttorch.rand((429,4,64), dtype=torch.float16, device=\"cuda\"),\n",
    "\ttorch.rand((429,4,64), dtype=torch.float16, device=\"cuda\"),\n",
    "\ttorch.tensor([0, 226, 429], dtype=torch.int32, device=\"cuda\"),\n",
    "\ttorch.tensor([0, 226, 429], dtype=torch.int32, device=\"cuda\"),\n",
    "\t226,\n",
    "\t226,\n",
    "\t0.0, None, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 64\n",
    "flash_attn_varlen_func(\n",
    "\ttorch.rand((429,4,64), dtype=torch.float16, device=\"cuda\"),\n",
    "\ttorch.rand((429,4,64), dtype=torch.float16, device=\"cuda\"),\n",
    "\ttorch.rand((429,4,64), dtype=torch.float16, device=\"cuda\"),\n",
    "\ttorch.tensor([0, 226, 429], dtype=torch.int32, device=\"cuda\"),\n",
    "\ttorch.tensor([0, 226, 429], dtype=torch.int32, device=\"cuda\"),\n",
    "\t512,\n",
    "\t512,\n",
    "\t0.0, None, True\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267885568"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaModel, AutoTokenizer, LlamaForCausalLM, LlamaConfig, LlamaForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf\")\n",
    "# _attn_implementation: flash_attention_2\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    \"BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf\",\n",
    "\tconfig=LlamaConfig.from_json_file(\"./llama_config.json\"),\n",
    "    ignore_mismatched_sizes=True,\n",
    "\ttorch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True\n",
    ")\n",
    "model.cuda()\n",
    "model.train()\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50,475,008 || all params: 318,348,288 || trainable%: 15.855278606053004\n"
     ]
    }
   ],
   "source": [
    "from peft import AdaLoraConfig, get_peft_model, IA3Model, IA3Config, LoraConfig\n",
    "\n",
    "ada_lora_config = AdaLoraConfig(\n",
    "    r=8,\n",
    "    init_r=12,\n",
    "    tinit=200,\n",
    "    tfinal=1000,\n",
    "    deltaT=10,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    modules_to_save=[\"score\"],\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=256,\n",
    "\tinference_mode = False,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "ia3_config = IA3Config(\n",
    "    peft_type=\"IA3\",\n",
    "    target_modules=[\"key\", \"value\", \"dense\"],\n",
    "    feedforward_modules=[\"dense\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.get_parameter(\"base_model.model.score.weight\").requires_grad = True\n",
    "peft_model.get_parameter(\"base_model.model.score.weight\").data = peft_model.get_parameter(\"base_model.model.score.weight\").data.to(torch.float32)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048, padding_idx=2)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaFlashAttention2(\n",
       "              (q_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): Linear(in_features=2048, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model\n",
    "# lora_A/lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora.aqlm import AqlmLoraLinear\n",
    "galore_params = []\n",
    "target_modules_list = [\"self_attn\", \"mlp\"]\n",
    "for module_name, module in peft_model.named_modules():\n",
    "    if not isinstance(module, AqlmLoraLinear):\n",
    "        continue\n",
    "\n",
    "    if not any(target_key in module_name for target_key in target_modules_list):\n",
    "        continue\n",
    "    \n",
    "    galore_params.append(module.lora_A.default.weight)\n",
    "    galore_params.append(module.lora_B.default.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_galore_params = [id(p) for p in galore_params]\n",
    "# make parameters without \"rank\" to another group\n",
    "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
    "# then call galore_adamw\n",
    "param_groups = [{'params': regular_params}, \n",
    "\t\t\t\t{'params': galore_params, 'rank': 64, 'update_proj_gap': 200, 'scale': 0.25, 'proj_type': 'std'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from galore_torch import GaLoreAdamW8bit\n",
    "optimizer = GaLoreAdamW8bit(param_groups, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): QuantizedLinear()\n",
       "          (k_proj): QuantizedLinear()\n",
       "          (v_proj): QuantizedLinear()\n",
       "          (o_proj): QuantizedLinear()\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantizedLinear()\n",
       "          (up_proj): QuantizedLinear()\n",
       "          (down_proj): QuantizedLinear()\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=2048, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 4, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, _get_unpad_data\n",
    "from flash_attn.bert_padding import unpad_input, index_first_axis\n",
    "pos_ids = torch.tensor([list(range(0,11)),list(range(0,11))], dtype=torch.long, device='cuda')\n",
    "atn_mask = torch.tensor([[1,1,1,1,1,1,1,1,1,0,0],[1,1,1,1,1,1,1,1,0,0,0]], device='cuda', dtype=torch.long)\n",
    "h = model.model.layers[0].input_layernorm(\n",
    "\tmodel.model.embed_tokens(\n",
    "\t\ttorch.LongTensor([\n",
    "\t\t\ttokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I want pumpkin pie!!!!<s><s>\"))),\n",
    "\t\t\ttokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I want pumpkin pie.<s><s><s>\")))\n",
    "\t\t]).cuda()\n",
    "\t)\n",
    ")\n",
    "q = model.model.layers[0].self_attn.q_proj(h).view(2, 11, 32, 64).transpose(1, 2)\n",
    "k = model.model.layers[0].self_attn.k_proj(h).view(2, 11, 4, 64).transpose(1, 2)\n",
    "v = model.model.layers[0].self_attn.v_proj(h).view(2, 11, 4, 64).transpose(1, 2)\n",
    "cos, sin = model.model.layers[0].self_attn.rotary_emb(v, pos_ids)\n",
    "q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "query_states = q.transpose(1, 2)\n",
    "key_states = k.transpose(1, 2)\n",
    "value_states = v.transpose(1, 2)\n",
    "indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(atn_mask)\n",
    "batch_size, kv_seq_len, num_key_value_heads, head_dim = key_states.shape\n",
    "index_first_axis(\n",
    "\tkey_states.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (9).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m\t\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_inputs_with_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI want pumpkin pie!!!!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m306\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m864\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m282\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3427\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9089\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5036\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6824\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6824\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1425\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1424\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1425\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1427\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (9)."
     ]
    }
   ],
   "source": [
    "model(\n",
    "\t\n",
    "\tlabels = torch.LongTensor([1, 306, 864, 282, 3427, 9089, 5036, 6824, 6824]).cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"]=\"affect\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"soda_lora.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "\t'xIntent': 0,\n",
    "\t'xReact': 1,\n",
    "\t'xAttr': 2,\n",
    "\t'xEffect': 3,\n",
    "\t'xWant': 4,\n",
    "\t'xNeed': 5\n",
    "}\n",
    "id2label = {\n",
    "\t0: 'xIntent',\n",
    "\t1: 'xReact',\n",
    "\t2: 'xAttr',\n",
    "\t3: 'xEffect',\n",
    "\t4: 'xWant',\n",
    "\t5: 'xNeed'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_from_disk(\"./datasets/soda/stringified_inputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogre = datasets.load_from_disk(\"./datasets/DialogRE/parity_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'speaker',\n",
       " '1',\n",
       " ':',\n",
       " 'hey',\n",
       " '!',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'hey',\n",
       " '!',\n",
       " 'speaker',\n",
       " '3',\n",
       " ':',\n",
       " 'hey',\n",
       " '—',\n",
       " 'o',\n",
       " '##oh',\n",
       " 'so',\n",
       " ',',\n",
       " 'how',\n",
       " 'was',\n",
       " 'vermont',\n",
       " '?',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'emily',\n",
       " 'is',\n",
       " '…',\n",
       " 'incredible',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'there',\n",
       " '-',\n",
       " 'there',\n",
       " 'are',\n",
       " 'no',\n",
       " 'words',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'it',\n",
       " ',',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'weekend',\n",
       " 'was',\n",
       " 'like',\n",
       " 'a',\n",
       " 'dream',\n",
       " '.',\n",
       " 'oh',\n",
       " '!',\n",
       " 'and',\n",
       " 'you',\n",
       " '!',\n",
       " 'ra',\n",
       " '##ch',\n",
       " '!',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'oh',\n",
       " ',',\n",
       " 'hey',\n",
       " '!',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'hey',\n",
       " '!',\n",
       " 'you',\n",
       " 'were',\n",
       " 'so',\n",
       " 'right',\n",
       " '!',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'what',\n",
       " '?',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'uh',\n",
       " ',',\n",
       " 'what',\n",
       " 'you',\n",
       " 'said',\n",
       " ',',\n",
       " 'about',\n",
       " 'us',\n",
       " 'being',\n",
       " 'in',\n",
       " 'a',\n",
       " 'place',\n",
       " 'where',\n",
       " 'we',\n",
       " 'could',\n",
       " 'finally',\n",
       " 'be',\n",
       " 'happy',\n",
       " 'for',\n",
       " 'each',\n",
       " 'other',\n",
       " '.',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'oh',\n",
       " ',',\n",
       " 'hmm',\n",
       " '.',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'i',\n",
       " ',',\n",
       " 'i',\n",
       " '-',\n",
       " 'i',\n",
       " 'admit',\n",
       " 'i',\n",
       " '-',\n",
       " 'i',\n",
       " 'wasn',\n",
       " '’',\n",
       " 't',\n",
       " 'quite',\n",
       " 'there',\n",
       " '.',\n",
       " 'y',\n",
       " '’',\n",
       " 'know',\n",
       " ',',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'the',\n",
       " 'thought',\n",
       " 'of',\n",
       " 'you',\n",
       " 'and',\n",
       " 'that',\n",
       " '-',\n",
       " 'that',\n",
       " 'josh',\n",
       " 'guy',\n",
       " '…',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'joshua',\n",
       " '.',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'joshua',\n",
       " '…',\n",
       " 'guy',\n",
       " 'at',\n",
       " 'that',\n",
       " 'club',\n",
       " ',',\n",
       " 'dancing',\n",
       " 'and',\n",
       " 'having',\n",
       " 'a',\n",
       " 'good',\n",
       " 'time',\n",
       " ',',\n",
       " 'the',\n",
       " 'thought',\n",
       " 'of',\n",
       " 'it',\n",
       " 'kinda',\n",
       " '…',\n",
       " 'y',\n",
       " '’',\n",
       " 'know',\n",
       " '.',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'i',\n",
       " '…',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'but',\n",
       " 'now',\n",
       " '!',\n",
       " 'i',\n",
       " '’',\n",
       " 'm',\n",
       " 'there',\n",
       " '!',\n",
       " 'i',\n",
       " '’',\n",
       " 'm',\n",
       " 'totally',\n",
       " 'there',\n",
       " '!',\n",
       " 'i',\n",
       " '’',\n",
       " 'm',\n",
       " '-',\n",
       " 'i',\n",
       " '’',\n",
       " 'm',\n",
       " 'finally',\n",
       " 'where',\n",
       " 'you',\n",
       " 'are',\n",
       " '!',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'oh',\n",
       " ',',\n",
       " 'thank',\n",
       " 'goodness',\n",
       " '!',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'and',\n",
       " '-',\n",
       " 'and',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'emily',\n",
       " '.',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'oh',\n",
       " ',',\n",
       " 'no',\n",
       " 'problem',\n",
       " '.',\n",
       " 'i',\n",
       " '’',\n",
       " 'm',\n",
       " 'so',\n",
       " 'glad',\n",
       " 'i',\n",
       " 'could',\n",
       " 'help',\n",
       " '.',\n",
       " 'happy',\n",
       " 'for',\n",
       " 'you',\n",
       " '.',\n",
       " '{entity_1}',\n",
       " ':',\n",
       " 'happy',\n",
       " 'for',\n",
       " 'you',\n",
       " '.',\n",
       " 'speaker',\n",
       " '4',\n",
       " ':',\n",
       " 'no',\n",
       " ',',\n",
       " 'happy',\n",
       " 'for',\n",
       " 'you',\n",
       " '!',\n",
       " '[SEP]',\n",
       " '{entity_1}',\n",
       " '[SEP]',\n",
       " 'emily',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogre['train'][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\projects\\\\affect\\\\TUCORE-GCN',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib',\n",
       " 'C:\\\\Users\\\\picokatx\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv',\n",
       " '',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages',\n",
       " 'D:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\AQLM\\\\src',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers',\n",
       " 'd:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers\")\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'tucore_gcn_transformers.tucore_gcn_llama_tokenizer.SpeakerLlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]\n"
     ]
    }
   ],
   "source": [
    "from tucore_gcn_transformers.tucore_gcn_llama_processor import Conversation, Message, SpeakerRelation\n",
    "from tucore_gcn_transformers.tucore_gcn_llama_tokenizer import SpeakerLlamaTokenizer\n",
    "from tucore_gcn_transformers.tucore_gcn_llama_pipeline import create_inputs, create_model_inputs\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = SpeakerLlamaTokenizer(vocab_file=\"./tokenizer.model\")\n",
    "c = Conversation(\n",
    "\tmessages=[\n",
    "\t\tMessage(\"Speaker 1\", \"Howdy! I'm Flowey, Flowey the Flower!\"), Message(\"Speaker 2\", \"Hello Flowey. I'm your very best friend!\"),\n",
    "\t], speaker_relations=[\n",
    "\t\tSpeakerRelation(\"Speaker 1\", \"Flowey\", rid=[10])\n",
    "\t]\n",
    ")\n",
    "inputs, sequence, entity_1, entity_2 = create_inputs(c, tokenizer, 2048, False)\n",
    "tokens,input_ids,input_mask,segment_ids,speaker_ids,mention_ids,turn_masks,graph = create_model_inputs(\n",
    "\tsequence,\n",
    "\tentity_1,\n",
    "\tentity_2,\n",
    "\ttokenizer,\n",
    "\tinputs,\n",
    "\tFalse,\n",
    "\t36,\n",
    "\t2048,\n",
    "\tmodel_type='llama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension, CUDA_HOME\n",
    "CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fused_softmax\n",
    "dir(fused_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.add_dll_directory(os.path.join(os.environ['CUDA_PATH'], 'bin'))\n",
    "os.add_dll_directory('D:/projects/affect/TUCORE-GCN/.venv/Lib/site-packages/torch/lib')\n",
    "import fused_dense_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enColsConst>; } if( configure_params ) { int ctas_per_sm; { check_cuda_((cudaOccupancyMaxActiveBlocksPerMultiprocessor( &ctas_per_sm, kernel, Kernel_t'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = open('trace.txt').read()\n",
    "a[450:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  12,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  '{entity_1}',\n",
       "  '▁:',\n",
       "  '▁How',\n",
       "  'dy',\n",
       "  '!',\n",
       "  '▁I',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  '▁',\n",
       "  '{entity_2}',\n",
       "  '▁,',\n",
       "  '▁',\n",
       "  '{entity_2}',\n",
       "  '▁',\n",
       "  '▁the',\n",
       "  '▁F',\n",
       "  'lower',\n",
       "  '!',\n",
       "  '</s>',\n",
       "  '{speaker_2}',\n",
       "  '▁:',\n",
       "  '▁Hello',\n",
       "  '▁',\n",
       "  '{entity_2}',\n",
       "  '▁.',\n",
       "  '▁I',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  '▁your',\n",
       "  '▁very',\n",
       "  '▁best',\n",
       "  '▁friend',\n",
       "  '!',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  '{entity_1}',\n",
       "  '</s>',\n",
       "  '{entity_2}',\n",
       "  '</s>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  ...]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True, False, False, ..., False, False, False],\n",
       "        [False,  True, False, ..., False, False, False],\n",
       "        [False, False,  True, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ...,  True, False, False],\n",
       "        [False, False, False, ..., False,  True, False],\n",
       "        [False, False, False, ..., False, False,  True]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ids.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"allenai/soda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused = list(ds['train'].features.keys())\n",
    "unused.remove(\"relation\")\n",
    "unused.remove(\"dialogue\")\n",
    "unused.remove(\"speakers\")\n",
    "lim_ds = ds.remove_columns(unused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relation': Value(dtype='string', id=None),\n",
       " 'dialogue': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'speakers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lim_ds['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'speaker_ids'],\n",
       "        num_rows: 1191582\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'speaker_ids'],\n",
       "        num_rows: 146346\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'speaker_ids'],\n",
       "        num_rows: 148968\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cast_ds = lim_ds.cast(datasets.Features(\n",
    "\t{\n",
    "\t\t\"relation\": datasets.ClassLabel(names=['xIntent', 'xReact', 'xAttr', 'xEffect', 'xWant', 'xNeed'], id=[0,1,2,3,4,5]),\n",
    "\t\t\"dialogue\": datasets.Sequence(feature=datasets.Value(dtype='string', id=None), length=-1, id=None),\n",
    "\t\t\"speakers\": datasets.Sequence(feature=datasets.Value(dtype='string', id=None), length=-1, id=None)\n",
    "\t}\n",
    "))\n",
    "cast_ds['train'].features['relation'].str2int('xWant')\n",
    "ren_ds = cast_ds.rename_columns({'relation': 'labels', 'dialogue': 'input_ids', \"speakers\": \"speaker_ids\"})\n",
    "ren_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<s>[INST]<<SYS>><</SYS>>\\n\\n[/INST]</s>\n",
    "<s>[INST][/INST]{model_reply_1}</s>\n",
    "<s>[INST][/INST]{model_reply_2}</s>\n",
    "<s>[INST][/INST]{model_reply_3}</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a9aac68324cc58ec8a8fe2ce418a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1191582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4886f270aa05497a90ceecfb5bb4b876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/146346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c1464ad55b4f3ab648e48a52ba973c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids'],\n",
       "        num_rows: 1191582\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids'],\n",
       "        num_rows: 146346\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids'],\n",
       "        num_rows: 148968\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf\")\n",
    "def prepare_inputs(inputs):\n",
    "\t# sysstr = \"<s>[INST]<<SYS>>Imagine you are multiple unique personalities. Given these dialogues.<</SYS>>\\n\\n[/INST]</s>\"\n",
    "\tdialog = [inputs['speaker_ids'][i]+\"[/INST]\"+inputs['input_ids'][i] for i in range(len(inputs['input_ids']))]\n",
    "\treturn {\n",
    "\t\t\"input_ids\": \"<s>\"+\"\".join(dialog)+\"</s>\",\n",
    "\t\t\"labels\": inputs['labels']\n",
    "\t}\n",
    "dialog_ds = ren_ds.map(lambda x: prepare_inputs(x))\n",
    "dialog_ds = dialog_ds.remove_columns(\"speaker_ids\")\n",
    "dialog_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64a88369e7a469f956b9f7fc9d62d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/1191582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee4db221a1e47e69dfa7abcd6547ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/146346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cb007ae401470591e5ce8ad1d05bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/148968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialog_ds.save_to_disk(\"./datasets/soda/stringified_inputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"prompt\": \"<s>[INST] <<SYS>>\\n{system prompt}\\n<</SYS>>\\n\\n{1st user prompt} [/INST]\", \"completion\": \" {1st response} </s>\"}\n",
    "{\"prompt\": \"<s>[INST] <<SYS>>\\n{system prompt}\\n<</SYS>>\\n\\n{1st user prompt} [/INST] {1st response} </s><s>[INST] {2nd user prompt} [/INST]\", \"completion\": \" {2nd response} </s>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|user|>\\nHello, how are you?</s>\\n<|assistant|>\\nI'm doing great. How can I help you today?</s>\\n<|user|>\\nI'd like to show off how chat templating works!</s>\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "\n",
    "   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "\n",
    "   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "\n",
    "   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_from_disk(\"./datasets/soda/formatted_inputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4097c52205634e6eacaf26d1ddbbece8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1191582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7517bf9e394d5aaf6fc0f5bbffa2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca46df96597434ea9cdaadca9780658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/146346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1525.0\n",
      "1422.0\n",
      "1177.0\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((ds['train'].num_rows))\n",
    "b = np.zeros((ds['test'].num_rows))\n",
    "c = np.zeros((ds['validation'].num_rows))\n",
    "def temp(x, i, db):\n",
    "\tdb[i] = len(x['input_ids'])\n",
    "\treturn None\n",
    "ds['train'].map(lambda x, i: temp(x, i, a), with_indices=True)\n",
    "ds['test'].map(lambda x, i: temp(x, i, b), with_indices=True)\n",
    "ds['validation'].map(lambda x, i: temp(x, i, c), with_indices=True)\n",
    "print(np.array(a).max())\n",
    "print(np.array(b).max())\n",
    "print(np.array(c).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xticks(np.arange(0, 2**11, 2**7))\n",
    "pd.Series(a).plot.hist(bins=100)\n",
    "pd.Series(b).plot.hist(bins=100)\n",
    "pd.Series(c).plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267885568"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaModel, AutoTokenizer, LlamaForCausalLM, LlamaConfig, LlamaForSequenceClassification\n",
    "# _attn_implementation: flash_attention_2\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    \"BlackSamorez/TinyLlama-1_1B-Chat-v1_0-AQLM-2Bit-1x16-hf\",\n",
    "\tconfig=LlamaConfig.from_json_file(\"./llama_config.json\"),\n",
    "    ignore_mismatched_sizes=True,\n",
    "\ttorch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True\n",
    ")\n",
    "model.cuda()\n",
    "model.train()\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory usage for aqlm llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268399110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([np.array(list(t.shape)).prod() for t in model.state_dict().values()]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.embed_tokens.weight', 65536000),\n",
       " ('model.layers.0.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.0.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.0.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.0.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.0.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.0.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.0.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.0.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.0.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.0.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.0.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.0.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.0.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.0.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.0.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.0.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.0.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.0.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.0.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.0.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.0.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.0.input_layernorm.weight', 2048),\n",
       " ('model.layers.0.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.1.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.1.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.1.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.1.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.1.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.1.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.1.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.1.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.1.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.1.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.1.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.1.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.1.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.1.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.1.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.1.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.1.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.1.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.1.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.1.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.1.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.1.input_layernorm.weight', 2048),\n",
       " ('model.layers.1.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.2.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.2.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.2.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.2.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.2.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.2.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.2.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.2.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.2.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.2.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.2.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.2.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.2.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.2.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.2.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.2.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.2.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.2.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.2.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.2.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.2.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.2.input_layernorm.weight', 2048),\n",
       " ('model.layers.2.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.3.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.3.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.3.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.3.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.3.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.3.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.3.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.3.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.3.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.3.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.3.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.3.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.3.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.3.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.3.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.3.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.3.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.3.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.3.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.3.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.3.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.3.input_layernorm.weight', 2048),\n",
       " ('model.layers.3.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.4.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.4.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.4.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.4.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.4.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.4.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.4.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.4.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.4.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.4.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.4.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.4.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.4.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.4.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.4.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.4.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.4.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.4.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.4.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.4.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.4.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.4.input_layernorm.weight', 2048),\n",
       " ('model.layers.4.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.5.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.5.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.5.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.5.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.5.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.5.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.5.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.5.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.5.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.5.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.5.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.5.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.5.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.5.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.5.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.5.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.5.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.5.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.5.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.5.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.5.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.5.input_layernorm.weight', 2048),\n",
       " ('model.layers.5.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.6.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.6.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.6.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.6.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.6.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.6.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.6.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.6.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.6.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.6.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.6.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.6.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.6.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.6.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.6.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.6.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.6.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.6.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.6.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.6.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.6.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.6.input_layernorm.weight', 2048),\n",
       " ('model.layers.6.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.7.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.7.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.7.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.7.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.7.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.7.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.7.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.7.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.7.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.7.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.7.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.7.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.7.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.7.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.7.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.7.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.7.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.7.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.7.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.7.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.7.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.7.input_layernorm.weight', 2048),\n",
       " ('model.layers.7.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.8.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.8.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.8.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.8.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.8.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.8.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.8.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.8.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.8.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.8.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.8.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.8.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.8.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.8.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.8.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.8.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.8.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.8.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.8.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.8.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.8.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.8.input_layernorm.weight', 2048),\n",
       " ('model.layers.8.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.9.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.9.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.9.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.9.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.9.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.9.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.9.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.9.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.9.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.9.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.9.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.9.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.9.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.9.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.9.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.9.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.9.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.9.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.9.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.9.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.9.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.9.input_layernorm.weight', 2048),\n",
       " ('model.layers.9.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.10.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.10.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.10.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.10.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.10.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.10.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.10.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.10.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.10.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.10.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.10.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.10.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.10.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.10.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.10.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.10.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.10.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.10.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.10.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.10.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.10.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.10.input_layernorm.weight', 2048),\n",
       " ('model.layers.10.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.11.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.11.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.11.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.11.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.11.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.11.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.11.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.11.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.11.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.11.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.11.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.11.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.11.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.11.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.11.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.11.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.11.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.11.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.11.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.11.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.11.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.11.input_layernorm.weight', 2048),\n",
       " ('model.layers.11.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.12.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.12.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.12.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.12.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.12.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.12.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.12.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.12.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.12.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.12.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.12.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.12.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.12.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.12.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.12.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.12.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.12.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.12.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.12.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.12.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.12.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.12.input_layernorm.weight', 2048),\n",
       " ('model.layers.12.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.13.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.13.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.13.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.13.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.13.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.13.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.13.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.13.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.13.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.13.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.13.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.13.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.13.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.13.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.13.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.13.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.13.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.13.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.13.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.13.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.13.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.13.input_layernorm.weight', 2048),\n",
       " ('model.layers.13.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.14.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.14.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.14.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.14.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.14.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.14.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.14.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.14.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.14.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.14.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.14.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.14.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.14.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.14.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.14.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.14.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.14.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.14.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.14.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.14.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.14.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.14.input_layernorm.weight', 2048),\n",
       " ('model.layers.14.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.15.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.15.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.15.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.15.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.15.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.15.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.15.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.15.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.15.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.15.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.15.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.15.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.15.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.15.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.15.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.15.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.15.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.15.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.15.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.15.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.15.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.15.input_layernorm.weight', 2048),\n",
       " ('model.layers.15.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.16.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.16.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.16.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.16.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.16.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.16.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.16.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.16.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.16.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.16.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.16.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.16.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.16.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.16.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.16.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.16.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.16.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.16.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.16.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.16.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.16.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.16.input_layernorm.weight', 2048),\n",
       " ('model.layers.16.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.17.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.17.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.17.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.17.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.17.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.17.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.17.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.17.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.17.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.17.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.17.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.17.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.17.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.17.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.17.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.17.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.17.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.17.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.17.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.17.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.17.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.17.input_layernorm.weight', 2048),\n",
       " ('model.layers.17.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.18.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.18.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.18.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.18.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.18.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.18.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.18.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.18.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.18.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.18.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.18.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.18.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.18.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.18.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.18.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.18.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.18.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.18.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.18.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.18.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.18.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.18.input_layernorm.weight', 2048),\n",
       " ('model.layers.18.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.19.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.19.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.19.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.19.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.19.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.19.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.19.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.19.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.19.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.19.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.19.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.19.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.19.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.19.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.19.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.19.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.19.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.19.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.19.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.19.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.19.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.19.input_layernorm.weight', 2048),\n",
       " ('model.layers.19.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.20.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.20.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.20.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.20.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.20.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.20.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.20.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.20.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.20.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.20.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.20.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.20.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.20.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.20.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.20.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.20.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.20.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.20.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.20.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.20.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.20.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.20.input_layernorm.weight', 2048),\n",
       " ('model.layers.20.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.21.self_attn.q_proj.codebooks', 524288),\n",
       " ('model.layers.21.self_attn.q_proj.codes', 524288),\n",
       " ('model.layers.21.self_attn.q_proj.scales', 2048),\n",
       " ('model.layers.21.self_attn.k_proj.codebooks', 524288),\n",
       " ('model.layers.21.self_attn.k_proj.codes', 65536),\n",
       " ('model.layers.21.self_attn.k_proj.scales', 256),\n",
       " ('model.layers.21.self_attn.v_proj.codebooks', 524288),\n",
       " ('model.layers.21.self_attn.v_proj.codes', 65536),\n",
       " ('model.layers.21.self_attn.v_proj.scales', 256),\n",
       " ('model.layers.21.self_attn.o_proj.codebooks', 524288),\n",
       " ('model.layers.21.self_attn.o_proj.codes', 524288),\n",
       " ('model.layers.21.self_attn.o_proj.scales', 2048),\n",
       " ('model.layers.21.mlp.gate_proj.codebooks', 524288),\n",
       " ('model.layers.21.mlp.gate_proj.codes', 1441792),\n",
       " ('model.layers.21.mlp.gate_proj.scales', 5632),\n",
       " ('model.layers.21.mlp.up_proj.codebooks', 524288),\n",
       " ('model.layers.21.mlp.up_proj.codes', 1441792),\n",
       " ('model.layers.21.mlp.up_proj.scales', 5632),\n",
       " ('model.layers.21.mlp.down_proj.codebooks', 524288),\n",
       " ('model.layers.21.mlp.down_proj.codes', 1441792),\n",
       " ('model.layers.21.mlp.down_proj.scales', 2048),\n",
       " ('model.layers.21.input_layernorm.weight', 2048),\n",
       " ('model.layers.21.post_attention_layernorm.weight', 2048),\n",
       " ('model.norm.weight', 2048),\n",
       " ('score.codebooks', 524288),\n",
       " ('score.codes', 1536),\n",
       " ('score.scales', 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "[(name, np.array(list(t.shape)).prod()) for name, t in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560665088"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# executed with quantized model loaded\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.711384415544448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "560665088/268399110*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.335634350149167"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "560665088/1034524672*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare normal LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1034524672"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "# _attn_implementation: flash_attention_2\n",
    "llama_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    "\ttorch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True\n",
    ")\n",
    "llama_model.cuda()\n",
    "llama_model.train()\n",
    "llama_model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.embed_tokens.weight', 65536000),\n",
       " ('model.layers.0.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.0.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.0.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.0.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.0.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.0.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.0.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.0.input_layernorm.weight', 2048),\n",
       " ('model.layers.0.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.1.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.1.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.1.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.1.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.1.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.1.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.1.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.1.input_layernorm.weight', 2048),\n",
       " ('model.layers.1.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.2.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.2.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.2.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.2.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.2.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.2.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.2.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.2.input_layernorm.weight', 2048),\n",
       " ('model.layers.2.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.3.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.3.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.3.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.3.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.3.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.3.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.3.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.3.input_layernorm.weight', 2048),\n",
       " ('model.layers.3.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.4.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.4.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.4.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.4.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.4.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.4.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.4.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.4.input_layernorm.weight', 2048),\n",
       " ('model.layers.4.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.5.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.5.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.5.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.5.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.5.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.5.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.5.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.5.input_layernorm.weight', 2048),\n",
       " ('model.layers.5.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.6.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.6.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.6.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.6.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.6.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.6.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.6.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.6.input_layernorm.weight', 2048),\n",
       " ('model.layers.6.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.7.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.7.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.7.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.7.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.7.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.7.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.7.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.7.input_layernorm.weight', 2048),\n",
       " ('model.layers.7.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.8.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.8.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.8.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.8.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.8.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.8.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.8.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.8.input_layernorm.weight', 2048),\n",
       " ('model.layers.8.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.9.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.9.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.9.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.9.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.9.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.9.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.9.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.9.input_layernorm.weight', 2048),\n",
       " ('model.layers.9.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.10.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.10.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.10.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.10.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.10.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.10.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.10.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.10.input_layernorm.weight', 2048),\n",
       " ('model.layers.10.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.11.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.11.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.11.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.11.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.11.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.11.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.11.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.11.input_layernorm.weight', 2048),\n",
       " ('model.layers.11.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.12.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.12.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.12.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.12.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.12.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.12.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.12.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.12.input_layernorm.weight', 2048),\n",
       " ('model.layers.12.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.13.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.13.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.13.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.13.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.13.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.13.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.13.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.13.input_layernorm.weight', 2048),\n",
       " ('model.layers.13.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.14.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.14.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.14.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.14.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.14.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.14.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.14.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.14.input_layernorm.weight', 2048),\n",
       " ('model.layers.14.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.15.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.15.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.15.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.15.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.15.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.15.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.15.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.15.input_layernorm.weight', 2048),\n",
       " ('model.layers.15.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.16.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.16.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.16.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.16.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.16.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.16.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.16.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.16.input_layernorm.weight', 2048),\n",
       " ('model.layers.16.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.17.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.17.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.17.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.17.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.17.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.17.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.17.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.17.input_layernorm.weight', 2048),\n",
       " ('model.layers.17.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.18.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.18.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.18.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.18.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.18.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.18.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.18.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.18.input_layernorm.weight', 2048),\n",
       " ('model.layers.18.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.19.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.19.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.19.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.19.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.19.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.19.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.19.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.19.input_layernorm.weight', 2048),\n",
       " ('model.layers.19.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.20.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.20.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.20.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.20.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.20.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.20.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.20.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.20.input_layernorm.weight', 2048),\n",
       " ('model.layers.20.post_attention_layernorm.weight', 2048),\n",
       " ('model.layers.21.self_attn.q_proj.weight', 4194304),\n",
       " ('model.layers.21.self_attn.k_proj.weight', 524288),\n",
       " ('model.layers.21.self_attn.v_proj.weight', 524288),\n",
       " ('model.layers.21.self_attn.o_proj.weight', 4194304),\n",
       " ('model.layers.21.mlp.gate_proj.weight', 11534336),\n",
       " ('model.layers.21.mlp.up_proj.weight', 11534336),\n",
       " ('model.layers.21.mlp.down_proj.weight', 11534336),\n",
       " ('model.layers.21.input_layernorm.weight', 2048),\n",
       " ('model.layers.21.post_attention_layernorm.weight', 2048),\n",
       " ('model.norm.weight', 2048),\n",
       " ('score.weight', 12288)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "[(name, np.array(list(t.shape)).prod()) for name, t in llama_model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085837824"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# executed with bert model loaded\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.129825651949265"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2085837824/1034524672*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50,475,008 || all params: 318,348,288 || trainable%: 15.855278606053004\n"
     ]
    }
   ],
   "source": [
    "from peft import AdaLoraConfig, get_peft_model, IA3Model, IA3Config, LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "\tinference_mode = False,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.get_parameter(\"base_model.model.score.weight\").requires_grad = True\n",
    "peft_model.get_parameter(\"base_model.model.score.weight\").data = peft_model.get_parameter(\"base_model.model.score.weight\").data.to(torch.float32)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048, padding_idx=2)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaFlashAttention2(\n",
       "              (q_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.AqlmLoraLinear(\n",
       "                (base_layer): QuantizedLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): Linear(in_features=2048, out_features=6, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('base_model.model.model.embed_tokens.weight', False),\n",
       " ('base_model.model.model.layers.0.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.0.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.0.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.0.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.0.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.0.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.0.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.0.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.0.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.0.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.0.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.0.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.0.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.0.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.1.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.1.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.1.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.1.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.1.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.1.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.1.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.1.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.1.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.1.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.1.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.1.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.1.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.1.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.2.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.2.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.2.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.2.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.2.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.2.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.2.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.2.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.2.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.2.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.2.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.2.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.2.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.2.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.3.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.3.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.3.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.3.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.3.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.3.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.3.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.3.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.3.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.3.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.3.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.3.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.3.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.3.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.4.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.4.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.4.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.4.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.4.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.4.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.4.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.4.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.4.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.4.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.4.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.4.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.4.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.4.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.5.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.5.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.5.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.5.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.5.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.5.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.5.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.5.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.5.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.5.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.5.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.5.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.5.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.5.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.6.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.6.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.6.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.6.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.6.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.6.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.6.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.6.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.6.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.6.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.6.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.6.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.6.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.6.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.7.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.7.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.7.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.7.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.7.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.7.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.7.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.7.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.7.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.7.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.7.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.7.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.7.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.7.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.8.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.8.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.8.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.8.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.8.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.8.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.8.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.8.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.8.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.8.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.8.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.8.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.8.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.8.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.9.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.9.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.self_attn.q_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.9.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.self_attn.k_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.9.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.self_attn.v_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.9.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.self_attn.o_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.9.mlp.gate_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.9.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.9.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.9.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.9.mlp.down_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.9.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.9.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.9.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.9.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.10.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.10.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.10.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.10.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.10.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.10.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.10.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.11.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.11.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.11.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.11.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.11.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.11.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.11.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.12.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.12.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.12.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.12.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.12.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.12.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.12.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.13.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.13.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.13.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.13.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.13.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.13.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.13.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.14.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.14.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.14.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.14.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.14.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.14.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.14.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.15.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.15.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.15.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.15.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.15.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.15.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.15.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.16.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.16.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.16.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.16.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.16.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.16.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.16.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.17.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.17.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.17.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.17.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.17.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.17.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.17.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.18.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.18.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.18.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.18.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.18.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.18.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.18.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.19.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.19.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.19.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.19.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.19.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.19.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.19.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.20.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.20.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.20.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.20.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.20.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.20.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.20.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.21.self_attn.q_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.q_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.self_attn.q_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.k_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.k_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.self_attn.k_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.v_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.v_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.self_attn.v_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.o_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.o_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.self_attn.o_proj.base_layer.scales',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.mlp.gate_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.mlp.gate_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.mlp.gate_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.mlp.up_proj.base_layer.codebooks', False),\n",
       " ('base_model.model.model.layers.21.mlp.up_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.mlp.up_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', True),\n",
       " ('base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', True),\n",
       " ('base_model.model.model.layers.21.mlp.down_proj.base_layer.codebooks',\n",
       "  False),\n",
       " ('base_model.model.model.layers.21.mlp.down_proj.base_layer.codes', False),\n",
       " ('base_model.model.model.layers.21.mlp.down_proj.base_layer.scales', False),\n",
       " ('base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight',\n",
       "  True),\n",
       " ('base_model.model.model.layers.21.input_layernorm.weight', False),\n",
       " ('base_model.model.model.layers.21.post_attention_layernorm.weight', False),\n",
       " ('base_model.model.model.norm.weight', False),\n",
       " ('base_model.model.score.weight', True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, p.requires_grad) for name, p in peft_model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora.aqlm import AqlmLoraLinear\n",
    "from aqlm import QuantizedLinear\n",
    "galore_params = []\n",
    "target_modules_list = [\"self_attn\", \"mlp\"]\n",
    "for module_name, module in peft_model.named_modules():\n",
    "    if not isinstance(module, AqlmLoraLinear):\n",
    "        continue\n",
    "\n",
    "    if not any(target_key in module_name for target_key in target_modules_list):\n",
    "        continue\n",
    "    galore_params.append(module.lora_A.default.weight)\n",
    "    galore_params.append(module.lora_B.default.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(galore_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_galore_params = [id(p) for p in galore_params]\n",
    "# make parameters without \"rank\" to another group\n",
    "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
    "# then call galore_adamw\n",
    "param_groups = [{'params': regular_params}, \n",
    "\t\t\t\t{'params': galore_params, 'rank': 8, 'update_proj_gap': 200, 'scale': 0.25, 'proj_type': 'std'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from galore_torch import GaLoreAdamW8bit\n",
    "optimizer = GaLoreAdamW8bit(param_groups, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "def collate_fn(inputs):\n",
    "  input_attn = [tokenizer(iids['input_ids'], padding=\"max_length\", truncation=\"only_first\", max_length=512) for iids in inputs]\n",
    "  return {\n",
    "    'input_ids': torch.tensor([ia['input_ids'] for ia in input_attn], dtype=torch.int32),\n",
    "    'attention_mask': torch.tensor([ia['attention_mask'] for ia in input_attn], dtype=torch.int32),\n",
    "    'labels': torch.tensor([iids['labels'] for iids in inputs], dtype=torch.long)\n",
    "  }\n",
    "def compute_metrics(eval_preds):\n",
    "  metrics = dict()\n",
    "  accuracy_metric = load('accuracy')\n",
    "  precision_metric = load('precision')\n",
    "  recall_metric = load('recall')\n",
    "  f1_metric = load('f1')\n",
    "\n",
    "\n",
    "  preds = eval_preds.predictions.argmax(axis=1)\n",
    "  labels = eval_preds.label_ids\n",
    "  metrics.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
    "  metrics.update(precision_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "  metrics.update(recall_metric.compute(predictions=preds, references=labels, average='weighted', zero_division=0))\n",
    "  metrics.update(f1_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"tinyllama_soda_corrected\",\n",
    "    remove_unused_columns=False,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "\teval_steps=0.0999,\n",
    "    save_strategy=\"steps\",\n",
    "\tsave_steps=0.0999,\n",
    "    learning_rate=2e-5,\n",
    "\tlr_scheduler_type = \"constant\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=16,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    label_names=[\"labels\"],\n",
    "\treport_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_constant_schedule\n",
    "trainer = Trainer(\n",
    "    peft_model,\n",
    "    args,\n",
    "    train_dataset=ds['train'].shard(10, 0),\n",
    "    eval_dataset=ds['test'].shard(10, 0),\n",
    "    tokenizer=tokenizer,\n",
    "\toptimizers=(optimizer, get_constant_schedule(optimizer)),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicokatx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\projects\\affect\\TUCORE-GCN\\wandb\\run-20240315_222714-rhvlbed0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/picokatx/affect/runs/rhvlbed0' target=\"_blank\">robust-leaf-12</a></strong> to <a href='https://wandb.ai/picokatx/affect' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/picokatx/affect' target=\"_blank\">https://wandb.ai/picokatx/affect</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/picokatx/affect/runs/rhvlbed0' target=\"_blank\">https://wandb.ai/picokatx/affect/runs/rhvlbed0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aad9f0bec7401b88745c3a8a392e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1964\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try to reduce mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "trainer.create_optimizer_and_scheduler(3000)\n",
    "optimizer = trainer.optimizer\n",
    "scheduler = trainer.lr_scheduler\n",
    "training_dataloader = trainer.get_train_dataloader()\n",
    "model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, training_dataloader, scheduler\n",
    ")\n",
    "batch = next(iter(training_dataloader))\n",
    "optimizer.zero_grad()\n",
    "inputs = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5632"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaFlashAttention2(\n",
       "    (q_proj): lora.AqlmLoraLinear(\n",
       "      (base_layer): QuantizedLinear()\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (k_proj): lora.AqlmLoraLinear(\n",
       "      (base_layer): QuantizedLinear()\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (v_proj): lora.AqlmLoraLinear(\n",
       "      (base_layer): QuantizedLinear()\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (o_proj): lora.AqlmLoraLinear(\n",
       "      (base_layer): QuantizedLinear()\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): QuantizedLinear()\n",
       "    (up_proj): QuantizedLinear()\n",
       "    (down_proj): QuantizedLinear()\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm()\n",
       "  (post_attention_layernorm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'codebook_size',\n",
       " 'codebooks',\n",
       " 'codes',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'gemm_op',\n",
       " 'gemv_op',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'in_features',\n",
       " 'in_group_size',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'nbits_per_codebook',\n",
       " 'num_codebooks',\n",
       " 'out_features',\n",
       " 'out_group_size',\n",
       " 'parameters',\n",
       " 'prepare_matmul_op',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'scales',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'source_cls',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'use_gemv_rule',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.265289728"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5632*128*8*23*2/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.543503872"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*2048*8*23/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 2048])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     16\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:817\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:805\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:817\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:805\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1362\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1362\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1374\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1020\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1010\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1011\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         cache_position,\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1020\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:741\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    740\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:494\u001b[0m, in \u001b[0;36mLlamaFlashAttention2.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mto(target_dtype)\n\u001b[0;32m    492\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mto(target_dtype)\n\u001b[1;32m--> 494\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flash_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    499\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:537\u001b[0m, in \u001b[0;36mLlamaFlashAttention2._flash_attention_forward\u001b[1;34m(self, query_states, key_states, value_states, attention_mask, query_length, dropout, softmax_scale)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    536\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 537\u001b[0m     query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upad_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_length\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m     cu_seqlens_q, cu_seqlens_k \u001b[38;5;241m=\u001b[39m cu_seq_lens\n\u001b[0;32m    542\u001b[0m     max_seqlen_in_batch_q, max_seqlen_in_batch_k \u001b[38;5;241m=\u001b[39m max_seq_lens\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:566\u001b[0m, in \u001b[0;36mLlamaFlashAttention2._upad_input\u001b[1;34m(self, query_layer, key_layer, value_layer, attention_mask, query_length)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upad_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_layer, key_layer, value_layer, attention_mask, query_length):\n\u001b[1;32m--> 566\u001b[0m     indices_k, cu_seqlens_k, max_seqlen_in_batch_k \u001b[38;5;241m=\u001b[39m \u001b[43m_get_unpad_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m     batch_size, kv_seq_len, num_key_value_heads, head_dim \u001b[38;5;241m=\u001b[39m key_layer\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    569\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m index_first_axis(\n\u001b[0;32m    570\u001b[0m         key_layer\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m kv_seq_len, num_key_value_heads, head_dim), indices_k\n\u001b[0;32m    571\u001b[0m     )\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:65\u001b[0m, in \u001b[0;36m_get_unpad_data\u001b[1;34m(attention_mask)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_unpad_data\u001b[39m(attention_mask):\n\u001b[0;32m     64\u001b[0m     seqlens_in_batch \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m---> 65\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     66\u001b[0m     max_seqlen_in_batch \u001b[38;5;241m=\u001b[39m seqlens_in_batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     67\u001b[0m     cu_seqlens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(torch\u001b[38;5;241m.\u001b[39mcumsum(seqlens_in_batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "trainer.create_optimizer_and_scheduler(3000)\n",
    "optimizer = trainer.optimizer\n",
    "scheduler = trainer.lr_scheduler\n",
    "training_dataloader = trainer.get_train_dataloader()\n",
    "model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, training_dataloader, scheduler\n",
    ")\n",
    "\n",
    "for batch in training_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs = batch\n",
    "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs[\"labels\"], use_cache=False)\n",
    "    loss = outputs.loss\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66864638de2741c7a5afa797bc413ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8728, 'grad_norm': 30.34687042236328, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 2.2417, 'grad_norm': 39.659637451171875, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8783, 'grad_norm': 30.25309944152832, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7977, 'grad_norm': 32.13945007324219, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9002, 'grad_norm': 24.224674224853516, 'learning_rate': 1e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1964\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "class ProfCallback(TrainerCallback):\n",
    "    def __init__(self, prof):\n",
    "        self.prof = prof\n",
    "    def on_substep_end(self, args, state, control, **kwargs):\n",
    "        self.prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\profiler\\profiler.py:339: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c3c88165bb4636be2eb8303070840a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1383, 'grad_norm': nan, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 2.3837, 'grad_norm': 56.449676513671875, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9804, 'grad_norm': 45.62903594970703, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9266, 'grad_norm': 37.55613327026367, 'learning_rate': 1e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m callback \u001b[38;5;241m=\u001b[39m ProfCallback(prof)\n\u001b[0;32m     12\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_callback(callback)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1964\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "\tactivities=[\n",
    "\t\ttorch.profiler.ProfilerActivity.CPU,\n",
    "\t\ttorch.profiler.ProfilerActivity.CUDA,\n",
    "\t],\n",
    "\tschedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),\n",
    "\trecord_shapes=True,\n",
    "\tprofile_memory=True,\n",
    "\twith_stack=True,\n",
    ") as prof:\n",
    "\tcallback = ProfCallback(prof)\n",
    "\ttrainer.add_callback(callback)\n",
    "\ttrainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"btephe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prof.export_memory_timeline(f\"hrwhrwhw.html\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.pop_callback(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ProfCallback()\n",
    "trainer.add_callback(callback)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = peft_model(\n",
    "\tinput_ids = torch.LongTensor([tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I want pumpkin pie!!!!\")))]).cuda(),\n",
    "\tlabels = torch.LongTensor([1]).cuda(),\n",
    "\toutput_hidden_states=True,\n",
    "\toutput_attentions=True\n",
    ")\n",
    "loss, logits, past_key_vals, attn = out.loss, out.logits, out.past_key_values, out.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
