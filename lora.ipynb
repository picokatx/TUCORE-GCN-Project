{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"food101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baklava'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(image_processor.size[\"height\"]),\n",
    "        CenterCrop(image_processor.size[\"height\"]),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 520,325 || all params: 86,396,674 || trainable%: 0.6022511931419953\n"
     ]
    }
   ],
   "source": [
    "from peft import AdaLoraConfig, get_peft_model, IA3Model, IA3Config, LoraConfig\n",
    "\n",
    "ada_lora_config = AdaLoraConfig(\n",
    "    r=8,\n",
    "    init_r=12,\n",
    "    tinit=200,\n",
    "    tfinal=1000,\n",
    "    deltaT=10,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "ia3_config = IA3Config(\n",
    "    peft_type=\"IA3\",\n",
    "    target_modules=[\"key\", \"value\", \"dense\"],\n",
    "    feedforward_modules=[\"dense\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, ada_lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "#\"trainable params: 520,325 || all params: 87,614,722 || trainable%: 0.5938785036606062\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics():\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"peft_test\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=32,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dd92937d724735a64f4a76fe45999f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.71, 'learning_rate': 0.004931972789115647, 'epoch': 0.07}\n",
      "{'loss': 1.9883, 'learning_rate': 0.0048639455782312924, 'epoch': 0.14}\n",
      "{'loss': 1.2399, 'learning_rate': 0.004795918367346939, 'epoch': 0.2}\n",
      "{'loss': 1.0627, 'learning_rate': 0.004727891156462586, 'epoch': 0.27}\n",
      "{'loss': 0.9556, 'learning_rate': 0.004659863945578231, 'epoch': 0.34}\n",
      "{'loss': 0.917, 'learning_rate': 0.004591836734693878, 'epoch': 0.41}\n",
      "{'loss': 0.8947, 'learning_rate': 0.004523809523809524, 'epoch': 0.47}\n",
      "{'loss': 0.8512, 'learning_rate': 0.00445578231292517, 'epoch': 0.54}\n",
      "{'loss': 0.892, 'learning_rate': 0.004387755102040816, 'epoch': 0.61}\n",
      "{'loss': 0.8804, 'learning_rate': 0.004319727891156463, 'epoch': 0.68}\n",
      "{'loss': 0.8493, 'learning_rate': 0.004251700680272108, 'epoch': 0.74}\n",
      "{'loss': 0.8486, 'learning_rate': 0.004183673469387756, 'epoch': 0.81}\n",
      "{'loss': 0.8406, 'learning_rate': 0.0041156462585034016, 'epoch': 0.88}\n",
      "{'loss': 0.802, 'learning_rate': 0.004047619047619048, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2aa48dbfb9d42d2a0e79a80a2c76815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4648701250553131, 'eval_runtime': 328.176, 'eval_samples_per_second': 76.94, 'eval_steps_per_second': 4.811, 'epoch': 0.99}\n",
      "{'loss': 0.7775, 'learning_rate': 0.003979591836734694, 'epoch': 1.01}\n",
      "{'loss': 0.7401, 'learning_rate': 0.0039115646258503405, 'epoch': 1.08}\n",
      "{'loss': 0.7486, 'learning_rate': 0.0038435374149659862, 'epoch': 1.15}\n",
      "{'loss': 0.7394, 'learning_rate': 0.0037755102040816324, 'epoch': 1.22}\n",
      "{'loss': 0.7224, 'learning_rate': 0.0037074829931972794, 'epoch': 1.28}\n",
      "{'loss': 0.7298, 'learning_rate': 0.0036394557823129256, 'epoch': 1.35}\n",
      "{'loss': 0.7716, 'learning_rate': 0.0035714285714285718, 'epoch': 1.42}\n",
      "{'loss': 0.7458, 'learning_rate': 0.003503401360544218, 'epoch': 1.49}\n",
      "{'loss': 0.7402, 'learning_rate': 0.003435374149659864, 'epoch': 1.55}\n",
      "{'loss': 0.7094, 'learning_rate': 0.0033673469387755102, 'epoch': 1.62}\n",
      "{'loss': 0.7616, 'learning_rate': 0.0032993197278911564, 'epoch': 1.69}\n",
      "{'loss': 0.7013, 'learning_rate': 0.003231292517006803, 'epoch': 1.76}\n",
      "{'loss': 0.7525, 'learning_rate': 0.003163265306122449, 'epoch': 1.82}\n",
      "{'loss': 0.6883, 'learning_rate': 0.0030952380952380953, 'epoch': 1.89}\n",
      "{'loss': 0.7116, 'learning_rate': 0.0030272108843537415, 'epoch': 1.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d7cc747160498283ab5ea6caaef382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41003984212875366, 'eval_runtime': 341.3613, 'eval_samples_per_second': 73.969, 'eval_steps_per_second': 4.626, 'epoch': 1.99}\n",
      "{'loss': 0.6716, 'learning_rate': 0.0029591836734693877, 'epoch': 2.03}\n",
      "{'loss': 0.6728, 'learning_rate': 0.002891156462585034, 'epoch': 2.1}\n",
      "{'loss': 0.6483, 'learning_rate': 0.00282312925170068, 'epoch': 2.16}\n",
      "{'loss': 0.6904, 'learning_rate': 0.002755102040816326, 'epoch': 2.23}\n",
      "{'loss': 0.6771, 'learning_rate': 0.002687074829931973, 'epoch': 2.3}\n",
      "{'loss': 0.6848, 'learning_rate': 0.0026190476190476194, 'epoch': 2.37}\n",
      "{'loss': 0.6883, 'learning_rate': 0.0025510204081632655, 'epoch': 2.43}\n",
      "{'loss': 0.6542, 'learning_rate': 0.0024829931972789117, 'epoch': 2.5}\n",
      "{'loss': 0.6782, 'learning_rate': 0.002414965986394558, 'epoch': 2.57}\n",
      "{'loss': 0.6697, 'learning_rate': 0.002346938775510204, 'epoch': 2.64}\n",
      "{'loss': 0.6685, 'learning_rate': 0.0022789115646258506, 'epoch': 2.7}\n",
      "{'loss': 0.6469, 'learning_rate': 0.002210884353741497, 'epoch': 2.77}\n",
      "{'loss': 0.6683, 'learning_rate': 0.002142857142857143, 'epoch': 2.84}\n",
      "{'loss': 0.6433, 'learning_rate': 0.002074829931972789, 'epoch': 2.91}\n",
      "{'loss': 0.6758, 'learning_rate': 0.0020068027210884353, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4113106fa09a4faaa30579cd99853f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37821701169013977, 'eval_runtime': 337.8259, 'eval_samples_per_second': 74.743, 'eval_steps_per_second': 4.674, 'epoch': 2.99}\n",
      "{'loss': 0.6124, 'learning_rate': 0.0019387755102040817, 'epoch': 3.04}\n",
      "{'loss': 0.6352, 'learning_rate': 0.001870748299319728, 'epoch': 3.11}\n",
      "{'loss': 0.584, 'learning_rate': 0.0018027210884353742, 'epoch': 3.18}\n",
      "{'loss': 0.615, 'learning_rate': 0.0017346938775510204, 'epoch': 3.24}\n",
      "{'loss': 0.6225, 'learning_rate': 0.0016734693877551022, 'epoch': 3.31}\n",
      "{'loss': 0.6091, 'learning_rate': 0.0016054421768707484, 'epoch': 3.38}\n",
      "{'loss': 0.645, 'learning_rate': 0.0015374149659863946, 'epoch': 3.45}\n",
      "{'loss': 0.6192, 'learning_rate': 0.001469387755102041, 'epoch': 3.51}\n",
      "{'loss': 0.6277, 'learning_rate': 0.0014013605442176871, 'epoch': 3.58}\n",
      "{'loss': 0.5923, 'learning_rate': 0.0013333333333333333, 'epoch': 3.65}\n",
      "{'loss': 0.6192, 'learning_rate': 0.0012653061224489795, 'epoch': 3.72}\n",
      "{'loss': 0.619, 'learning_rate': 0.0011972789115646258, 'epoch': 3.78}\n",
      "{'loss': 0.5933, 'learning_rate': 0.0011292517006802722, 'epoch': 3.85}\n",
      "{'loss': 0.5796, 'learning_rate': 0.0010612244897959184, 'epoch': 3.92}\n",
      "{'loss': 0.6058, 'learning_rate': 0.0009931972789115648, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baeecdf69a1848ecbf59cff1e1f50ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36065277457237244, 'eval_runtime': 335.3836, 'eval_samples_per_second': 75.287, 'eval_steps_per_second': 4.708, 'epoch': 3.99}\n",
      "{'loss': 0.5912, 'learning_rate': 0.0009251700680272108, 'epoch': 4.05}\n",
      "{'loss': 0.5885, 'learning_rate': 0.0008571428571428572, 'epoch': 4.12}\n",
      "{'loss': 0.5673, 'learning_rate': 0.0007891156462585034, 'epoch': 4.19}\n",
      "{'loss': 0.5821, 'learning_rate': 0.0007210884353741496, 'epoch': 4.26}\n",
      "{'loss': 0.6128, 'learning_rate': 0.000653061224489796, 'epoch': 4.33}\n",
      "{'loss': 0.5744, 'learning_rate': 0.0005850340136054422, 'epoch': 4.39}\n",
      "{'loss': 0.5872, 'learning_rate': 0.0005170068027210885, 'epoch': 4.46}\n",
      "{'loss': 0.5888, 'learning_rate': 0.0004489795918367347, 'epoch': 4.53}\n",
      "{'loss': 0.582, 'learning_rate': 0.000380952380952381, 'epoch': 4.6}\n",
      "{'loss': 0.5681, 'learning_rate': 0.00031292517006802724, 'epoch': 4.66}\n",
      "{'loss': 0.5654, 'learning_rate': 0.00024489795918367346, 'epoch': 4.73}\n",
      "{'loss': 0.5919, 'learning_rate': 0.00017687074829931976, 'epoch': 4.8}\n",
      "{'loss': 0.5915, 'learning_rate': 0.00010884353741496598, 'epoch': 4.87}\n",
      "{'loss': 0.5993, 'learning_rate': 4.081632653061225e-05, 'epoch': 4.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9592867c1e6d480f895eb72f53268fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3536034822463989, 'eval_runtime': 309.0997, 'eval_samples_per_second': 81.689, 'eval_steps_per_second': 5.108, 'epoch': 4.97}\n",
      "{'train_runtime': 9997.6021, 'train_samples_per_second': 37.884, 'train_steps_per_second': 0.074, 'train_loss': 0.7547856937460348, 'epoch': 4.97}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=735, training_loss=0.7547856937460348, metrics={'train_runtime': 9997.6021, 'train_samples_per_second': 37.884, 'train_steps_per_second': 0.074, 'train_loss': 0.7547856937460348, 'epoch': 4.97})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    peft_model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaLORA: 10.6s/iter\n",
    "- LORA: 13.22s/iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
