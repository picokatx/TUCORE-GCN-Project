{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "import aqlm\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import datasets\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers\")\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'label_ids', 'input_ids', 'input_mask', 'segment_ids', 'speaker_ids', 'mention_ids', 'turn_masks', 'graph', 'graph_data'],\n",
       "        num_rows: 5997\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'label_ids', 'input_ids', 'input_mask', 'segment_ids', 'speaker_ids', 'mention_ids', 'turn_masks', 'graph', 'graph_data'],\n",
       "        num_rows: 1862\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'label_ids', 'input_ids', 'input_mask', 'segment_ids', 'speaker_ids', 'mention_ids', 'turn_masks', 'graph', 'graph_data'],\n",
       "        num_rows: 1914\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tucore_data = datasets.load_from_disk(\"./datasets/DialogRE/parity_2/\")\n",
    "tucore_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertflashtest2 import TUCOREGCN_BertForSequenceClassification, TUCOREGCN_BertConfig\n",
    "cfg = TUCOREGCN_BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "b = TUCOREGCN_BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "b = b.to(torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BertForSequenceClassification(\n",
       "  (tucoregcn_bert): TUCOREGCN_Bert(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (speaker_embeddings): Embedding(512, 768)\n",
       "      )\n",
       "      (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "      (emb_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (encoder): BertEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Block(\n",
       "            (mixer): MHA(\n",
       "              (Wqkv): FusedDense(in_features=768, out_features=2304, bias=True)\n",
       "              (inner_attn): FlashSelfAttention(\n",
       "                (drop): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (inner_cross_attn): FlashCrossAttention(\n",
       "                (drop): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): FusedDense(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "            (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (mlp): FusedMLP(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "            (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): FusedDense(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (turnAttention): TurnAttention(\n",
       "      (w_qs): FusedDense(in_features=768, out_features=768, bias=False)\n",
       "      (w_ks): FusedDense(in_features=768, out_features=768, bias=False)\n",
       "      (w_vs): FusedDense(in_features=768, out_features=768, bias=False)\n",
       "      (out_lin): FusedDense(in_features=768, out_features=768, bias=False)\n",
       "      (drop_path): StochasticDepth(p=0.0, mode=row)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (GCN_layers): ModuleList(\n",
       "      (0-1): 2 x RelGraphConvLayer(\n",
       "        (activation): ReLU()\n",
       "        (conv): HeteroGraphConv(\n",
       "          (mods): ModuleDict(\n",
       "            (speaker): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (dialog): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "            (entity): GraphConv(in=768, out=768, normalization=right, activation=None)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.6, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (LSTM_layers): ModuleList(\n",
       "      (0-1): 2 x TurnLevelLSTM(\n",
       "        (lstm): LSTM(768, 768, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (bilstm2hiddnesize): FusedDense(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): FusedDense(in_features=6912, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucoregcn_bert.bert.embeddings.word_embeddings.weight True\n",
      "tucoregcn_bert.bert.embeddings.position_embeddings.weight True\n",
      "tucoregcn_bert.bert.embeddings.token_type_embeddings.weight True\n",
      "tucoregcn_bert.bert.embeddings.speaker_embeddings.weight True\n",
      "tucoregcn_bert.bert.emb_ln.weight True\n",
      "tucoregcn_bert.bert.emb_ln.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.0.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.0.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.0.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.0.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.0.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.1.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.1.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.1.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.1.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.1.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.2.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.2.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.2.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.2.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.2.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.3.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.3.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.3.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.3.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.3.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.4.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.4.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.4.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.4.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.4.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.5.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.5.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.5.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.5.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.5.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.6.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.6.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.6.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.6.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.6.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.7.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.7.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.7.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.7.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.7.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.8.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.8.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.8.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.8.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.8.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.9.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.9.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.9.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.9.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.9.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.10.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.10.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.10.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.10.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.10.norm2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mixer.Wqkv.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mixer.Wqkv.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mixer.out_proj.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mixer.out_proj.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.11.norm1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.11.norm1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mlp.fc1.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mlp.fc1.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mlp.fc2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.11.mlp.fc2.bias True\n",
      "tucoregcn_bert.bert.encoder.layers.11.norm2.weight True\n",
      "tucoregcn_bert.bert.encoder.layers.11.norm2.bias True\n",
      "tucoregcn_bert.bert.pooler.dense.weight True\n",
      "tucoregcn_bert.bert.pooler.dense.bias True\n",
      "tucoregcn_bert.turnAttention.w_qs.weight True\n",
      "tucoregcn_bert.turnAttention.w_ks.weight True\n",
      "tucoregcn_bert.turnAttention.w_vs.weight True\n",
      "tucoregcn_bert.turnAttention.out_lin.weight True\n",
      "tucoregcn_bert.turnAttention.layer_norm.weight True\n",
      "tucoregcn_bert.turnAttention.layer_norm.bias True\n",
      "tucoregcn_bert.GCN_layers.0.weight True\n",
      "tucoregcn_bert.GCN_layers.0.h_bias True\n",
      "tucoregcn_bert.GCN_layers.0.loop_weight True\n",
      "tucoregcn_bert.GCN_layers.1.weight True\n",
      "tucoregcn_bert.GCN_layers.1.h_bias True\n",
      "tucoregcn_bert.GCN_layers.1.loop_weight True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1 True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.weight True\n",
      "tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.bias True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1 True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1_reverse True\n",
      "tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.weight True\n",
      "tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "for n, p in b.named_parameters():\n",
    "\tprint(n, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard = tucore_data['train'][0:32]\n",
    "device = \"cuda\"\n",
    "label_ids = torch.LongTensor(shard[\"label_ids\"]).contiguous().float().to(device)\n",
    "input_ids = torch.LongTensor(shard[\"input_ids\"]).contiguous().to(device)\n",
    "segment_ids = torch.LongTensor(shard[\"segment_ids\"]).contiguous().to(device)\n",
    "input_masks = torch.LongTensor(shard[\"input_mask\"]).contiguous().to(device)\n",
    "mention_ids = torch.LongTensor(shard[\"mention_ids\"]).contiguous().to(device)\n",
    "speaker_ids = torch.LongTensor(shard[\"speaker_ids\"]).contiguous().to(device)\n",
    "turn_mask = torch.LongTensor(shard[\"turn_masks\"]).contiguous().to(torch.float16).to(device)\n",
    "# forgot to flatten the list 1\n",
    "graphs = [pickle.loads(g)[0].to(device) for g in shard['graph']]\n",
    "#print(shard[\"tokens\"])\n",
    "#print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 512, 64]) torch.Size([32, 1, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.2008, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 2.0977, -4.8203,  2.6484,  ..., -3.0352, -1.8633,  2.3730],\n",
       "        [-1.5371, -0.7388, -2.7207,  ..., -2.2441,  1.3584, -1.2959],\n",
       "        [ 0.0704, -3.6660,  0.5093,  ...,  0.1266,  1.3408, -0.2920],\n",
       "        ...,\n",
       "        [ 0.7241,  0.9243,  1.0332,  ..., -2.3984, -0.9561,  0.3440],\n",
       "        [ 2.1250, -0.6787,  0.4321,  ..., -4.2852,  2.3691,  0.3171],\n",
       "        [ 2.4395, -5.1055,  1.6514,  ..., -0.4211,  3.2871,  4.8672]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = b(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_masks, speaker_ids=speaker_ids, graphs=graphs, mention_ids=mention_ids, turn_mask=turn_mask, labels=label_ids)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TUCOREGCN_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.classifier.bias', 'bert.classifier.weight', 'bert.tucoregcn_bert.GCN_layers.0.h_bias', 'bert.tucoregcn_bert.GCN_layers.0.loop_weight', 'bert.tucoregcn_bert.GCN_layers.0.weight', 'bert.tucoregcn_bert.GCN_layers.1.h_bias', 'bert.tucoregcn_bert.GCN_layers.1.loop_weight', 'bert.tucoregcn_bert.GCN_layers.1.weight', 'bert.tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.bias', 'bert.tucoregcn_bert.LSTM_layers.0.bilstm2hiddnesize.weight', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_hh_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.bias_ih_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_hh_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1', 'bert.tucoregcn_bert.LSTM_layers.0.lstm.weight_ih_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.bias', 'bert.tucoregcn_bert.LSTM_layers.1.bilstm2hiddnesize.weight', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_hh_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.bias_ih_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_hh_l1_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l0_reverse', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1', 'bert.tucoregcn_bert.LSTM_layers.1.lstm.weight_ih_l1_reverse', 'bert.tucoregcn_bert.bert.embeddings.LayerNorm.bias', 'bert.tucoregcn_bert.bert.embeddings.LayerNorm.weight', 'bert.tucoregcn_bert.bert.embeddings.position_embeddings.weight', 'bert.tucoregcn_bert.bert.embeddings.speaker_embeddings.weight', 'bert.tucoregcn_bert.bert.embeddings.token_type_embeddings.weight', 'bert.tucoregcn_bert.bert.embeddings.word_embeddings.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.0.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.0.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.1.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.1.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.10.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.10.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.11.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.11.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.2.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.2.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.3.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.3.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.4.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.4.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.5.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.5.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.6.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.6.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.7.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.7.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.8.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.8.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.output.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.self.key.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.self.key.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.self.query.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.self.query.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.self.value.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.attention.self.value.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.intermediate.dense.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.output.LayerNorm.weight', 'bert.tucoregcn_bert.bert.encoder.layer.9.output.dense.bias', 'bert.tucoregcn_bert.bert.encoder.layer.9.output.dense.weight', 'bert.tucoregcn_bert.bert.pooler.dense.bias', 'bert.tucoregcn_bert.bert.pooler.dense.weight', 'bert.tucoregcn_bert.turnAttention.layer_norm.bias', 'bert.tucoregcn_bert.turnAttention.layer_norm.weight', 'bert.tucoregcn_bert.turnAttention.out_lin.weight', 'bert.tucoregcn_bert.turnAttention.w_ks.weight', 'bert.tucoregcn_bert.turnAttention.w_qs.weight', 'bert.tucoregcn_bert.turnAttention.w_vs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from tucore_gcn_bert_modelling import TUCOREGCN_BertForSequenceClassification as OldTUCORE, TUCOREGCN_BertConfig as OldTUCOREConfig\n",
    "from transformers.models.bert.modeling_bert import BertAttention\n",
    "cfg_old = OldTUCOREConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "b_old = OldTUCORE.from_pretrained(\"bert-base-uncased\", config=cfg).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7859, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.3151,  0.0591, -0.2975,  0.0018, -0.7019, -0.2182,  0.2195,  0.5404,\n",
       "          0.3145,  1.0241,  0.2763, -0.9889,  0.0384, -1.3534,  0.9168,  1.1087,\n",
       "         -0.8001,  1.0619,  1.4883, -1.1176, -0.4786, -0.4132, -0.4819, -0.3732,\n",
       "          0.0739,  0.4452, -0.2933, -0.8128,  0.1401, -0.2243,  1.3199, -0.8920,\n",
       "          0.7906,  1.4601, -0.0192, -0.0884]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0783, 0.1116,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0900, 0.1012,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0963, 0.0825,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0969, 0.0985,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1077, 0.1363,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1054, 0.1091,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1347, 0.1345,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1327, 0.1240,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1124, 0.1106,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1056, 0.1198,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1593, 0.1052,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1613, 0.1073,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]]]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_old = b_old(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_masks, speaker_ids=speaker_ids, graphs=graphs, mention_ids=mention_ids, turn_mask=turn_mask, labels=label_ids)\n",
    "out_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = b.bert(\n",
    "            input_ids,\n",
    "            position_ids = None,\n",
    "            speaker_ids=speaker_ids,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_masks,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0575, -0.6606,  0.3245,  ..., -1.3594,  1.3398, -2.0703],\n",
       "         [ 0.1743,  0.2061, -0.6011,  ..., -0.6396,  1.6611, -0.7979],\n",
       "         [ 1.0068, -0.9053, -0.0556,  ..., -1.1133,  0.2900, -1.3008],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 512, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1]]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertflashtest2 import BertModel, TUCOREGCN_BertConfig\n",
    "cfg = TUCOREGCN_BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "b = BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "b = b.cuda().to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndNoAttention(last_hidden_state=tensor([[[-1.0557,  0.1991, -0.9048,  ...,  0.8823, -1.6025, -0.3289],\n",
       "         [-1.1992,  0.2029, -1.1611,  ...,  1.0039, -1.1309,  0.7852],\n",
       "         [-0.1595,  0.1234, -0.3401,  ...,  0.6060, -1.0635,  0.3459],\n",
       "         [ 0.6133, -0.2169, -0.1602,  ...,  0.6953, -1.4170,  0.0222]],\n",
       "\n",
       "        [[ 0.2034,  0.4106, -0.1943,  ...,  0.0327, -0.6909,  0.5894],\n",
       "         [ 0.3235,  0.2522, -0.0910,  ...,  0.6362, -1.7666,  1.6436],\n",
       "         [ 0.7251,  0.2554,  0.1146,  ..., -0.2432, -1.7236,  0.5312],\n",
       "         [-0.2998,  0.9570,  0.6396,  ...,  0.4163, -1.9893,  0.0149]]],\n",
       "       device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<DropoutAddLayerNormFnBackward>), pooler_output=tensor([[ 0.3389, -0.1530,  0.2493,  ...,  0.1101, -0.5947, -0.5073],\n",
       "        [ 0.2275,  0.0264, -0.6108,  ..., -0.0265, -0.2347, -0.5386]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<TanhBackward0>), hidden_states=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(input_ids=torch.tensor([[242,411,4253, 4], [1,1,4, 4]], device='cuda', dtype=torch.long), speaker_ids=torch.tensor([[1,1,1,0], [1,1,1,0]], device='cuda', dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from berttest import BertModel, BertConfig\n",
    "cfg = BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "b = BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "b = b.cuda().to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b(input_ids=torch.tensor([[242,411,4253, 4], [1,1,4, 4]], device='cuda', dtype=torch.long), speaker_ids=torch.tensor([[1,1,1,0], [1,1,1,0]], device='cuda', dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from berttest import BertModel as FBertModel, BertConfig as FBertConfig\n",
    "cfg = FBertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "b = FBertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "b = b.cuda().to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([2, 4, 768])\n",
    "torch.Size([0, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1273,  1.0635,  0.8647,  ...,  1.6279, -0.1172, -0.7095],\n",
       "         [-1.3789,  2.6348, -0.3916,  ...,  2.0254,  0.5186,  0.9683],\n",
       "         [ 1.0078,  0.1206,  0.2401,  ...,  1.5693,  0.1462, -0.5308],\n",
       "         [ 0.4731,  1.6836,  0.8052,  ...,  1.3770, -0.3347, -1.5928]],\n",
       "\n",
       "        [[ 0.3369,  0.4458,  0.3726,  ...,  0.7710, -0.9214, -0.5576],\n",
       "         [-0.3430,  0.8477,  0.6670,  ...,  1.5322, -0.2281, -0.1907],\n",
       "         [ 1.0498,  0.2827,  1.6172,  ...,  1.2080, -0.7671, -0.5688],\n",
       "         [ 1.0684,  0.0049,  0.1741,  ...,  1.8379, -0.2996, -1.2695]]],\n",
       "       device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<DropoutAddLayerNormFnBackward>), pooler_output=tensor([[ 0.3062,  0.4792,  0.3333,  ...,  0.4758,  0.6953, -0.1066],\n",
       "        [-0.5225,  0.3008,  0.2153,  ...,  0.2267,  0.1681, -0.0210]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(input_ids=torch.tensor([[242,411,4253, 4], [1,1,4, 4]], device='cuda', dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='cuda:0')\n",
      "torch.Size([8, 768])\n",
      "torch.Size([8, 768])\n",
      "tensor([[[ 0.1370, -0.6714, -0.3083,  ..., -0.1620, -0.5376, -0.4927],\n",
      "         [ 0.2620,  0.2482,  0.4829,  ..., -0.3311,  0.2043, -0.1761],\n",
      "         [-0.7661, -1.0352,  0.5298,  ...,  0.4292,  0.2485, -1.2930],\n",
      "         ...,\n",
      "         [ 0.4753, -0.0040,  0.0613,  ...,  0.3113,  0.6152,  0.3567],\n",
      "         [ 0.7925, -0.7310, -0.2534,  ...,  0.9414, -0.9277, -0.5991],\n",
      "         [ 0.2864,  0.2115, -0.1990,  ..., -0.1597, -0.9922,  0.8960]],\n",
      "\n",
      "        [[-0.1299, -0.1632, -0.0262,  ..., -0.3716,  0.1635, -0.0753],\n",
      "         [-1.0771, -0.2529, -1.2334,  ..., -0.1819,  0.8052, -0.0424],\n",
      "         [-0.4263,  0.1910,  0.7476,  ..., -0.1449,  0.6797, -0.6064],\n",
      "         ...,\n",
      "         [ 0.0396, -0.4363,  0.6763,  ...,  1.5762,  0.7280,  0.0129],\n",
      "         [ 0.5576, -0.7529,  0.0433,  ...,  0.6987, -0.5986, -0.6836],\n",
      "         [ 0.1493,  0.1188,  0.0728,  ..., -0.1699, -0.0368,  0.2072]],\n",
      "\n",
      "        [[ 0.6011, -1.6709,  0.3667,  ...,  0.1437, -0.3079, -0.8687],\n",
      "         [-0.0687, -0.2477,  0.4478,  ..., -0.3291,  0.0886,  0.2864],\n",
      "         [-0.3428, -0.2842, -0.4976,  ..., -0.6421,  0.6377, -0.7607],\n",
      "         ...,\n",
      "         [ 0.1428, -0.0657, -0.1876,  ...,  0.7358, -0.1672,  0.5913],\n",
      "         [ 0.8154,  0.0067,  0.1273,  ...,  0.5854, -0.0866, -0.4043],\n",
      "         [-0.3179,  0.3232,  0.0648,  ..., -1.1436, -0.5342,  0.5825]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.6235,  0.1277,  0.0720,  ..., -0.2090, -0.5913, -1.0049],\n",
      "         [-0.3965,  0.3438, -0.7402,  ..., -0.8452,  0.8232, -0.4861],\n",
      "         [ 0.0127,  0.0194,  0.2397,  ...,  0.0745,  1.0234,  0.2871],\n",
      "         ...,\n",
      "         [ 0.3181, -0.3530,  0.4302,  ...,  0.8677,  0.7212,  0.8188],\n",
      "         [ 0.2338, -0.8608, -0.6255,  ...,  1.2090, -0.8315, -1.2178],\n",
      "         [ 0.2898,  0.3220,  0.0286,  ..., -0.5859, -0.0678,  0.7979]],\n",
      "\n",
      "        [[-0.8159, -0.9102,  0.2603,  ..., -0.6582, -0.6514, -1.3242],\n",
      "         [ 0.2871,  0.1674,  0.6841,  ..., -0.1685,  0.1129, -1.3135],\n",
      "         [-0.7661, -0.4709, -0.2372,  ..., -0.5171,  0.0359, -0.5425],\n",
      "         ...,\n",
      "         [-0.3875, -0.6758,  0.0737,  ...,  0.7202, -0.0262,  0.0988],\n",
      "         [ 0.6714, -0.4075, -0.4392,  ...,  0.3254, -0.1204,  0.1096],\n",
      "         [-0.9531,  0.2279,  0.3892,  ..., -0.9121, -0.9961,  0.3977]],\n",
      "\n",
      "        [[-0.7651, -0.9829,  0.2166,  ...,  0.9136, -1.1631, -0.7988],\n",
      "         [-0.1146,  1.6104,  0.3047,  ..., -0.0731, -0.3279, -0.4600],\n",
      "         [-0.6343,  0.0871, -0.4014,  ..., -0.4656,  0.3789, -0.6035],\n",
      "         ...,\n",
      "         [-0.1777, -1.2949, -0.2267,  ...,  0.0219, -0.1364,  0.1089],\n",
      "         [ 0.7061,  0.5083,  0.7549,  ..., -0.6685, -0.4980,  0.1879],\n",
      "         [ 0.3293,  0.3901, -0.2085,  ...,  0.0105, -1.4502, -0.1484]]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True) tensor([0, 4, 8], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m242\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m411\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4253\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\tucore_gcn_transformers\\bertflashtest.py:1413\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, speaker_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict, masked_tokens_mask)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1412\u001b[0m     subset_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m sequence_output, all_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# output_attentions=output_attentions,\u001b[39;49;00m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# return_dict=return_dict,\u001b[39;49;00m\n\u001b[0;32m   1420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1421\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m )\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\tucore_gcn_transformers\\bertflashtest.py:1040\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, key_padding_mask, subset_mask, output_hidden_states)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1039\u001b[0m         all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m-> 1040\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1042\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\tucore_gcn_transformers\\bertflashtest.py:564\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, hidden_states, residual, mixer_subset, mixer_kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m         rowscale1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\n\u001b[0;32m    560\u001b[0m             torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[0;32m    561\u001b[0m                 mixer_out\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mmixer_out\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mmixer_out\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    562\u001b[0m             )\n\u001b[0;32m    563\u001b[0m         )\n\u001b[1;32m--> 564\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mfused_add_norm_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmixer_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrowscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrowscale1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprenorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp, nn\u001b[38;5;241m.\u001b[39mIdentity):\n\u001b[0;32m    575\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\flash_attn\\ops\\layer_norm.py:677\u001b[0m, in \u001b[0;36mdropout_add_layer_norm\u001b[1;34m(x0, residual, weight, bias, dropout_p, epsilon, rowscale, layerscale, prenorm, residual_in_fp32, return_dropout_mask)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdropout_add_layer_norm\u001b[39m(\n\u001b[0;32m    662\u001b[0m     x0,\n\u001b[0;32m    663\u001b[0m     residual,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    672\u001b[0m     return_dropout_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m ):\n\u001b[0;32m    674\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"residual_in_fp32 only has an effect if residual is None.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;124;03m    Otherwise residual dtype is residual.dtype.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDropoutAddLayerNormFn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrowscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayerscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprenorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dropout_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\flash_attn\\ops\\layer_norm.py:328\u001b[0m, in \u001b[0;36mDropoutAddLayerNormFn.forward\u001b[1;34m(ctx, x0, residual, gamma, beta, rowscale, colscale, dropout_p, epsilon, residual_in_fp32, prenorm, is_rms_norm, return_dmask)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    314\u001b[0m     ctx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     return_dmask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m maybe_align(\u001b[43mx0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m(), \u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m    329\u001b[0m     residual \u001b[38;5;241m=\u001b[39m maybe_align(residual\u001b[38;5;241m.\u001b[39mcontiguous(), \u001b[38;5;241m16\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m maybe_align(gamma\u001b[38;5;241m.\u001b[39mcontiguous(), \u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'contiguous'"
     ]
    }
   ],
   "source": [
    "b(input_ids=torch.tensor([[242,411,4253, 4], [1,1,4, 4]], device='cuda', dtype=torch.long), speaker_ids=torch.tensor([[1,1,1,0], [1,1,1,0]], device='cuda', dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUCOREGCN_BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_probs_stochastic_dropout_prob\": [\n",
       "    0.0\n",
       "  ],\n",
       "  \"classifier_dropout\": null,\n",
       "  \"debug_ret\": false,\n",
       "  \"dropout_cls\": \"pytorch_dropout\",\n",
       "  \"fused_bias_fc\": [\n",
       "    true\n",
       "  ],\n",
       "  \"fused_dropout_add_ln\": [\n",
       "    true\n",
       "  ],\n",
       "  \"fused_mlp\": [\n",
       "    true\n",
       "  ],\n",
       "  \"gcn_act\": \"relu\",\n",
       "  \"gcn_dropout\": 0.6,\n",
       "  \"gcn_layers\": 2,\n",
       "  \"hidden_act\": \"gelu_fast\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"hidden_stochastic_dropout_prob\": [\n",
       "    0.0\n",
       "  ],\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\",\n",
       "    \"15\": \"LABEL_15\",\n",
       "    \"16\": \"LABEL_16\",\n",
       "    \"17\": \"LABEL_17\",\n",
       "    \"18\": \"LABEL_18\",\n",
       "    \"19\": \"LABEL_19\",\n",
       "    \"20\": \"LABEL_20\",\n",
       "    \"21\": \"LABEL_21\",\n",
       "    \"22\": \"LABEL_22\",\n",
       "    \"23\": \"LABEL_23\",\n",
       "    \"24\": \"LABEL_24\",\n",
       "    \"25\": \"LABEL_25\",\n",
       "    \"26\": \"LABEL_26\",\n",
       "    \"27\": \"LABEL_27\",\n",
       "    \"28\": \"LABEL_28\",\n",
       "    \"29\": \"LABEL_29\",\n",
       "    \"30\": \"LABEL_30\",\n",
       "    \"31\": \"LABEL_31\",\n",
       "    \"32\": \"LABEL_32\",\n",
       "    \"33\": \"LABEL_33\",\n",
       "    \"34\": \"LABEL_34\",\n",
       "    \"35\": \"LABEL_35\"\n",
       "  },\n",
       "  \"id2token\": {\n",
       "    \"1\": \"per:positive_impression\",\n",
       "    \"10\": \"per:girl/boyfriend\",\n",
       "    \"11\": \"per:neighbor\",\n",
       "    \"12\": \"per:roommate\",\n",
       "    \"13\": \"per:children\",\n",
       "    \"14\": \"per:other_family\",\n",
       "    \"15\": \"per:parents\",\n",
       "    \"16\": \"per:siblings\",\n",
       "    \"17\": \"per:spouse\",\n",
       "    \"18\": \"per:place_of_residence\",\n",
       "    \"19\": \"per:place_of_birth\",\n",
       "    \"2\": \"per:negative_impression\",\n",
       "    \"20\": \"per:visited_place\",\n",
       "    \"21\": \"per:origin\",\n",
       "    \"22\": \"per:employee_or_member_of\",\n",
       "    \"23\": \"per:schools_attended\",\n",
       "    \"24\": \"per:works\",\n",
       "    \"25\": \"per:age\",\n",
       "    \"26\": \"per:date_of_birth\",\n",
       "    \"27\": \"per:major\",\n",
       "    \"28\": \"per:place_of_work\",\n",
       "    \"29\": \"per:title\",\n",
       "    \"3\": \"per:acquaintance\",\n",
       "    \"30\": \"per:alternate_names\",\n",
       "    \"31\": \"per:pet\",\n",
       "    \"32\": \"gpe:residents_of_place\",\n",
       "    \"33\": \"gpe:births_in_place\",\n",
       "    \"34\": \"gpe:visitors_of_place\",\n",
       "    \"35\": \"org:employees_or_members\",\n",
       "    \"36\": \"org:students\",\n",
       "    \"37\": \"unanswerable\",\n",
       "    \"4\": \"per:alumni\",\n",
       "    \"5\": \"per:boss\",\n",
       "    \"6\": \"per:subordinate\",\n",
       "    \"7\": \"per:client\",\n",
       "    \"8\": \"per:dates\",\n",
       "    \"9\": \"per:friends\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_15\": 15,\n",
       "    \"LABEL_16\": 16,\n",
       "    \"LABEL_17\": 17,\n",
       "    \"LABEL_18\": 18,\n",
       "    \"LABEL_19\": 19,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_20\": 20,\n",
       "    \"LABEL_21\": 21,\n",
       "    \"LABEL_22\": 22,\n",
       "    \"LABEL_23\": 23,\n",
       "    \"LABEL_24\": 24,\n",
       "    \"LABEL_25\": 25,\n",
       "    \"LABEL_26\": 26,\n",
       "    \"LABEL_27\": 27,\n",
       "    \"LABEL_28\": 28,\n",
       "    \"LABEL_29\": 29,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_30\": 30,\n",
       "    \"LABEL_31\": 31,\n",
       "    \"LABEL_32\": 32,\n",
       "    \"LABEL_33\": 33,\n",
       "    \"LABEL_34\": 34,\n",
       "    \"LABEL_35\": 35,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mark_shared_params\": [\n",
       "    false\n",
       "  ],\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"norm_cls\": [\n",
       "    \"pytorch_layernorm\"\n",
       "  ],\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_attentions\": true,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"prenorm\": [\n",
       "    false\n",
       "  ],\n",
       "  \"problem_type\": \"multi_label_classification\",\n",
       "  \"residual_in_fp32\": [\n",
       "    false\n",
       "  ],\n",
       "  \"return_residual\": [\n",
       "    false\n",
       "  ],\n",
       "  \"sequence_parallel\": [\n",
       "    false\n",
       "  ],\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"turn_attention_dropout_prob\": 0.1,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"use_flash_attn\": [\n",
       "    true\n",
       "  ],\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers\")\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\\")\n",
    "from bertflashtest import BertModel, TUCOREGCN_BertConfig\n",
    "cfgs = TUCOREGCN_BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "cfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropout_layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1205,  0.1024, -0.1764,  ...,  0.0771, -0.1893,  0.0387],\n",
       "          [-0.2876, -0.0755,  0.0746,  ...,  0.0045,  0.0380, -0.0340],\n",
       "          [ 0.0745, -0.1656, -0.0764,  ...,  0.0611, -0.1659, -0.1878],\n",
       "          ...,\n",
       "          [ 0.2771, -0.3058, -0.1100,  ..., -0.1033, -0.3371,  0.2852],\n",
       "          [ 0.3683, -0.0591, -0.2646,  ...,  0.1142,  0.0963, -0.0448],\n",
       "          [ 0.1025, -0.5471,  0.0992,  ..., -0.0914, -0.2076, -0.0386]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor([[[ 0.7346,  0.7809,  0.4057,  ...,  0.1942,  0.6727, -0.0043],\n",
       "          [ 0.2975,  0.9328,  0.4387,  ...,  0.6514,  0.0458,  0.7582],\n",
       "          [ 0.5049,  0.7073,  0.3685,  ...,  0.0822,  0.8734,  0.8965],\n",
       "          ...,\n",
       "          [ 0.5644,  0.7097,  0.2842,  ...,  0.1281,  0.3838,  0.3247],\n",
       "          [ 0.7250,  0.5849,  0.0344,  ...,  0.2448,  0.7588,  0.4986],\n",
       "          [ 0.4457,  0.9543,  0.0584,  ...,  0.2401,  0.6713,  0.5698]]],\n",
       "        device='cuda:0', grad_fn=<DropoutAddLayerNormFnBackward>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flash_attn.modules.block import Block\n",
    "b = Block(768, fused_dropout_add_ln=True).cuda()\n",
    "b(torch.rand((1, 512, 768), device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "FWD: Unsupported hidden_size or types: 1024FloatFloatFloatFloatFloat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m b \u001b[38;5;241m=\u001b[39m Block(\u001b[38;5;241m1024\u001b[39m, fused_dropout_add_ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\flash_attn\\modules\\block.py:163\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, hidden_states, residual, mixer_subset, mixer_kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m         rowscale1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\n\u001b[0;32m    157\u001b[0m             torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[0;32m    158\u001b[0m                 hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m             )\n\u001b[0;32m    162\u001b[0m         )                \n\u001b[1;32m--> 163\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mfused_add_norm_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrowscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrowscale1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprenorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mixer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     mixer_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\flash_attn\\ops\\layer_norm.py:677\u001b[0m, in \u001b[0;36mdropout_add_layer_norm\u001b[1;34m(x0, residual, weight, bias, dropout_p, epsilon, rowscale, layerscale, prenorm, residual_in_fp32, return_dropout_mask)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdropout_add_layer_norm\u001b[39m(\n\u001b[0;32m    662\u001b[0m     x0,\n\u001b[0;32m    663\u001b[0m     residual,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    672\u001b[0m     return_dropout_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m ):\n\u001b[0;32m    674\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"residual_in_fp32 only has an effect if residual is None.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;124;03m    Otherwise residual dtype is residual.dtype.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDropoutAddLayerNormFn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrowscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayerscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprenorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dropout_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\flash_attn\\ops\\layer_norm.py:334\u001b[0m, in \u001b[0;36mDropoutAddLayerNormFn.forward\u001b[1;34m(ctx, x0, residual, gamma, beta, rowscale, colscale, dropout_p, epsilon, residual_in_fp32, prenorm, is_rms_norm, return_dmask)\u001b[0m\n\u001b[0;32m    332\u001b[0m rowscale \u001b[38;5;241m=\u001b[39m maybe_align(rowscale\u001b[38;5;241m.\u001b[39mcontiguous(), \u001b[38;5;241m16\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m rowscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    333\u001b[0m colscale \u001b[38;5;241m=\u001b[39m maybe_align(colscale\u001b[38;5;241m.\u001b[39mcontiguous(), \u001b[38;5;241m16\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m colscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m zmat, xmat, dmask, mu, rsigma \u001b[38;5;241m=\u001b[39m \u001b[43m_dropout_add_layer_norm_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrowscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_rms_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# Only need to save x0 if we need to compute gradient wrt colscale\u001b[39;00m\n\u001b[0;32m    347\u001b[0m x0_saved \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;28;01mif\u001b[39;00m colscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\affect\\TUCORE-GCN\\.venv\\Lib\\site-packages\\flash_attn\\ops\\layer_norm.py:33\u001b[0m, in \u001b[0;36m_dropout_add_layer_norm_forward\u001b[1;34m(x0, residual, gamma, beta, rowscale, colscale, dropout_p, epsilon, residual_in_fp32, is_rms_norm)\u001b[0m\n\u001b[0;32m     31\u001b[0m residualmat \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_size)) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     32\u001b[0m rowscale \u001b[38;5;241m=\u001b[39m rowscale\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m rowscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m zmat, xmat, dmask, mu, rsigma \u001b[38;5;241m=\u001b[39m \u001b[43mdropout_layer_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_add_ln_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidualmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrowscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_rms_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# dmask is None if dropout_p == 0.0\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# xmat is None if dropout_p == 0.0 and residual is None and residual_dtype != input_dtype\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zmat, xmat \u001b[38;5;28;01mif\u001b[39;00m xmat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x0mat, dmask, mu, rsigma\n",
      "\u001b[1;31mRuntimeError\u001b[0m: FWD: Unsupported hidden_size or types: 1024FloatFloatFloatFloatFloat"
     ]
    }
   ],
   "source": [
    "b = Block(1024, fused_dropout_add_ln=True).cuda()\n",
    "b(torch.rand((1, 512, 1024), device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'dropout_add_ln_bwd',\n",
       " 'dropout_add_ln_fwd',\n",
       " 'dropout_add_ln_parallel_residual_bwd',\n",
       " 'dropout_add_ln_parallel_residual_fwd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer_norm.dropout_add_ln_bwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(0).to(dtype=torch.float16).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn.modules.block import Block\n",
    "block = Block(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\tucore_gcn_transformers\")\n",
    "sys.path.append(\"d:\\\\projects\\\\affect\\\\TUCORE-GCN\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "config = BertConfig.from_json_file(\"./models/BERT/tucoregcn_bert_mlc.json\")\n",
    "bertmodel = BertModel.from_pretrained(\"bert-base-uncased\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for name, p in bertmodel.named_parameters():\n",
    "\tprint(name)\n",
    "\tp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bertmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
